[
  {
    "id": "3e41c9a2-5662-4ef5-92a0-1614518103a6",
    "title": "Info",
    "upload_date": "2025-02-03",
    "file_name": "temp_theo23w (1).pdf",
    "chunks": [
      "Organization: 23w 1 (1)\n! See the course page on www.moodle.tum.de for more details (information\non moodle has precedence; this slide might become outdated!)\n\u25b7 Lectures each Thursday starting at 15:30 (sharp) till approx. 18:00\n1.5 lectures (135min) plus a short break (10-15min) in between.\n(Because of Dies Academicus, 07.12.23, lecture might be moved to 08.12.23, 9:45-11:45)\n\u25b7 Main tutorials: C. Welzel-Mohr, 2 groups on Thursdays, 8:15 resp. 10:15.\nOptional tutorial: N. Kienzle, 1 group on Mondays, 8:15.\n(Registration/assignment see moodle; TA: Christoph Welzel-Mohr)\n\u25b7 Grade bonus: three graded exercises on Thursdays (probably 16.11.23, 21.12.23,\n01.01.24); (at most) 0.3, applies only after passing, (as before)\n\u25b7 Exams: endterm and retake as usual, registration required, 150min, auxiliary\nmeans as usual (single hand-written cheat sheet, non-programmable calculator)\nRead the moodle course page; and use the forum or Zulip in case\nof questions!",
      "\u25b7 Exams: endterm and retake as usual, registration required, 150min, auxiliary\nmeans as usual (single hand-written cheat sheet, non-programmable calculator)\nRead the moodle course page; and use the forum or Zulip in case\nof questions!\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Theory of Computation and Information Theory\n(INHN0013)\n\u00a9 M. Luttenberger (23w)\nDepartment of Computer Science\nTUM CIT\n24. November 2023\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\nIntroduction\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Effective calculation, systems of calculation (calculi) 6 (6)\n\u25b7 Classic notion of calculation/computation as known from school:\nFinite system of rigorous rules/instructions for solving some problem, e.g.:\n\u25b7 Originally, \u201calgorithm\u201d was used for rules for computing with integers, e.g.\nEuclidean division or Euclidean algorithm.\n\u25b7 Evaluating an arithmetic expression.\n\u25b7 Computing the derivative of elementary functions.\n\u25b7 Solving a system of linear equations by means of Gaussian elemination.\n\u25b7 Rewriting a propositional formula into CNF (clause set).\n\u25b7 Deciding the (un)satisfiability of a clause set using DPLL (resolution).\nRules describe in general a nondeterministic algorithm:\n\u25b7 No predetermined order, but still guaranteed to reach/arrive at the correct\nresult/answer after a finite number of steps if applied correctly.\n\u25b7 Rules can be applied mechanically using pen and paper; no understanding of\ntheir correctness or meaning required.",
      "\u25b7 No predetermined order, but still guaranteed to reach/arrive at the correct\nresult/answer after a finite number of steps if applied correctly.\n\u25b7 Rules can be applied mechanically using pen and paper; no understanding of\ntheir correctness or meaning required.\n(But clever use of rules might simplify/speed up the computation.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Etymology of \u201calgorithm\u201d 7 (7)\n(taken from wikipeida)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Mechanical calculating machines 8 (8)\n\u25b7 E.g. when evaluating arithmetic expressions\n5 \u00b7 3 \u2212 3 \u00b7 4\nwe still need to decide the order of evaluation:\n5 \u00b7 3 \u2212 3 \u00b7 4 \u2192 15 \u2212 3 \u00b7 4 \u2192 15 \u2212 12 \u2192 3\nvs.\n5 \u00b7 3 \u2212 3 \u00b7 4 \u2192 3 \u00b7 (5 \u2212 4) \u2192 3 \u00b7 1 \u2192 3\nBut: (i) as long as we reduce the number of involved operations, we are\nguaranteed to terminate, and (ii) as long as we apply only valid rules we\narrive at the same result no matter the order of evaluation.\n\u25b7 Only requires rules for addition/substraction/multiplication by pen and paper.\n\u25b7 Rules can be physically realized, e.g. multiplication by means of a Leibniz\nwheel, leading to the first mechanical calculating machines.\n\u25b7 Heuristics as \u201ctie breaker\u201d if several rules can be applied.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rewriting: Gaussian elimination 9 (9)\n\uf8eb\n\uf8ed\n6 \u22122 0 1\n21 \u22127 1 \u22121\n18 \u22126 1 \u22121\n\uf8f6\n\uf8f8\n| \u00b71\n6\n\uf8eb\n\uf8ed\n1 0 0\n0 1 0\n0 0 1\n\uf8f6\n\uf8f8\n| \u00b71\n6\n\u25b7 Many \u201cmathematical procedures\u201d are nondeterministic in the sense that they\noften require to make some suitable choice,\n\u25b7 like the choice of the pivot in Gaussian elimination or in the simplex algorithm\nor quick sort\n\u25b7 or the choice of the variable in DPLL or the choice of the clauses in resolution.\nbut the concrete choice is unimportant for the end result.\n! Yet, the precise choice still might influence the length of the calculation\n(runtime). For many important problems making the optimal choice is often\njust as difficult as solving the problem at hand.\nHence, we need to resort to heuristics to resolve nondeterminism.\n(See e.g. later: self-reducibility of SAT.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rewriting: Gaussian elimination 9 (10)\n\uf8eb\n\uf8ed\n1 \u22121\n3 0 1\n6\n21 \u22127 1 \u22121\n18 \u22126 1 \u22121\n\uf8f6\n\uf8f8 \u2190 \u2212\n\u221221\n+\n\u2190 \u2212 \u2212 \u2212 \u2212 \u2212\n\u221218\n+\n\uf8eb\n\uf8ed\n1\n6 0 0\n0 1 0\n0 0 1\n\uf8f6\n\uf8f8 \u2190 \u2212\n\u221221\n+\n\u2190 \u2212 \u2212 \u2212 \u2212 \u2212\n\u221218\n+\n\u25b7 Many \u201cmathematical procedures\u201d are nondeterministic in the sense that they\noften require to make some suitable choice,\n\u25b7 like the choice of the pivot in Gaussian elimination or in the simplex algorithm\nor quick sort\n\u25b7 or the choice of the variable in DPLL or the choice of the clauses in resolution.\nbut the concrete choice is unimportant for the end result.\n! Yet, the precise choice still might influence the length of the calculation\n(runtime). For many important problems making the optimal choice is often\njust as difficult as solving the problem at hand.\nHence, we need to resort to heuristics to resolve nondeterminism.\n(See e.g. later: self-reducibility of SAT.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rewriting: Gaussian elimination 9 (11)\n\uf8eb\n\uf8ed\n1 \u22121\n3 0 1\n6\n0 0 1 \u22129\n2\n0 0 1 \u22124\n\uf8f6\n\uf8f8 | \u00b71\n\uf8eb\n\uf8ed\n1\n6 0 0\n\u22127\n2 1 0\n\u22123 0 1\n\uf8f6\n\uf8f8 | \u00b71\n\u25b7 Many \u201cmathematical procedures\u201d are nondeterministic in the sense that they\noften require to make some suitable choice,\n\u25b7 like the choice of the pivot in Gaussian elimination or in the simplex algorithm\nor quick sort\n\u25b7 or the choice of the variable in DPLL or the choice of the clauses in resolution.\nbut the concrete choice is unimportant for the end result.\n! Yet, the precise choice still might influence the length of the calculation\n(runtime). For many important problems making the optimal choice is often\njust as difficult as solving the problem at hand.\nHence, we need to resort to heuristics to resolve nondeterminism.\n(See e.g. later: self-reducibility of SAT.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rewriting: Gaussian elimination 9 (12)\n\uf8eb\n\uf8ed\n1 \u22121\n3 0 1\n6\n0 0 1 \u22129\n2\n0 0 1 \u22124\n\uf8f6\n\uf8f8\n\u2190 \u2212\n\u22121\n+\n\uf8eb\n\uf8ed\n1\n6 0 0\n\u22127\n2 1 0\n\u22123 0 1\n\uf8f6\n\uf8f8\n\u2190 \u2212\n\u22121\n+\n\u25b7 Many \u201cmathematical procedures\u201d are nondeterministic in the sense that they\noften require to make some suitable choice,\n\u25b7 like the choice of the pivot in Gaussian elimination or in the simplex algorithm\nor quick sort\n\u25b7 or the choice of the variable in DPLL or the choice of the clauses in resolution.\nbut the concrete choice is unimportant for the end result.\n! Yet, the precise choice still might influence the length of the calculation\n(runtime). For many important problems making the optimal choice is often\njust as difficult as solving the problem at hand.\nHence, we need to resort to heuristics to resolve nondeterminism.\n(See e.g. later: self-reducibility of SAT.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rewriting: Gaussian elimination 9 (13)\n\uf8eb\n\uf8ed\n1 \u22121\n3 0 1\n6\n0 0 1 \u22129\n2\n0 0 0 1\n2\n\uf8f6\n\uf8f8\n| \u00b72\n\uf8eb\n\uf8ed\n1\n6 0 0\n\u22127\n2 1 0\n1\n2 \u22121 1\n\uf8f6\n\uf8f8\n| \u00b72\n\u25b7 Many \u201cmathematical procedures\u201d are nondeterministic in the sense that they\noften require to make some suitable choice,\n\u25b7 like the choice of the pivot in Gaussian elimination or in the simplex algorithm\nor quick sort\n\u25b7 or the choice of the variable in DPLL or the choice of the clauses in resolution.\nbut the concrete choice is unimportant for the end result.\n! Yet, the precise choice still might influence the length of the calculation\n(runtime). For many important problems making the optimal choice is often\njust as difficult as solving the problem at hand.\nHence, we need to resort to heuristics to resolve nondeterminism.\n(See e.g. later: self-reducibility of SAT.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rewriting: Gaussian elimination 9 (14)\n\uf8eb\n\uf8ed\n1 \u22121\n3 0 1\n6\n0 0 1 \u22129\n2\n0 0 0 1\n\uf8f6\n\uf8f8\n\u2190 \u2212\n\u22121\n6\n+\n\u2190 \u2212\n9\n2\n+\n\uf8eb\n\uf8ed\n1\n6 0 0\n\u22127\n2 1 0\n1 \u22122 2\n\uf8f6\n\uf8f8\n\u2190 \u2212\n\u22121\n6\n+\n\u2190 \u2212\n9\n2\n+\n\u25b7 Many \u201cmathematical procedures\u201d are nondeterministic in the sense that they\noften require to make some suitable choice,\n\u25b7 like the choice of the pivot in Gaussian elimination or in the simplex algorithm\nor quick sort\n\u25b7 or the choice of the variable in DPLL or the choice of the clauses in resolution.\nbut the concrete choice is unimportant for the end result.\n! Yet, the precise choice still might influence the length of the calculation\n(runtime). For many important problems making the optimal choice is often\njust as difficult as solving the problem at hand.\nHence, we need to resort to heuristics to resolve nondeterminism.\n(See e.g. later: self-reducibility of SAT.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rewriting: Gaussian elimination 9 (15)\n\uf8eb\n\uf8ed\n1 \u22121\n3 0 0\n0 0 1 0\n0 0 0 1\n\uf8f6\n\uf8f8\n\uf8eb\n\uf8ed\n0 1\n3 \u22121\n3\n1 \u22128 9\n1 \u22122 2\n\uf8f6\n\uf8f8\n\u25b7 Many \u201cmathematical procedures\u201d are nondeterministic in the sense that they\noften require to make some suitable choice,\n\u25b7 like the choice of the pivot in Gaussian elimination or in the simplex algorithm\nor quick sort\n\u25b7 or the choice of the variable in DPLL or the choice of the clauses in resolution.\nbut the concrete choice is unimportant for the end result.\n! Yet, the precise choice still might influence the length of the calculation\n(runtime). For many important problems making the optimal choice is often\njust as difficult as solving the problem at hand.\nHence, we need to resort to heuristics to resolve nondeterminism.\n(See e.g. later: self-reducibility of SAT.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPLL (basic version) 10 (16)\n{{\u00acp, q,\u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acp, \u00acs}, {\u00acp, r}}\n{{q, \u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acs}} {{\u00acq, \u00acr, s}, {r}}\n{{\u00acr, s}, {r}, {\u00acs}} {{\u00acr, s}, {r}, {\u00acs}}\n{{s}, {\u00acs}} {{}, {\u00acs}}\n{{}} {{}}\n{{s}, {\u00acs}} {{}, {\u00acs}}\n{{}} {{}}\n{{\u00acr, s}, {r}}\n{{s}}\n{}\np := true p := false\nq := true\nq := false\nr := true r := false\ns := true s := false\nr := true r := false\ns := true s := false\nq := true\nr := true\ns := true\n\u25b7 Variable order in the example: alphabetic order\n\u25b7 Variable order influences running time, but finding optimal order just as hard as SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPLL (basic version) 10 (17)\n{{\u00acp, q,\u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acp, \u00acs}, {\u00acp, r}}\n{{q, \u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acs}} {{\u00acq, \u00acr, s}, {r}}\n{{\u00acr, s}, {r}, {\u00acs}} {{\u00acr, s}, {r}, {\u00acs}}\n{{s}, {\u00acs}} {{}, {\u00acs}}\n{{}} {{}}\n{{s}, {\u00acs}} {{}, {\u00acs}}\n{{}} {{}}\n{{\u00acr, s}, {r}}\n{{s}}\n{}\np := true p := false\nq := true\nq := false\nr := true r := false\ns := true s := false\nr := true r := false\ns := true s := false\nq := true\nr := true\ns := true\n\u25b7 Variable order in the example: alphabetic order\n\u25b7 Variable order influences running time, but finding optimal order just as hard as SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPLL (basic version) 10 (18)\n{{\u00acp, q,\u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acp, \u00acs}, {\u00acp, r}}\n{{q, \u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acs}} {{\u00acq, \u00acr, s}, {r}}\n{{\u00acr, s}, {r}, {\u00acs}} {{\u00acr, s}, {r}, {\u00acs}}\n{{s}, {\u00acs}} {{}, {\u00acs}}\n{{}} {{}}\n{{s}, {\u00acs}} {{}, {\u00acs}}\n{{}} {{}}\n{{\u00acr, s}, {r}}\n{{s}}\n{}\np := true p := false\nq := true\nq := false\nr := true r := false\ns := true s := false\nr := true r := false\ns := true s := false\nq := true\nr := true\ns := true\n\u25b7 Variable order in the example: alphabetic order\n\u25b7 Variable order influences running time, but finding optimal order just as hard as SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPLL (basic version) 10 (19)\n{{\u00acp, q,\u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acp, \u00acs}, {\u00acp, r}}\n{{q, \u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acs}} {{\u00acq, \u00acr, s}, {r}}\n{{\u00acr, s}, {r}, {\u00acs}} {{\u00acr, s}, {r}, {\u00acs}}\n{{s}, {\u00acs}} {{}, {\u00acs}}\n{{}} {{}}\n{{s}, {\u00acs}} {{}, {\u00acs}}\n{{}} {{}}\n{{\u00acr, s}, {r}}\n{{s}}\n{}\np := true p := false\nq := true\nq := false\nr := true r := false\ns := true s := false\nr := true r := false\ns := true s := false\nq := true\nr := true\ns := true\n\u25b7 Variable order in the example: alphabetic order\n\u25b7 Variable order influences running time, but finding optimal order just as hard as SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPLL (basic version) 10 (20)\n{{\u00acp, q,\u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acp, \u00acs}, {\u00acp, r}}\n{{q, \u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acs}} {{\u00acq, \u00acr, s}, {r}}\n{{\u00acr, s}, {r}, {\u00acs}} {{\u00acr, s}, {r}, {\u00acs}}\n{{s}, {\u00acs}} {{}, {\u00acs}}\n{{}} {{}}\n{{s}, {\u00acs}} {{}, {\u00acs}}\n{{}} {{}}\n{{\u00acr, s}, {r}}\n{{s}}\n{}\np := true p := false\nq := true\nq := false\nr := true r := false\ns := true s := false\nr := true r := false\ns := true s := false\nq := true\nr := true\ns := true\n\u25b7 Variable order in the example: alphabetic order\n\u25b7 Variable order influences running time, but finding optimal order just as hard as SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPLL (basic version) 10 (21)\n{{\u00acp, q,\u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acp, \u00acs}, {\u00acp, r}}\n{{q, \u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acs}} {{\u00acq, \u00acr, s}, {r}}\n{{\u00acr, s}, {r}, {\u00acs}} {{\u00acr, s}, {r}, {\u00acs}}\n{{s}, {\u00acs}} {{}, {\u00acs}}\n{{}} {{}}\n{{s}, {\u00acs}} {{}, {\u00acs}}\n{{}} {{}}\n{{\u00acr, s}, {r}}\n{{s}}\n{}\np := true p := false\nq := true\nq := false\nr := true r := false\ns := true s := false\nr := true r := false\ns := true s := false\nq := true\nr := true\ns := true\n\u25b7 Variable order in the example: alphabetic order\n\u25b7 Variable order influences running time, but finding optimal order just as hard as SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPLL (basic version) 10 (22)\n{{\u00acp, q,\u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acp, \u00acs}, {\u00acp, r}}\n{{q, \u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acs}} {{\u00acq, \u00acr, s}, {r}}\n{{\u00acr, s}, {r}, {\u00acs}} {{\u00acr, s}, {r}, {\u00acs}}\n{{s}, {\u00acs}} {{}, {\u00acs}}\n{{}} {{}}\n{{s}, {\u00acs}} {{}, {\u00acs}}\n{{}} {{}}\n{{\u00acr, s}, {r}}\n{{s}}\n{}\np := true p := false\nq := true\nq := false\nr := true r := false\ns := true s := false\nr := true r := false\ns := true s := false\nq := true\nr := true\ns := true\n\u25b7 Variable order in the example: alphabetic order\n\u25b7 Variable order influences running time, but finding optimal order just as hard as SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPLL (basic version) 10 (23)\n{{\u00acp, q,\u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acp, \u00acs}, {\u00acp, r}}\n{{q, \u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acs}} {{\u00acq, \u00acr, s}, {r}}\n{{\u00acr, s}, {r}, {\u00acs}} {{\u00acr, s}, {r}, {\u00acs}}\n{{s}, {\u00acs}} {{}, {\u00acs}}\n{{}} {{}}\n{{s}, {\u00acs}} {{}, {\u00acs}}\n{{}} {{}}\n{{\u00acr, s}, {r}}\n{{s}}\n{}\np := true p := false\nq := true\nq := false\nr := true r := false\ns := true s := false\nr := true r := false\ns := true s := false\nq := true\nr := true\ns := true\n\u25b7 Variable order in the example: alphabetic order\n\u25b7 Variable order influences running time, but finding optimal order just as hard as SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Hilbert\u2019s program 11 (24)\n\u25b7 David Hilbert\u2019s program in short: \u201cmechanical proofs\u201d\n\u25b7 I.e. find a (countables) set of (proof) rules so that a computer can decide if a\nmathematical statement is valid (always true) or invalid (some\ncounterexample).\n\u25b7 Propositional logics can describe arithmetic of n-bit integers Z 2n (e.g. CLA):\n(z0 \u2194 (x0 \u2295 y0)) \u2227 (z1 \u2194 (x0 \u2227 y0))\n\u25b7 Hilbert\u2019s tenth problem (1900): given an arithmetic expression over the\nintegers Z (i.e. a polynomial), decide if there is an integer solution, e.g.\nthere exist x, y\u2208 Z with x2 + y2 = 1\nis true as we can choose e.g. x = 1 and y = 0 (decision problem).\n! While validity of propositional formulas can be decided \u201cmechanically\u201d using\ne.g. a truth table, Hilbert\u2019s tenth problem (essentially Z instead of Z n) cannot\nbe deciced by a (Turing) program as shown by Matiyasevich (1970).\n(Every (Turing) program can be encoded into an instance of Hilbert\u2019s 10th problem.)",
      "e.g. a truth table, Hilbert\u2019s tenth problem (essentially Z instead of Z n) cannot\nbe deciced by a (Turing) program as shown by Matiyasevich (1970).\n(Every (Turing) program can be encoded into an instance of Hilbert\u2019s 10th problem.)\n(Both the existential theory of the reals and Presburger arithmetic are decidable.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Models of computation 12 (25)\n\u25b7 Proving the undecidability of Hilbert\u2019s tenth problem, the more general\nEntscheidungsproblem, and the related Halting problem requires first to\nprecisely define what \u201ccomputation\u201d and \u201ccalculation\u201d means.\n\u25b7 To this end, several models of computation have been proposed, e.g.:\n\u25b7 1933: \u00b5-recursive functions (G \u00a8odel,Herbrand,Kleene)\n\u25b7 1936: Lambda calculus (Church)\n\u25b7 1936: Turing machines (Turing)\n\u25b7 (string) rewriting systems (Thue, Post, Markov),\nabstract rewriting systems, grammars (Chomsky)\n\u25b7 Register/counter machines and Random access machines\n\u25b7 Cf. also History of the Church-Turing thesis on wikipedia.\n! So far it could always be shown that all models can simulate each other,\ni.e. if we can solve a problem using a particular model of computation, we\ncan translate this solution/algorithm into every other models.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Church-Turing thesis/conjecture 13 (26)\nWe shall use the expression \u201ccomputable function\u201d to mean a function\ncalculable by a [Turing] machine, and let \u201ceffectively calculable\u201d refer to\nthe intuitive idea without particular identification with any one of these\ndefinitions (Turing, Systems of Logic Based on Ordinals).\n! Church-Turing thesis/conjecture:\nEvery effectively calculable function is also computable by a Turing machine,\n\u25b7 I.e. the conjecture is that, if we can compute something using any kind of\nmodel of computation, then we can also compute it using a Turing machine\n(or \u00b5-recursion or lambda calculus or register machines or ...).\n\u25b7 We will restrict ourselves here to two important classic models\n\u25b7 Turing machines: intentionally simplistic model of a computer, simpler than\nvon Neumann architecture (CPU). Allows to precisely define the behavior in\ncontrast to most CPUs/programming languages (cf. the JVM/byte code).",
      "\u25b7 We will restrict ourselves here to two important classic models\n\u25b7 Turing machines: intentionally simplistic model of a computer, simpler than\nvon Neumann architecture (CPU). Allows to precisely define the behavior in\ncontrast to most CPUs/programming languages (cf. the JVM/byte code).\n\u25b7 Grammars/string rewriting systems: can be understood as a formalization of\ncalculating by means of rewriting expressions.\nand show that the two can simulate each other.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: alphabets and words 15 (28)\nD [n] = {1, 2, . . . , n} for n \u2208 N 0 with [0] = \u2205; Z n := {0, 1, . . . , n\u2212 1}.\nD An alphabet A is a set so that a tuple (a1, . . . , an) (ai \u2208 A) can be uniquely\nrecovered from the word a1 . . . an with \u03b5 := () the empty word (also, \u03bb for ()).\n\u25b7 e.g. {0, 1}, {00, 01, 10, 11}, {0, 10, 11} are alphabets; but not {1, 01, 011}.\n\u25b7 e.g. {0, 1, . . . ,10, . . . ,15} vs. {0, 1, . . . , A, . . . , F}\n\u25b7 e.g. {A, B, C, . . . , Z} and {\u2227, \u2228, \u2192, (, )} and {if, while, do} are alphabets.\n\u25b7 Also often \u03a3 (\u201csymbol\u201d) or \u0393 (\u03b3\u03c1\u03b1\u03d5\u03c9,\u201cgrapheme\u201d) for alphabets.\nD If A is an alphabet, then (tuple as word) :\nA0 = {\u03b5} An = {a1 . . . an | ai \u2208 A for all i \u2208 [n]} A\u2217 =\n[\ni\u2208N 0\nAi\n|u|: length of the word (as tuple) u \u2208 A\u2217 with |u| \u2208N 0 and |\u03b5| = 0\nuv: concatenation of two words u, v\u2208 A\u2217\nD (A\u2217, \u25e6, \u03b5) with u \u25e6 v := uv is also called the free monoid generated by A\n(Concatenation of words u \u25e6 v = uv is associative with \u03b5 the neutral element.)",
      "[\ni\u2208N 0\nAi\n|u|: length of the word (as tuple) u \u2208 A\u2217 with |u| \u2208N 0 and |\u03b5| = 0\nuv: concatenation of two words u, v\u2208 A\u2217\nD (A\u2217, \u25e6, \u03b5) with u \u25e6 v := uv is also called the free monoid generated by A\n(Concatenation of words u \u25e6 v = uv is associative with \u03b5 the neutral element.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "cf. Phoenician alphabet (image source) and history of alphabets\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: (formal) languages 17 (30)\nD L \u2286 A\u2217 is called a language; if L, L\u2032 \u2286 A\u2217 are languages, then\n\u25b7 LL\u2032 = {uv | u \u2208 L, v\u2208 L\u2032} concatenation of two languages L, L\u2032 \u2286 A\u2217\n\u25b7 Natural powers of a language:\nL0 := {\u03b5} Lk+1 := LLk = {w(1) . . . w(k+1) | w(1), . . . , w(k+1) \u2208 L}(k \u2208 N 0)\nKleene star/closure (finite, but unbounded repetition of words in L)\nL\u2217 :=\n[\nk\u2208N 0\nLk = {\u03b5, w(1) . . . w(k) | k \u2208 N , w(1), . . . , w(k) \u2208 L}\n\u25b7 Example: Let L := {a, ba} and L\u2032 := {b, ab}.\n\u25b7 LL\u2032 = {a b, a ab, ba b, ba b}\n\u25b7 L\u2032L = {b a, b ba, ab a, ab ba}\n\u25b7 L\u2217 = {\u03b5} \u222a {a, ba} \u222a {a a, a ba, ba a, ba ba} \u222a. . .\n\u25b7 (L\u2032)\u2217 = {\u03b5} \u222a {b, ab} \u222a {b b, b ab, ab b, ab ab} \u222a. . .\n! \u2205\u2217 = {\u03b5}\n\u25b7 Cf. expansion of (a + ba)(b + ab) w/o commutativity.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: (formal) languages 17 (31)\nD L \u2286 A\u2217 is called a language; if L, L\u2032 \u2286 A\u2217 are languages, then\n\u25b7 LL\u2032 = {uv | u \u2208 L, v\u2208 L\u2032} concatenation of two languages L, L\u2032 \u2286 A\u2217\n\u25b7 Natural powers of a language:\nL0 := {\u03b5} Lk+1 := LLk = {w(1) . . . w(k+1) | w(1), . . . , w(k+1) \u2208 L}(k \u2208 N 0)\nKleene star/closure (finite, but unbounded repetition of words in L)\nL\u2217 :=\n[\nk\u2208N 0\nLk = {\u03b5, w(1) . . . w(k) | k \u2208 N , w(1), . . . , w(k) \u2208 L}\nD Let A be an alphabet: (2A\u2217\n, \u222a, \u25e6, \u2205, {\u03b5}) with L \u25e6 L\u2032 := LL\u2032 is also called the\nlanguage semiring or free semiring generated by A:\n\u25b7 Union of languages L \u222a L\u2032 is associative, commutative with neutral element \u2205.\nConcatenation of languages L \u25e6 L\u2032 := LL\u2032 is associative with {\u03b5} the neutral\nelement; distributes over union; and \u2205 \u201cannihilates\u201d:\nL(L\u2032L\u2032\u2032) = (LL\u2032)L\u2032\u2032, L{\u03b5} = L = {\u03b5}L, L(L\u2032 \u222a L\u2032\u2032) = LL\u2032 \u222a LL\u2032\u2032,\n(L\u2032 \u222a L\u2032\u2032)L = L\u2032L \u222a L\u2032\u2032L, L\u2205 = \u2205 = \u2205L.\n! In general: LL\u2032 \u0338= L\u2032L, L(L\u2032 \u2229 L\u2032\u2032) \u228a LL\u2032 \u2229 LL\u2032\u2032.",
      "Concatenation of languages L \u25e6 L\u2032 := LL\u2032 is associative with {\u03b5} the neutral\nelement; distributes over union; and \u2205 \u201cannihilates\u201d:\nL(L\u2032L\u2032\u2032) = (LL\u2032)L\u2032\u2032, L{\u03b5} = L = {\u03b5}L, L(L\u2032 \u222a L\u2032\u2032) = LL\u2032 \u222a LL\u2032\u2032,\n(L\u2032 \u222a L\u2032\u2032)L = L\u2032L \u222a L\u2032\u2032L, L\u2205 = \u2205 = \u2205L.\n! In general: LL\u2032 \u0338= L\u2032L, L(L\u2032 \u2229 L\u2032\u2032) \u228a LL\u2032 \u2229 LL\u2032\u2032.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Alphabets and encoding 18 (32)\n\u25b7 Unary alphabets (e.g. {|}) can represent a single natural number (cf. tally):\n\u25b7 Using G\u00a8odel\u2019s encoding any finite list of natural numbers can be encoded:\n(n1, . . . , nk) 7\u2192 2n1 3n2 5n3 \u00b7\u00b7\u00b7 pnk\nk\nIdentifying \u03a3 with [|\u03a3|], every finite word can thus be encoded in unary, e.g.:\nTHEO 7\u2192 (20, 8, 5, 15) 7\u2192 |2203855715\n\u2265 |1026\n(1 = 2030 . . .encodes \u03b5.)\n\u25b7 Alphabets \u03a3 with at least two symbols ( |\u03a3| > 1) allow for exponentially\nsmaller representations (recall logb n = log2 b \u00b7 log2 n):\n111111111111111111111111111111111111111111 in unary\nvs 42 in decimal (MSDF)\nvs 010101 in binary (LSBF)\nvs 0211 in ternary (LSDF)\nvs 2A in hexadecimal (MSDF).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Alphabets and encoding 18 (33)\n\u25b7 Consider a binary alphabet like {0, 1} or {a, b}:\n\u25b7 Given some alphabet \u03a3 = {a1, . . . , ak} choose m s.t. k \u2264 2m.\nE.g. m = 1 + \u230alog2 k\u230b\n\u25b7 Fix some injective map c from \u03a3 to {0, 1}m, e.g. use least-significant bit first:\nc(a1) := 000 . . .0 c(a2) := 100 . . .0 c(a3) := 010 . . .0 . . .\nGiven w = w1 . . . wl \u2208 \u03a3\u2217 map (encode) it to c(w1) . . . c(wl).\n(I.e. c is extended to a homomorphism.)\nWe then have |c(w)| = m|w|, i.e. the length is only increased by a constant\nfactor; and we can recover w by splitting c(w) into blocks of m bits.\n\u25b7 A typical example is ASCII.\n\u25b7 For more efficient encodings see Base64 and UTF8.\n\u25b7 At the end of the lecture we will discuss how to come up with (prefix-free)\nencodings that minimize the expected length.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: binary relations and reflexive-transitive closure (*) 19 (34)\n! The symbol \u201c \u2217\u201d is also used for denoting the reflexive-transitive closure of a\nbinary relation R \u2286 X \u00d7 X as the underlying concept is the same:\n\u25b7 Product/composition and powers for binary relations R, S\u2286 X \u00d7 X\nRS = {(x, z) | (x, y) \u2208 R, (y, z) \u2208 S}\nR0 := Id = {(x, x) | x \u2208 X} Rk+1 := RRk (k \u2208 N 0)\ni.e.: (x, z) \u2208 Rk iff there exist y1, . . . , yk\u22121 \u2208 X s.t. xRy1R . . . Ryk\u22121Rz.\nIf we visualize R as edge relation of the directed graph (X, R) with nodes X,\nwe have: (x, z) \u2208 Rk iff there is an x-z-path of length exactly k wrt. R.\n\u25b7 The reflexive-transitive closure resp. transitive closure of R is then\nR\u2217 :=\n[\nk\u2208N 0\nRk resp. R+ := RR\u2217 =\n[\nk\u2208N\nRk\ni.e.: (x, z) \u2208 R\u2217 iff there is some finite x-z-path wrt. R iff x can reach z.\n! Typically: \u2212 \u2192for \u201c(binary) successor/step relation\u201d, \u03a3, \u0393 for alphabets.\n\u25b7 \u2212 \u2192\u2217: reflexive-transitive closure (\u201creachable in finite, but unbounded time\u201d)\n\u25b7 \u03a3\u2217, \u0393\u2217, L\u2217: Kleene closure (for L \u2286 \u03a3\u2217)",
      "[\nk\u2208N\nRk\ni.e.: (x, z) \u2208 R\u2217 iff there is some finite x-z-path wrt. R iff x can reach z.\n! Typically: \u2212 \u2192for \u201c(binary) successor/step relation\u201d, \u03a3, \u0393 for alphabets.\n\u25b7 \u2212 \u2192\u2217: reflexive-transitive closure (\u201creachable in finite, but unbounded time\u201d)\n\u25b7 \u03a3\u2217, \u0393\u2217, L\u2217: Kleene closure (for L \u2286 \u03a3\u2217)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: Kozen\u2019s characterization of R\u2217 (*) 20 (35)\n\u25b7 For now just a reminder: see DS and also later regular expressions again.\nT For all relations R, S, X\u2286 A \u00d7 A (cf. DS, Kozen):\n\u25b7 R\u2217 = IdA \u222a R R\u2217 = IdA \u222a R\u2217 R.\n\u25b7 If S \u222a R X\u2286 X, then R\u2217 S \u2286 X.\n\u25b7 If S \u222a X R\u2286 X, then S R\u2217 \u2286 X.\nC For all R, S\u2286 A \u00d7 A:\n! R\u2217S is the \u2286-least solution of S \u222a R X= X.\nS R\u2217 is the \u2286-least solution of S \u222a X R= X.\nD R+ := RR\u2217 is the transitive closure of R.\n\u25b7 R\u2217 = R\u2217R\u2217 = (R\u2217)\u2217 = (R+)\u2217 = (R\u2217)+\n\u25b7 If R \u2286 S, then R\u2217 \u2286 S\u2217 and R+ \u2286 S+.\n\u25b7 (R \u222a S)\u2217 = R\u2217(SR\u2217)\u2217 = (R\u2217S)\u2217R\u2217\n\u25b7 If RX = XS, then R\u2217X = XS\u2217.\n(Proof by induction, see DS.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From languages to relations and back again 21 (36)\n\u25b7 The use of \u2217 for both languages and relations can be motivated as follows:\nLet \u03a3 be an alphabet, and define\nR\u03b5 := Id\u03a3\u2217 = {(w, w) | w \u2208 \u03a3\u2217} Ra := {(w, wa) | a \u2208 w\u03a3\u2217}\nand extend this to words w, w\u2032 \u2208 \u03a3\u2217 and languages L \u2286 \u03a3\u2217 by\nRww\u2032 := RwRw\u2032 RL :=\n[\nw\u2208L\nRw R{w} = Rw\nWe can recover L from its representations as relation:\nL = {w | (\u03b5, w) \u2208 RL} = \u03b5RL\nWe now have that:\nRL\u2217 = R\u2217\nL\n(i.e. h(L) := RL is an injective homomorphism from the language semiring to the semiring\nof binary relations that is also compatible with \u2217; cf. also C \u2192 R 2\u00d72.)\n\u25b7 Hence, all properties of \u2217 for binary relations carry over to languages, e.g.:\n(L\u2217)\u2217 = L\u2217 L\u2217L\u2217 = L\u2217 (L \u222a L\u2032)\u2217 = (L\u2217L\u2032)\u2217L\u2217 = L\u2217(L\u2032L\u2217)\u2217\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Alphabets as trees 22 (37)\n\u25b7 Any finite alphabet \u03a3 can be visualized as infinite |\u03a3|-ary tree (\u03a3\u2217, R\u03a3).\nFor example \u03a3 = {a, b} leads to the infinite binary tree:\n\u03b5\na b\naa ab ba bb\naaa aab aba abb baa bab bba bbb\n(which is the Cayley graph of the free monoid \u03a3\u2217 wrt. \u03a3 as generators.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "3 Turing machines, computable functions, semidecidable languages\nTuring machines\nComputable and non-computable functions\nSemidecidable/recognizable/recursively enumerable languages\nHalting problem, Rice\u2019s theorem, deciding properties of computable functions\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Turing machines: informally 25 (40)\n\u25b7 Proposed by Alan Turing as one model/definition of computation in 1936.\nIntentionally very simplistic in order to simplify proofs/arguments.\n\u25b7 In terms of the von Neumann architecture (1945):\n\u25b7 Memory unit, input and output device all captured by the notion ot tape.\nA tape is an infinite tape drive.\nA tape consists of countably infinitely many (atomic memory) cells.\nIn the simplest case just one single tape initially containing just the input.\nOnly the cell at the current position of the (tape) head can be read.\n\u25b7 Control unit and arithemtic unit captured by states and transition rules.\n! In contrast to a real physical computer,\n\u25b7 a transition rules may be nondeterministic, i.e. several rules might be\napplicable.\n\u25b7 the tapes of a Turing machine are unbounded, i.e. a Turing machine has\nunbounded memory, but in finite time it can only access a finite amount of it.",
      "! In contrast to a real physical computer,\n\u25b7 a transition rules may be nondeterministic, i.e. several rules might be\napplicable.\n\u25b7 the tapes of a Turing machine are unbounded, i.e. a Turing machine has\nunbounded memory, but in finite time it can only access a finite amount of it.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Turing machines with a single tape 26 (41)\nD (one-tape) Turing machine (1TM) M = (Q, \u03a3, \u0393, \u03b4, q0, \u25a1, F)\n\u25b7 Q: finite set of states\n\u25b7 \u03a3: the finite input alphabet, e.g. \u03a3 = {0, 1}\n\u25b7 \u0393: finite tape alphabet with \u03a3 \u2286 \u0393, e.g. \u0393 = {0, 1, \u25a1}\n\u25b7 \u25a1: the blank symbol with \u25a1 \u2208 \u0393 \\ \u03a3 (\u201cnone\u201d)\n\u25b7 q0: the initial state with q0 \u2208 Q\n\u25b7 F: the final states with F \u2286 Q\n\u25b7 \u03b4: the transition rules with \u03b4 \u2286 (Q \\ F \u00d7 \u0393) \u00d7 (Q \u00d7 \u0393 \u00d7 {\u22121, 0, +1})\nA transition rule ((q, a), (r, b, d)) \u2208 \u03b4 stands for the instruction:\nif state == \u2019q\u2019 and tape[pos] == \u2019a\u2019:\nstate = \u2019r\u2019 # update state\ntape[pos] = \u2019b\u2019 # update tape\npos += d # move tape head\nA 1TM is deterministic (D1TM) if |(q, a)\u03b4| \u22641 for every (q, a) \u2208 Q \u00d7 \u0393,\notherwise it is nondeterministic.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A nondeterministic 1TM 27 (42)\n\u03a3 = {a, b}\n\u0393 = \u03a3 \u222a {\u25a1}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA \u25a1 A \u25a1 0\nA a A a +1\nA a B a +1\nq a r b d\nB \u25a1 B \u25a1 0\nB a B a 0\nB b H b 0\nAstart B H\n\u25a1: \u25a1/0\na: a/+ 1\na: a/+ 1\n\u25a1: \u25a1/0\na: a/+ 1\nb: b/0\nA\n. . .\u25a1 a a b \u25a1 . . .\nAaab\n\u25b7 Accepted language: L(M) = {akabw | k \u2208 N 0, w\u2208 {a, b}\u2217}\n\u25b7 A nondeterministic 1TM can have multiple maximal runs for the same input.\nThe maximal runs can be illustrated as a directed (not necessarily finite) graph\non the configurations.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A nondeterministic 1TM 27 (43)\n\u03a3 = {a, b}\n\u0393 = \u03a3 \u222a {\u25a1}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA \u25a1 A \u25a1 0\nA a A a +1\nA a B a +1\nq a r b d\nB \u25a1 B \u25a1 0\nB a B a 0\nB b H b 0\nAstart B H\n\u25a1: \u25a1/0\na: a/+ 1\na: a/+ 1\n\u25a1: \u25a1/0\na: a/+ 1\nb: b/0\nA\n. . .\u25a1 a a b \u25a1 . . .\nAaab \u2212 \u2192M aAab\n\u25b7 Accepted language: L(M) = {akabw | k \u2208 N 0, w\u2208 {a, b}\u2217}\n\u25b7 A nondeterministic 1TM can have multiple maximal runs for the same input.\nThe maximal runs can be illustrated as a directed (not necessarily finite) graph\non the configurations.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A nondeterministic 1TM 27 (44)\n\u03a3 = {a, b}\n\u0393 = \u03a3 \u222a {\u25a1}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA \u25a1 A \u25a1 0\nA a A a +1\nA a B a +1\nq a r b d\nB \u25a1 B \u25a1 0\nB a B a 0\nB b H b 0\nAstart B H\n\u25a1: \u25a1/0\na: a/+ 1\na: a/+ 1\n\u25a1: \u25a1/0\na: a/+ 1\nb: b/0\nB\n. . .\u25a1 a a b \u25a1 . . .\nAaab \u2212 \u2192M aAab \u2212 \u2192M aaBb\n\u25b7 Accepted language: L(M) = {akabw | k \u2208 N 0, w\u2208 {a, b}\u2217}\n\u25b7 A nondeterministic 1TM can have multiple maximal runs for the same input.\nThe maximal runs can be illustrated as a directed (not necessarily finite) graph\non the configurations.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A nondeterministic 1TM 27 (45)\n\u03a3 = {a, b}\n\u0393 = \u03a3 \u222a {\u25a1}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA \u25a1 A \u25a1 0\nA a A a +1\nA a B a +1\nq a r b d\nB \u25a1 B \u25a1 0\nB a B a 0\nB b H b 0\nAstart B H\n\u25a1: \u25a1/0\na: a/+ 1\na: a/+ 1\n\u25a1: \u25a1/0\na: a/+ 1\nb: b/0\nH\n. . .\u25a1 a a b \u25a1 . . .\nAaab \u2212 \u2192M aAab \u2212 \u2192M aaBb \u2212 \u2192M aaHb\n\u25b7 Accepted language: L(M) = {akabw | k \u2208 N 0, w\u2208 {a, b}\u2217}\n\u25b7 A nondeterministic 1TM can have multiple maximal runs for the same input.\nThe maximal runs can be illustrated as a directed (not necessarily finite) graph\non the configurations.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A nondeterministic 1TM 27 (46)\n\u03a3 = {a, b}\n\u0393 = \u03a3 \u222a {\u25a1}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA \u25a1 A \u25a1 0\nA a A a +1\nA a B a +1\nq a r b d\nB \u25a1 B \u25a1 0\nB a B a 0\nB b H b 0\nAstart B H\n\u25a1: \u25a1/0\na: a/+ 1\na: a/+ 1\n\u25a1: \u25a1/0\na: a/+ 1\nb: b/0\nAaab aAab\naBab\naaAb\naaBb aaHb\nA\u25a1\nAb\nAaa\u25a1 aAa\u25a1\naBa\u25a1\naaA\u25a1\naaB\u25a1\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM: configuration, run 28 (47)\nD Let M = (Q, \u03a3, \u0393, \u03b4, q0, \u25a1, F) be a 1TM. wlog Q \u2229 \u0393 = \u2205.\nA configuration\n\u03b1q\u03b2 := (\u03b1, q, \u03b2) \u2208 \u0393\u2217 \u00d7 Q \u00d7 \u0393\u2217\nof a 1TM describes the current tape content and the position of the head:\nThe current tape content is . . .\u25a1\u25a1\u03b1\u03b2\u25a1\u25a1. . .\nThe currently read symbol is the first symbol of \u03b2\u25a1\u25a1. . .\ni.e. \u03b1\u03b2 \u2208 \u0393\u2217 is implicitly extended in both directions by infinitely many blanks;\nthus, if \u03b2 = \u03b5, then the 1TM currently reads a blank \u25a1.\n! \u03b1q\u03b2 and \u25a1. . .\u25a1\u03b1q\u03b2\u25a1. . .\u25a1 actually represent the same situation.\nUsually convention: drop trailing and leading blanks ( \u25a1).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM: configuration, run 28 (48)\nD Let M = (Q, \u03a3, \u0393, \u03b4, q0, \u25a1, F) be a 1TM. wlog Q \u2229 \u0393 = \u2205.\nThe rules \u03b4 generate the binary relation \u2212 \u2192M on \u0393\u2217 \u00d7 Q \u00d7 \u0393\u2217:\n\u2212 \u2192M := {(\u03b1qa\u03b2, \u03b1br\u03b2) | ((q, a), (r, b,+1)) \u2208 \u03b4, \u03b1, \u03b2\u2208 \u0393\u2217}\n\u222a { (\u03b1zqa\u03b2, \u03b1rzb\u03b2) | ((q, a), (r, b,\u22121)) \u2208 \u03b4, \u03b1, \u03b2\u2208 \u0393\u2217, z\u2208 \u0393}\n\u222a { (\u03b1qa\u03b2, \u03b1rb\u03b2) | ((q, a), (r, b,0)) \u2208 \u03b4, \u03b1, \u03b2\u2208 \u0393\u2217}\nA run of the 1TM is a path \u03b10q0\u03b20 \u2212 \u2192M \u03b11q1\u03b21 \u2212 \u2192M . . .wrt. \u2212 \u2192M .\n\u25b7 \u201crun\u201d is just another word for \u201ccomputation/calculation of the 1TM\u201d.\n\u25b7 A run on input x1 . . . xl \u2208 \u03a3l is a path wrt. \u2212 \u2192M starting at q0x1 . . . xl\n\u25b7 A run can be finite or infinite; a finite run is maximal if it cannot further be\nextended (dead end reached); an infinite run is always maximal.\n\u25b7 \u03b4 is deterministic iff every configuration has at most one successor wrt. \u2212 \u2192M\niff for every configuration there is exactly one maximal run.\n\u25b7 Wrt. \u2212 \u2192M we can think of \u0393\u2217 \u00d7 Q \u00d7 \u0393\u2217 as the infinite directed graph of all\ncalculations (semantics of the 1TM). \u2212 \u2192\u2217\nM is the reflexive-transitive closure.",
      "iff for every configuration there is exactly one maximal run.\n\u25b7 Wrt. \u2212 \u2192M we can think of \u0393\u2217 \u00d7 Q \u00d7 \u0393\u2217 as the infinite directed graph of all\ncalculations (semantics of the 1TM). \u2212 \u2192\u2217\nM is the reflexive-transitive closure.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM: configuration, run 28 (49)\nD Let M = (Q, \u03a3, \u0393, \u03b4, q0, \u25a1, F) be a 1TM. wlog Q \u2229 \u0393 = \u2205.\nThe 1TM terminates/halts on a given input w \u2208 \u03a3\u2217 if all maximal runs\nstarting in q0w are finite.\nA run on input w \u2208 \u03a3\u2217 is accepting if it ends in a configuration \u0393\u2217 \u00d7F \u00d7\u0393\u2217.\n\u25b7 By definition, a run that visits a configuration \u0393\u2217 \u00d7 F \u00d7 \u0393\u2217 is always finite as\nwe require \u03b4 \u2286 (Q \\ F \u00d7 \u0393) \u00d7 (Q \u00d7 \u0393 \u00d7 {\u22121, 0, +1}).\n\u25b7 We can always let an 1TM empty its tape before entering a final state.\n\u25b7 Analogously, we can turn a \u201cdead end\u201d into a \u201cdeadlock\u201d:\nIf (q, a)\u03b4 = \u2205 and q \u0338\u2208 F, add ((q, a), (q, a,0)) to \u03b4 (loop forever).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM: configuration, run 28 (50)\nD Let M = (Q, \u03a3, \u0393, \u03b4, q0, \u25a1, F) be a 1TM. wlog Q \u2229 \u0393 = \u2205.\nThe 1TM accepts the input w \u2208 \u03a3\u2217 if there is at least one accepting run.\nThe language accepted/recognized by the 1TM M is\nL(M) := {w \u2208 \u03a3\u2217 | q0w \u2212 \u2192\u2217\nM \u03b1qf \u03b2, qf \u2208 F}\nAn accepting run on w can be thought of as a proof/certificate that\nw \u2208 L(M).\nIf M is nondeterministic, then there might be multiple runs on input w: but\nfor w \u2208 L(M) it suffices, if at least one of them is accepting. (Sometimes a\n\u201cproof attempt\u201d fails.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Busy beaver 29 (51)\n\u03a3 = \u2205\n\u0393 = \u03a3 \u222a {\u25a1, x}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA \u25a1 B x +1\nA x B x \u22121\nB \u25a1 A x \u22121\nB x H x +1\nAstart B H\u25a1: x/+ 1; x: x/\u22121\n\u25a1: x/\u22121\nx: x/+ 1\nA\n. . .\u25a1 \u25a1 \u25a1 \u25a1 \u25a1 \u25a1 \u25a1 \u25a1 . . .\n\u25a1\u25a1A\u25a1\u25a1\n! \u03b4 is often read as labeled edges between the states Q.\n\u25b7 A (standard) busy beaver is a deterministic 1TM that takes no input (only\nblanks), terminates eventually (only a finite maximal run), tries to write a\nmaximal number of xs (or any other symbol except \u25a1).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Busy beaver 29 (52)\n\u03a3 = \u2205\n\u0393 = \u03a3 \u222a {\u25a1, x}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA \u25a1 B x +1\nA x B x \u22121\nB \u25a1 A x \u22121\nB x H x +1\nAstart B H\u25a1: x/+ 1; x: x/\u22121\n\u25a1: x/\u22121\nx: x/+ 1\nB\n. . .\u25a1 \u25a1 \u25a1 x \u25a1 \u25a1 \u25a1 \u25a1 . . .\n\u25a1\u25a1A\u25a1\u25a1 \u2212 \u2192M \u25a1\u25a1xB\u25a1\n! \u03b4 is often read as labeled edges between the states Q.\n\u25b7 A (standard) busy beaver is a deterministic 1TM that takes no input (only\nblanks), terminates eventually (only a finite maximal run), tries to write a\nmaximal number of xs (or any other symbol except \u25a1).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Busy beaver 29 (53)\n\u03a3 = \u2205\n\u0393 = \u03a3 \u222a {\u25a1, x}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA \u25a1 B x +1\nA x B x \u22121\nB \u25a1 A x \u22121\nB x H x +1\nAstart B H\u25a1: x/+ 1; x: x/\u22121\n\u25a1: x/\u22121\nx: x/+ 1\nA\n. . .\u25a1 \u25a1 \u25a1 x x \u25a1 \u25a1 \u25a1 . . .\n\u25a1\u25a1A\u25a1\u25a1 \u2212 \u2192M \u25a1\u25a1xB\u25a1 \u2212 \u2192M \u25a1\u25a1Axx\n! \u03b4 is often read as labeled edges between the states Q.\n\u25b7 A (standard) busy beaver is a deterministic 1TM that takes no input (only\nblanks), terminates eventually (only a finite maximal run), tries to write a\nmaximal number of xs (or any other symbol except \u25a1).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Busy beaver 29 (54)\n\u03a3 = \u2205\n\u0393 = \u03a3 \u222a {\u25a1, x}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA \u25a1 B x +1\nA x B x \u22121\nB \u25a1 A x \u22121\nB x H x +1\nAstart B H\u25a1: x/+ 1; x: x/\u22121\n\u25a1: x/\u22121\nx: x/+ 1\nB\n. . .\u25a1 \u25a1 \u25a1 x x \u25a1 \u25a1 \u25a1 . . .\n\u25a1\u25a1A\u25a1\u25a1 \u2212 \u2192M \u25a1\u25a1xB\u25a1 \u2212 \u2192M \u25a1\u25a1Axx \u2212 \u2192M \u25a1B\u25a1xx\n! \u03b4 is often read as labeled edges between the states Q.\n\u25b7 A (standard) busy beaver is a deterministic 1TM that takes no input (only\nblanks), terminates eventually (only a finite maximal run), tries to write a\nmaximal number of xs (or any other symbol except \u25a1).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Busy beaver 29 (55)\n\u03a3 = \u2205\n\u0393 = \u03a3 \u222a {\u25a1, x}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA \u25a1 B x +1\nA x B x \u22121\nB \u25a1 A x \u22121\nB x H x +1\nAstart B H\u25a1: x/+ 1; x: x/\u22121\n\u25a1: x/\u22121\nx: x/+ 1\nA\n. . .\u25a1 \u25a1 x x x \u25a1 \u25a1 \u25a1 . . .\n\u25a1\u25a1A\u25a1\u25a1 \u2212 \u2192M \u25a1\u25a1xB\u25a1 \u2212 \u2192M \u25a1\u25a1Axx \u2212 \u2192M \u25a1B\u25a1xx \u2212 \u2192M A\u25a1xxx\n! \u03b4 is often read as labeled edges between the states Q.\n\u25b7 A (standard) busy beaver is a deterministic 1TM that takes no input (only\nblanks), terminates eventually (only a finite maximal run), tries to write a\nmaximal number of xs (or any other symbol except \u25a1).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Busy beaver 29 (56)\n\u03a3 = \u2205\n\u0393 = \u03a3 \u222a {\u25a1, x}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA \u25a1 B x +1\nA x B x \u22121\nB \u25a1 A x \u22121\nB x H x +1\nAstart B H\u25a1: x/+ 1; x: x/\u22121\n\u25a1: x/\u22121\nx: x/+ 1\nB\n. . .\u25a1 x x x x \u25a1 \u25a1 \u25a1 . . .\n\u25a1\u25a1A\u25a1\u25a1 \u2212 \u2192M \u25a1\u25a1xB\u25a1 \u2212 \u2192M \u25a1\u25a1Axx \u2212 \u2192M \u25a1B\u25a1xx \u2212 \u2192M A\u25a1xxx \u2212 \u2192M xBxxx\n! \u03b4 is often read as labeled edges between the states Q.\n\u25b7 A (standard) busy beaver is a deterministic 1TM that takes no input (only\nblanks), terminates eventually (only a finite maximal run), tries to write a\nmaximal number of xs (or any other symbol except \u25a1).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Busy beaver 29 (57)\n\u03a3 = \u2205\n\u0393 = \u03a3 \u222a {\u25a1, x}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA \u25a1 B x +1\nA x B x \u22121\nB \u25a1 A x \u22121\nB x H x +1\nAstart B H\u25a1: x/+ 1; x: x/\u22121\n\u25a1: x/\u22121\nx: x/+ 1\nH\n. . .\u25a1 x x x x \u25a1 \u25a1 \u25a1 . . .\n\u25a1\u25a1A\u25a1\u25a1 \u2212 \u2192M \u25a1\u25a1xB\u25a1 \u2212 \u2192M \u25a1\u25a1Axxx \u2212 \u2192M \u25a1B\u25a1xx \u2212 \u2192M A\u25a1xxx \u2212 \u2192M xBxxx \u2212 \u2192M xxHxx\n! \u03b4 is often read as labeled edges between the states Q.\n\u25b7 A (standard) busy beaver is a deterministic 1TM that takes no input (only\nblanks), terminates eventually (only a finite maximal run), tries to write a\nmaximal number of xs (or any other symbol except \u25a1).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing \u201c +1\u201d in LSBF 30 (58)\n\u03a3 = {0, 1}\n\u0393 = \u03a3 \u222a {\u25a1}\nQ = {inc, H}\nF = {H} q0 = inc\nq a r b d\ninc \u25a1 H \u25a1 1\ninc 0 H 1 0\ninc 1 inc 0 +1\nincstart H\n1: 0/+ 1\n0: 1/0; \u25a1: \u25a1/1\ninc\n. . .\u25a1 1 1 1 0 1 \u25a1 . . .\ninc11101\n\u25b7 Usually, we are interested in deterministic TMs (programs/algorithms).\n\u25b7 Using e.g. least-significant-bit-first representation, e.g. J11K10 = J1101K2 and\nJ12K10 = J0011K2, TMs can implement arithmetic operations. (Adapt the 1TM\nto MSBF.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing \u201c +1\u201d in LSBF 30 (59)\n\u03a3 = {0, 1}\n\u0393 = \u03a3 \u222a {\u25a1}\nQ = {inc, H}\nF = {H} q0 = inc\nq a r b d\ninc \u25a1 H \u25a1 1\ninc 0 H 1 0\ninc 1 inc 0 +1\nincstart H\n1: 0/+ 1\n0: 1/0; \u25a1: \u25a1/1\ninc\n. . .\u25a1 0 1 1 0 1 \u25a1 . . .\ninc11101 \u2212 \u2192M 0inc1101\n\u25b7 Usually, we are interested in deterministic TMs (programs/algorithms).\n\u25b7 Using e.g. least-significant-bit-first representation, e.g. J11K10 = J1101K2 and\nJ12K10 = J0011K2, TMs can implement arithmetic operations. (Adapt the 1TM\nto MSBF.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing \u201c +1\u201d in LSBF 30 (60)\n\u03a3 = {0, 1}\n\u0393 = \u03a3 \u222a {\u25a1}\nQ = {inc, H}\nF = {H} q0 = inc\nq a r b d\ninc \u25a1 H \u25a1 1\ninc 0 H 1 0\ninc 1 inc 0 +1\nincstart H\n1: 0/+ 1\n0: 1/0; \u25a1: \u25a1/1\ninc\n. . .\u25a1 0 0 1 0 1 \u25a1 . . .\ninc11101 \u2212 \u2192M 0inc1101 \u2212 \u2192M 00inc101\n\u25b7 Usually, we are interested in deterministic TMs (programs/algorithms).\n\u25b7 Using e.g. least-significant-bit-first representation, e.g. J11K10 = J1101K2 and\nJ12K10 = J0011K2, TMs can implement arithmetic operations. (Adapt the 1TM\nto MSBF.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing \u201c +1\u201d in LSBF 30 (61)\n\u03a3 = {0, 1}\n\u0393 = \u03a3 \u222a {\u25a1}\nQ = {inc, H}\nF = {H} q0 = inc\nq a r b d\ninc \u25a1 H \u25a1 1\ninc 0 H 1 0\ninc 1 inc 0 +1\nincstart H\n1: 0/+ 1\n0: 1/0; \u25a1: \u25a1/1\ninc\n. . .\u25a1 0 0 0 0 1 \u25a1 . . .\ninc11101 \u2212 \u2192M 0inc1101 \u2212 \u2192M 00inc101 \u2212 \u2192M 000inc01\n\u25b7 Usually, we are interested in deterministic TMs (programs/algorithms).\n\u25b7 Using e.g. least-significant-bit-first representation, e.g. J11K10 = J1101K2 and\nJ12K10 = J0011K2, TMs can implement arithmetic operations. (Adapt the 1TM\nto MSBF.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing \u201c +1\u201d in LSBF 30 (62)\n\u03a3 = {0, 1}\n\u0393 = \u03a3 \u222a {\u25a1}\nQ = {inc, H}\nF = {H} q0 = inc\nq a r b d\ninc \u25a1 H \u25a1 1\ninc 0 H 1 0\ninc 1 inc 0 +1\nincstart H\n1: 0/+ 1\n0: 1/0; \u25a1: \u25a1/1\nH\n. . .\u25a1 0 0 0 1 1 \u25a1 . . .\ninc11101 \u2212 \u2192M 0inc1101 \u2212 \u2192M 00inc101 \u2212 \u2192M 000inc01 \u2212 \u2192M 000H11\n\u25b7 Usually, we are interested in deterministic TMs (programs/algorithms).\n\u25b7 Using e.g. least-significant-bit-first representation, e.g. J11K10 = J1101K2 and\nJ12K10 = J0011K2, TMs can implement arithmetic operations. (Adapt the 1TM\nto MSBF.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing \u201c +\u201d in LSBF 31 (63)\n\u03a3 =\nn\n\u00140\n0\n\u0015\n,\n\u00141\n0\n\u0015\n,\n\u00140\n1\n\u0015\n,\n\u00141\n1\n\u0015\no\n\u0393 = \u03a3 \u222a {\u25a1, 0, 1} Q = {add0, add1, H}F = {H}q0 = add0\nadd0start add1\nH\n\u00140\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 1/+ 1;\n\u00141\n0\n\u0015\n: 1/+ 1;\n\u00141\n1\n\u0015\n: 0/+ 1\n\u25a1: \u25a1/0\n\u00141\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 0/+ 1;\n\u00141\n1\n\u0015\n: 1/+ 1;\n\u00140\n0\n\u0015\n: 1/+ 1\n\u25a1: 1/0\nadd0\n. . .\u25a1\n\u00140\n1\n\u0015 \u00141\n1\n\u0015 \u00141\n0\n\u0015 \u00141\n1\n\u0015\n\u25a1 \u25a1 . . .\nadd0\n\u00140\n1\n\u0015\u00141\n1\n\u0015\u00141\n0\n\u0015\u00141\n1\n\u0015\n\u25a1\n! Using e.g. vectors/tuples (and at the cost of additional \u201cbook-keeping\u201d and\nintermediate steps), a 1TM can also simulate multiple tapes (with independent\nheads). (cf. memory bus width, bit rate)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing \u201c +\u201d in LSBF 31 (64)\n\u03a3 =\nn\n\u00140\n0\n\u0015\n,\n\u00141\n0\n\u0015\n,\n\u00140\n1\n\u0015\n,\n\u00141\n1\n\u0015\no\n\u0393 = \u03a3 \u222a {\u25a1, 0, 1} Q = {add0, add1, H}F = {H}q0 = add0\nadd0start add1\nH\n\u00140\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 1/+ 1;\n\u00141\n0\n\u0015\n: 1/+ 1;\n\u00141\n1\n\u0015\n: 0/+ 1\n\u25a1: \u25a1/0\n\u00141\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 0/+ 1;\n\u00141\n1\n\u0015\n: 1/+ 1;\n\u00140\n0\n\u0015\n: 1/+ 1\n\u25a1: 1/0\nadd0\n. . .\u25a1 1\n\u00141\n1\n\u0015 \u00141\n0\n\u0015 \u00141\n1\n\u0015\n\u25a1 \u25a1 . . .\nadd0\n\u00140\n1\n\u0015\u00141\n1\n\u0015\u00141\n0\n\u0015\u00141\n1\n\u0015\n\u25a1 \u2212 \u2192M 1add0\n\u00141\n1\n\u0015\u00141\n0\n\u0015\u00141\n1\n\u0015\n\u25a1\n! Using e.g. vectors/tuples (and at the cost of additional \u201cbook-keeping\u201d and\nintermediate steps), a 1TM can also simulate multiple tapes (with independent\nheads). (cf. memory bus width, bit rate)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing \u201c +\u201d in LSBF 31 (65)\n\u03a3 =\nn\n\u00140\n0\n\u0015\n,\n\u00141\n0\n\u0015\n,\n\u00140\n1\n\u0015\n,\n\u00141\n1\n\u0015\no\n\u0393 = \u03a3 \u222a {\u25a1, 0, 1} Q = {add0, add1, H}F = {H}q0 = add0\nadd0start add1\nH\n\u00140\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 1/+ 1;\n\u00141\n0\n\u0015\n: 1/+ 1;\n\u00141\n1\n\u0015\n: 0/+ 1\n\u25a1: \u25a1/0\n\u00141\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 0/+ 1;\n\u00141\n1\n\u0015\n: 1/+ 1;\n\u00140\n0\n\u0015\n: 1/+ 1\n\u25a1: 1/0\nadd1\n. . .\u25a1 1 0\n\u00141\n0\n\u0015 \u00141\n1\n\u0015\n\u25a1 \u25a1 . . .\nadd0\n\u00140\n1\n\u0015\u00141\n1\n\u0015\u00141\n0\n\u0015\u00141\n1\n\u0015\n\u25a1 \u2212 \u2192M 1add0\n\u00141\n1\n\u0015\u00141\n0\n\u0015\u00141\n1\n\u0015\n\u25a1 \u2212 \u2192M 10add1\n\u00141\n0\n\u0015\u00141\n1\n\u0015\n\u25a1\n! Using e.g. vectors/tuples (and at the cost of additional \u201cbook-keeping\u201d and\nintermediate steps), a 1TM can also simulate multiple tapes (with independent\nheads). (cf. memory bus width, bit rate)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing \u201c +\u201d in LSBF 31 (66)\n\u03a3 =\nn\n\u00140\n0\n\u0015\n,\n\u00141\n0\n\u0015\n,\n\u00140\n1\n\u0015\n,\n\u00141\n1\n\u0015\no\n\u0393 = \u03a3 \u222a {\u25a1, 0, 1} Q = {add0, add1, H}F = {H}q0 = add0\nadd0start add1\nH\n\u00140\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 1/+ 1;\n\u00141\n0\n\u0015\n: 1/+ 1;\n\u00141\n1\n\u0015\n: 0/+ 1\n\u25a1: \u25a1/0\n\u00141\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 0/+ 1;\n\u00141\n1\n\u0015\n: 1/+ 1;\n\u00140\n0\n\u0015\n: 1/+ 1\n\u25a1: 1/0\nadd1\n. . .\u25a1 1 0 0\n\u00141\n1\n\u0015\n\u25a1 \u25a1 . . .\nadd0\n\u00140\n1\n\u0015\u00141\n1\n\u0015\u00141\n0\n\u0015\u00141\n1\n\u0015\n\u25a1 \u2212 \u2192M 1add0\n\u00141\n1\n\u0015\u00141\n0\n\u0015\u00141\n1\n\u0015\n\u25a1 \u2212 \u2192M 10add1\n\u00141\n0\n\u0015\u00141\n1\n\u0015\n\u25a1 \u2212 \u2192M 100add1\n\u00141\n1\n\u0015\n\u25a1\n! Using e.g. vectors/tuples (and at the cost of additional \u201cbook-keeping\u201d and\nintermediate steps), a 1TM can also simulate multiple tapes (with independent\nheads). (cf. memory bus width, bit rate)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing \u201c +\u201d in LSBF 31 (67)\n\u03a3 =\nn\n\u00140\n0\n\u0015\n,\n\u00141\n0\n\u0015\n,\n\u00140\n1\n\u0015\n,\n\u00141\n1\n\u0015\no\n\u0393 = \u03a3 \u222a {\u25a1, 0, 1} Q = {add0, add1, H}F = {H}q0 = add0\nadd0start add1\nH\n\u00140\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 1/+ 1;\n\u00141\n0\n\u0015\n: 1/+ 1;\n\u00141\n1\n\u0015\n: 0/+ 1\n\u25a1: \u25a1/0\n\u00141\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 0/+ 1;\n\u00141\n1\n\u0015\n: 1/+ 1;\n\u00140\n0\n\u0015\n: 1/+ 1\n\u25a1: 1/0\nadd1\n. . .\u25a1 1 0 0 1 \u25a1 \u25a1 . . .\nadd0\n\u00140\n1\n\u0015\u00141\n1\n\u0015\u00141\n0\n\u0015\u00141\n1\n\u0015\n\u25a1 \u2212 \u2192\u2217\nM 100add1\n\u00141\n1\n\u0015\n\u25a1 \u2212 \u2192M 1001add1\u25a1\n! Using e.g. vectors/tuples (and at the cost of additional \u201cbook-keeping\u201d and\nintermediate steps), a 1TM can also simulate multiple tapes (with independent\nheads). (cf. memory bus width, bit rate)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing \u201c +\u201d in LSBF 31 (68)\n\u03a3 =\nn\n\u00140\n0\n\u0015\n,\n\u00141\n0\n\u0015\n,\n\u00140\n1\n\u0015\n,\n\u00141\n1\n\u0015\no\n\u0393 = \u03a3 \u222a {\u25a1, 0, 1} Q = {add0, add1, H}F = {H}q0 = add0\nadd0start add1\nH\n\u00140\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 1/+ 1;\n\u00141\n0\n\u0015\n: 1/+ 1;\n\u00141\n1\n\u0015\n: 0/+ 1\n\u25a1: \u25a1/0\n\u00141\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 0/+ 1;\n\u00141\n1\n\u0015\n: 1/+ 1;\n\u00140\n0\n\u0015\n: 1/+ 1\n\u25a1: 1/0\nH\n. . .\u25a1 1 0 0 1 1 \u25a1 . . .\nadd0\n\u00140\n1\n\u0015\u00141\n1\n\u0015\u00141\n0\n\u0015\u00141\n1\n\u0015\n\u25a1 \u2212 \u2192\u2217\nM 1001add1\u25a1 \u2212 \u2192M 1001H1\n! Using e.g. vectors/tuples (and at the cost of additional \u201cbook-keeping\u201d and\nintermediate steps), a 1TM can also simulate multiple tapes (with independent\nheads). (cf. memory bus width, bit rate)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Multiple tapes 32 (69)\nq1\n. . .\u25a1 a b a b b \u25a1 . . .\nq2\n. . .\u25a1 b b b a b \u25a1 . . .\nqb,a\n. . .\u25a1\n\u0014a\nb\n\u0015\u0014q1b\nb\n\u0015\u0014a\nb\n\u0015\u0014 b\nq2a\n\u0015\u0014b\nb\n\u0015\n\u25a1 . . .\n\u25b7 Multiple tapes and multiple 1TMs can be simulated by means of a\nvector/tuple alphabet, and e.g. simulating one step of each tape/1TM using\nround-robin scheduling (at a quadratic runtime increase) .\nThus, a 1TM can simulate a Petri net (e.g. one tape per counter) .\n\u25b7 The central point is that, as \u0393 and Q can be any finite set, a 1TM can have\narbitrary finite \u201cbit width\u201d; see also the linear speedup theorem.\n\u25b7 Further, a TM M can \u201ccall\u201d a TM M\u2032 e.g. by simulating M\u2032 on an additional\ntape resp. an extended alphabet and waiting for M\u2032 to halt in a final state.\n(Recall that by definition a TM cannot continue from a final state.)No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "\u201cSimulating\u201d machine code (fixed bit width) 33 (70)\n\u03a3 = {00, 01, 10, 11} \u0393 = \u03a3 \u222a {\u25a1, ADD, MUL, AND}\nQ = {bgn, add, addx, mul, mulx, and, andx, H | x \u2208 Z 4} F = {H} q0 = bgn\nbgn\nadd\nmul\nand\nmul0\nmul1\nmul2\nmul3\nHMUL:\u25a1/+ 1ADD:\u25a1/+ 1\nAND:\u25a1/+ 1\n00:\u25a1/+ 1\n01:\u25a1/+ 1\n10:\u25a1/+ 1\n11:\u25a1/+ 1\n00: 00/+ 1, . . . ,11: 10/+ 1\nbgn\n. . .\u25a1 MUL 10 11 \u25a1 . . .\n\u25b7 Here, we \u201cemulate\u201d a part of a very simple 2-bit ALU (this time MSBF) .\n! The states Q can be any finite set, e.g. any combination of values stored in\nthe program counter, registers, cache, RAM.\n! The tape can represent swap space, heap, stack, any kind of storage device\n(hard disks, floppy disks, tape drives, . . . ) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "\u201cSimulating\u201d machine code (fixed bit width) 33 (71)\n\u03a3 = {00, 01, 10, 11} \u0393 = \u03a3 \u222a {\u25a1, ADD, MUL, AND}\nQ = {bgn, add, addx, mul, mulx, and, andx, H | x \u2208 Z 4} F = {H} q0 = bgn\nbgn\nadd\nmul\nand\nmul0\nmul1\nmul2\nmul3\nHMUL:\u25a1/+ 1ADD:\u25a1/+ 1\nAND:\u25a1/+ 1\n00:\u25a1/+ 1\n01:\u25a1/+ 1\n10:\u25a1/+ 1\n11:\u25a1/+ 1\n00: 00/+ 1, . . . ,11: 10/+ 1\nmul\n. . .\u25a1 \u25a1 10 11 \u25a1 . . .\n\u25b7 Here, we \u201cemulate\u201d a part of a very simple 2-bit ALU (this time MSBF) .\n! The states Q can be any finite set, e.g. any combination of values stored in\nthe program counter, registers, cache, RAM.\n! The tape can represent swap space, heap, stack, any kind of storage device\n(hard disks, floppy disks, tape drives, . . . ) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "\u201cSimulating\u201d machine code (fixed bit width) 33 (72)\n\u03a3 = {00, 01, 10, 11} \u0393 = \u03a3 \u222a {\u25a1, ADD, MUL, AND}\nQ = {bgn, add, addx, mul, mulx, and, andx, H | x \u2208 Z 4} F = {H} q0 = bgn\nbgn\nadd\nmul\nand\nmul0\nmul1\nmul2\nmul3\nHMUL:\u25a1/+ 1ADD:\u25a1/+ 1\nAND:\u25a1/+ 1\n00:\u25a1/+ 1\n01:\u25a1/+ 1\n10:\u25a1/+ 1\n11:\u25a1/+ 1\n00: 00/+ 1, . . . ,11: 10/+ 1\nmul2\n. . .\u25a1 \u25a1 \u25a1 11 \u25a1 . . .\n\u25b7 Here, we \u201cemulate\u201d a part of a very simple 2-bit ALU (this time MSBF) .\n! The states Q can be any finite set, e.g. any combination of values stored in\nthe program counter, registers, cache, RAM.\n! The tape can represent swap space, heap, stack, any kind of storage device\n(hard disks, floppy disks, tape drives, . . . ) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "\u201cSimulating\u201d machine code (fixed bit width) 33 (73)\n\u03a3 = {00, 01, 10, 11} \u0393 = \u03a3 \u222a {\u25a1, ADD, MUL, AND}\nQ = {bgn, add, addx, mul, mulx, and, andx, H | x \u2208 Z 4} F = {H} q0 = bgn\nbgn\nadd\nmul\nand\nmul0\nmul1\nmul2\nmul3\nHMUL:\u25a1/+ 1ADD:\u25a1/+ 1\nAND:\u25a1/+ 1\n00:\u25a1/+ 1\n01:\u25a1/+ 1\n10:\u25a1/+ 1\n11:\u25a1/+ 1\n00: 00/+ 1, . . . ,11: 10/+ 1\nH\n. . .\u25a1 \u25a1 \u25a1 10 \u25a1 . . .\n\u25b7 Here, we \u201cemulate\u201d a part of a very simple 2-bit ALU (this time MSBF) .\n! The states Q can be any finite set, e.g. any combination of values stored in\nthe program counter, registers, cache, RAM.\n! The tape can represent swap space, heap, stack, any kind of storage device\n(hard disks, floppy disks, tape drives, . . . ) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "\u201cSimulating\u201d machine code (fixed bit width) 33 (74)\n\u03a3 = {00, 01, 10, 11} \u0393 = \u03a3 \u222a {\u25a1, ADD, MUL, AND}\nQ = {bgn, add, addx, mul, mulx, and, andx, H | x \u2208 Z 4} F = {H} q0 = bgn\nbgn\nadd\nmul\nand\nmul0\nmul1\nmul2\nmul3\nHMUL:\u25a1/+ 1ADD:\u25a1/+ 1\nAND:\u25a1/+ 1\n00:\u25a1/+ 1\n01:\u25a1/+ 1\n10:\u25a1/+ 1\n11:\u25a1/+ 1\n00: 00/+ 1, . . . ,11: 10/+ 1\nH\n. . .\u25a1 \u25a1 \u25a1 10 \u25a1 . . .\n! For fixed bit width we can encode any standard CPU in \u03b4.\n! I.e. an 1TM can emulate or even simulate any standard computer\nand interpret any program in machine code (very slowly).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "\u201cSimulating\u201d machine code (fixed bit width) 33 (75)\n\u03a3 = {00, 01, 10, 11} \u0393 = \u03a3 \u222a {\u25a1, ADD, MUL, AND}\nQ = {bgn, add, addx, mul, mulx, and, andx, H | x \u2208 Z 4} F = {H} q0 = bgn\nbgn\nadd\nmul\nand\nmul0\nmul1\nmul2\nmul3\nHMUL:\u25a1/+ 1ADD:\u25a1/+ 1\nAND:\u25a1/+ 1\n00:\u25a1/+ 1\n01:\u25a1/+ 1\n10:\u25a1/+ 1\n11:\u25a1/+ 1\n00: 00/+ 1, . . . ,11: 10/+ 1\nH\n. . .\u25a1 \u25a1 \u25a1 10 \u25a1 . . .\n\u25b7 Informally, as a 1TM can run machine code for a given CPU, it can also\n\u25b7 run C/C++/Java (Jave byte code)/Python code.\n\u25b7 run numerical algorithms using floats.\n\u25b7 implement arithmetics on N , Z , Q using big integers.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "\u201cSimulating\u201d machine code (fixed bit width) 33 (76)\n\u03a3 = {00, 01, 10, 11} \u0393 = \u03a3 \u222a {\u25a1, ADD, MUL, AND}\nQ = {bgn, add, addx, mul, mulx, and, andx, H | x \u2208 Z 4} F = {H} q0 = bgn\nbgn\nadd\nmul\nand\nmul0\nmul1\nmul2\nmul3\nHMUL:\u25a1/+ 1ADD:\u25a1/+ 1\nAND:\u25a1/+ 1\n00:\u25a1/+ 1\n01:\u25a1/+ 1\n10:\u25a1/+ 1\n11:\u25a1/+ 1\n00: 00/+ 1, . . . ,11: 10/+ 1\nH\n. . .\u25a1 \u25a1 \u25a1 10 \u25a1 . . .\n\u25b7 The behavior of a 1TM can be defined precisely \u2013 in contrast to CPUs (cf.\nSMT) and programming languages (depends on e.g. compiler (version)) .\n! 1TMs serve as formal model of a computer (with unbounded memory).\nIf something is impossible for an 1TM, it is also impossible for any (standard)\ncomputer.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "\u201cSimulating\u201d machine code (fixed bit width) 33 (77)\n\u03a3 = {00, 01, 10, 11} \u0393 = \u03a3 \u222a {\u25a1, ADD, MUL, AND}\nQ = {bgn, add, addx, mul, mulx, and, andx, H | x \u2208 Z 4} F = {H} q0 = bgn\nbgn\nadd\nmul\nand\nmul0\nmul1\nmul2\nmul3\nHMUL:\u25a1/+ 1ADD:\u25a1/+ 1\nAND:\u25a1/+ 1\n00:\u25a1/+ 1\n01:\u25a1/+ 1\n10:\u25a1/+ 1\n11:\u25a1/+ 1\n00: 00/+ 1, . . . ,11: 10/+ 1\nH\n. . .\u25a1 \u25a1 \u25a1 10 \u25a1 . . .\n\u25b7 A deteterministic 1TM can also (at the cost of an increased runtime)\n\u25b7 simulate nondeterministic multi-tape TMs (including Petri nets),\n\u25b7 simulate CPUs with SMT/multiple cores, or a parallel computer,\n\u25b7 evaluate any neural network,\n\u25b7 simulate a quantum computer.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Simulating a 1TM 34 (78)\ndef bfs(Q, \u03a3, \u0393, \u03b4, q0, \u25a1, F, w):\ncur = {(\u03b5, q0, w)}\nwhile len(cur) > 0:\nnxt = {}\nfor (\u03b1, p, \u03b2) in cur:\nif \u03b2 == \u03b5: \u03b2 = \u25a1\nfor ((q, a), (r, b, d)) in \u03b4:\nif q == p and a == \u03b2[0]:\nif d == 0:\nnxt.add((\u03b1, r, b\u03b2[1:]))\nif d == +1:\nnxt.add((\u03b1b, r, \u03b2[1:]))\nif d == \u22121:\nif \u03b1 == \u03b5: \u03b1 = \u25a1\nnxt.add((\u03b1[:-1], r, \u03b1[-1]b\u03b2[1:]))\ncur = nxt\n\u25b7 We can simulate all possible runs of a (nondeterministic) 1TM using BFS:\nSimply compute all k-step successors q0w \u2212 \u2192k\nM for increasing k.\n\u25b7 In general, the number of k-step successors grows exponentially, e.g.\n\u03b4 = {(q0, x, q0, y,+1) | x \u2208 {0, 1, \u25a1}, y\u2208 {0, 1}}\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Simulating a 1TM 34 (79)\ndef bfs(Q, \u03a3, \u0393, \u03b4, q0, \u25a1, F, w):\ncur = {(\u03b5, q0, w)}\nwhile len(cur) > 0:\nnxt = {}\nfor (\u03b1, p, \u03b2) in cur:\nif \u03b2 == \u03b5: \u03b2 = \u25a1\nfor ((q, a), (r, b, d)) in \u03b4:\nif q == p and a == \u03b2[0]:\nif d == 0:\nnxt.add((\u03b1, r, b\u03b2[1:]))\nif d == +1:\nnxt.add((\u03b1b, r, \u03b2[1:]))\nif d == \u22121:\nif \u03b1 == \u03b5: \u03b1 = \u25a1\nnxt.add((\u03b1[:-1], r, \u03b1[-1]b\u03b2[1:]))\ncur = nxt\n\u25b7 If the 1TM is deterministic (D1TM), then the BFS reduces to following a\npath.\nI.e. the program only requires a single while-loop and two lists (or one cyclic\nlist) for representing the tape content.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Simulating a 1TM 34 (80)\ndef bfs(Q, \u03a3, \u0393, \u03b4, q0, \u25a1, F, w):\ncur = {(\u03b5, q0, w)}\nwhile len(cur) > 0:\nnxt = {}\nfor (\u03b1, p, \u03b2) in cur:\nif \u03b2 == \u03b5: \u03b2 = \u25a1\nfor ((q, a), (r, b, d)) in \u03b4:\nif q == p and a == \u03b2[0]:\nif d == 0:\nnxt.add((\u03b1, r, b\u03b2[1:]))\nif d == +1:\nnxt.add((\u03b1b, r, \u03b2[1:]))\nif d == \u22121:\nif \u03b1 == \u03b5: \u03b1 = \u25a1\nnxt.add((\u03b1[:-1], r, \u03b1[-1]b\u03b2[1:]))\ncur = nxt\n! As every program/algorithm (given in any known programming language,\nencoded in UTF8) can be \u201ccompiled\u201d to a deterministic 1TM, this means\nthat every program can be rewritten as a deterministic program consisting\nonly of a single while-loop and a single list.\n(In the end, this is what a CPU is doing.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Simulating a 1TM 34 (81)\ndef bfs(Q, \u03a3, \u0393, \u03b4, q0, \u25a1, F, w):\ncur = {(\u03b5, q0, w)}\nwhile len(cur) > 0:\nnxt = {}\nfor (\u03b1, p, \u03b2) in cur:\nif \u03b2 == \u03b5: \u03b2 = \u25a1\nfor ((q, a), (r, b, d)) in \u03b4:\nif q == p and a == \u03b2[0]:\nif d == 0:\nnxt.add((\u03b1, r, b\u03b2[1:]))\nif d == +1:\nnxt.add((\u03b1b, r, \u03b2[1:]))\nif d == \u22121:\nif \u03b1 == \u03b5: \u03b1 = \u25a1\nnxt.add((\u03b1[:-1], r, \u03b1[-1]b\u03b2[1:]))\ncur = nxt\n\u25b7 In particular, we can compile above program into a deterministic 1TM:\nT There is a universal D1TM (UTM) that takes the binary encoding of\nM = (Q, \u03a3, \u0393, \u03b4, q0, \u25a1, F) and an input w, and then simulates M on w.\n(An UTM is a very simple \u201cCPU\u201d that can even be simulated using Excel.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Simulating a 1TM 34 (82)\ndef bfs(Q, \u03a3, \u0393, \u03b4, q0, \u25a1, F, w):\ncur = {(\u03b5, q0, w)}\nwhile len(cur) > 0:\nnxt = {}\nfor (\u03b1, p, \u03b2) in cur:\nif \u03b2 == \u03b5: \u03b2 = \u25a1\nfor ((q, a), (r, b, d)) in \u03b4:\nif q == p and a == \u03b2[0]:\nif d == 0:\nnxt.add((\u03b1, r, b\u03b2[1:]))\nif d == +1:\nnxt.add((\u03b1b, r, \u03b2[1:]))\nif d == \u22121:\nif \u03b1 == \u03b5: \u03b1 = \u25a1\nnxt.add((\u03b1[:-1], r, \u03b1[-1]b\u03b2[1:]))\ncur = nxt\nT Thus every (nondeterministic) 1TM can be transformed into a D1TM\n\u25b7 Nondeterminism will make a difference later when we consider the length of a\nrun (run time) resp. the space used by a run.\nIt is also useful for abstracting from user input.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "3 Turing machines, computable functions, semidecidable languages\nTuring machines\nComputable and non-computable functions\nSemidecidable/recognizable/recursively enumerable languages\nHalting problem, Rice\u2019s theorem, deciding properties of computable functions\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Computable functions: preliminaries 36 (84)\n\u25b7 We will use TMs to formally define \u201ccomputable\u201d.\nD A partial function f : A ,\u2192 B is a binary relation f \u2286 A \u00d7 B so that for all\na \u2208 A we have |{b \u2208 B | (a, b) \u2208 f}| \u22641 (at most one image) .\n\u25b7 i.e. f(a) might be undefined for some a \u2208 A (e.g. f : R ,\u2192 R , x7\u2192 1\nx ).\n\u25b7 Every (total) function f : A \u2192 B is of course also a partial function.\nD lsbf(n) (msbf(n)) denotes the least (most) significant bit first representation\nwithout trailing (leading) zeros of n \u2208 N .\n\u25b7 e.g. 42 = 21 + 23 + 25 so lsbf(42) = 010101 and msbf(42) = 101010.\n! wlog we may always assume \u03a3 = {0, 1}:\n\u25b7 A simple approach is to fix some k \u2208 N with |\u03a3| \u22642k and to injectively map\n\u03a3 into {0, 1}k; then always first read k bits to decode the original symbol.\n\u25b7 E.g. for \u03a3 = {a, b, c} use a 7\u2192 00, b 7\u2192 10, c 7\u2192 01 (with 11 unsused).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Computable functions over words 37 (85)\nD A partial function f : \u03a3\u2217 ,\u2192 \u03a3\u2217 is computable if there exists a\ndeterministic 1TM M = (Q, \u03a3, \u0393, \u03b4, q0, \u25a1, {qf }) so that for all w \u2208 \u03a3\u2217\n\u25b7 If f(w) = w\u2032 is defined, then q0w \u2212 \u2192\u2217\nM qf w\u2032,\ni.e. M terminates on input w in a final state with w\u2032 written as output on the\ntape.\n\u25b7 If f(w) is undefined, then M runs forever or \u201ccrashes\u201d in some state q \u0338\u2208 F.\n! wlog we can always assume F = {qf }:\n\u25b7 Simply add a new state qf and add transitions from each old final state to the\nnew final state qf .\n! wlog every maximal run either reaches qf (and thus is finite) or it is infinite.\n\u25b7 If (q, a)\u03b4 = \u2205, add ((q, a), (q, a,0)) to \u03b4.\n\u25b7 Hence we can always assume:\nIf f(x) is defined, then the call to M returns f(x); otherwise the call will not\nreturn/terminate.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Computable functions over words 37 (86)\nD A partial function f : \u03a3\u2217 ,\u2192 \u03a3\u2217 is computable if there exists a\ndeterministic 1TM M = (Q, \u03a3, \u0393, \u03b4, q0, \u25a1, {qf }) so that for all w \u2208 \u03a3\u2217\n\u25b7 If f(w) = w\u2032 is defined, then q0w \u2212 \u2192\u2217\nM qf w\u2032,\ni.e. M terminates on input w in a final state with w\u2032 written as output on the\ntape.\n\u25b7 If f(w) is undefined, then M runs forever or \u201ccrashes\u201d in some state q \u0338\u2208 F.\n! More precisely, above definition is the definition of Turing computable.\nUsing the Church-Turing conjecture, we identify \u201c(intuitively) computable\u201d\nwith Turing computable, and thus simply write \u201ccomputable\u201d.\n! All data on a computer is always a finite (binary) string, so computers only\ncompute (partial) functions on (binary) strings.\nIn case of programs, partial functions typically arise from programs that do\nnot terminate or fail silently/crash for a given input x; but error\nmessages/(caught) exception can be considered the image of x.",
      "compute (partial) functions on (binary) strings.\nIn case of programs, partial functions typically arise from programs that do\nnot terminate or fail silently/crash for a given input x; but error\nmessages/(caught) exception can be considered the image of x.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Computable functions over N 38(87)\nD A partial function f : N k ,\u2192 N is computable if there exists a\ndeterministic 1TM M = (Q, \u03a3, \u0393, \u03b4, q0, \u25a1, {qf }) with \u03a3 = {0, 1, #} so that\nfor all n1, . . . , nk \u2208 N\n\u25b7 If f(n1, . . . , nk) = m is defined, then\nq0lsbf(n1)# . . .#lsbf(nk) \u2212 \u2192\u2217\nM qf lsbf(m)\n(# only serves as seperator; msbf can be used alternatively.)\n\u25b7 If f(n1, . . . , nk) is undefined, then M runs forever or halts in a state q \u0338\u2208 F.\n! This definition is only for convenience:\n\u25b7 Using e.g. UTF8 encoding, we can encode \u201c (n1, . . . , nk)\u201d as a binary string.\nThus every f : N k ,\u2192 N defines a function F : {0, 1}\u2217 ,\u2192 {0, 1}\u2217.\n(For an invalid/malformed encoding either return a default error value or leaf F\nundefined.)\n\u25b7 Every f : {0, 1}\u2217 ,\u2192 {0, 1}\u2217 defines a function F : N ,\u2192 N .\n(Simply encode w \u2208 {0, 1}\u2217 as lsbf\u22121(w1).)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cantor, noncomputable functions, (special) halting problem 39 (88)\n! There is a computable total surjective \u201cdecoding\u201d that maps w \u2208 {0, 1}\u2217 to\nthe description Mw = (Q, \u03a3, \u0393, \u03b4, q0, \u25a1, F) of a D1TM.\n\u25b7 E.g. read w as (potentially malformed) UTF8 encoding of the description of a\nD1TM Mw (cf. source code of a program/algorithm) .\nIf w is malformed (i.e. does not describe a D1TM) , let Mw default the D1TM\nthat rejects every input immediately.\nF Not every function f : N \u2192 N is computable:\n\u25b7 {0, 1}\u2217 \u2192 N , w7\u2192 lsfb\u22121(w1) is a bijection, so |N | = |{0, 1}\u2217|.\n\u25b7 By Cantor\u2019s diagonalization argument: |N | < |{0, 1}N |\n\u25b7 Trivially: |{0, 1}N | \u2264 |N N |.\n\u25b7 I.e. there are only countably many 1TMs, and thus computable functions,\nbut uncountably many characteristic functions (subsets, languages)\nf : {0, 1}\u2217 \u2192 {0, 1}, and thus also functions f : {0, 1}\u2217 \u2192 {0, 1}\u2217.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cantor, noncomputable functions, (special) halting problem 39 (89)\n! There is a computable total surjective \u201cdecoding\u201d that maps w \u2208 {0, 1}\u2217 to\nthe description Mw = (Q, \u03a3, \u0393, \u03b4, q0, \u25a1, F) of a D1TM.\n\u25b7 Cantor\u2019s diagonalization argument can also be adapted to obtain an example\nof an \u201cnoncomputable\u201d function.\n\u25b7 Reminder for |N | < |2N |:\n\u25b7 Assume f : N\nbij\n\u2212 \u21922N (i.e. assume |N | = |2N |).\n\u25b7 Define D := {k \u2208 N | k \u0338\u2208 f(k)} \u2286N .\n(If f is a function, then D is a well-defined set.)\n\u25b7 Let d := f\u22121(D).\n(As f is bijective/surjective, D has to have a pre-image d.).\n\u25b7 We obtain the contradiction (cf. Russell\u2019s paradox) :\nd \u2208 D iff d \u0338\u2208 f(d) = D\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cantor, noncomputable functions, (special) halting problem 39 (90)\n! There is a computable total surjective \u201cdecoding\u201d that maps w \u2208 {0, 1}\u2217 to\nthe description Mw = (Q, \u03a3, \u0393, \u03b4, q0, \u25a1, F) of a D1TM.\nT Special halting problem: The total function h: {0, 1}\u2217 \u2192 {0, 1}\nh(w) :=\n(\n1 if Mw halts on input w\n0 else\nis not computable. (\u201chalt\u201d: maximal run is finite)\n\u25b7 Assume M is a DTM that computes h.\n\u25b7 Then the following is also DTM M\u2032:\non input w run M to obtain y := h(w); while y = 1 pass;\n\u25b7 Thus there is some w\u2032 \u2208 {0, 1}\u2217 s.t. M\u2032 = Mw\u2032.\n\u25b7 M\u2032 = Mw\u2032 halts on w\u2032 iff h(w\u2032) = 0 iff Mw\u2032 = M\u2032 does not halt on w\u2032\n! h is a (total) function for which no (finite) TM (algorithm, program) exists.\nBut the partial function h\u2032 that is defined only on h\u22121(1) is computable:\nSimply run Mw on w; then return 1 (if the simulation terminates) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "3 Turing machines, computable functions, semidecidable languages\nTuring machines\nComputable and non-computable functions\nSemidecidable/recognizable/recursively enumerable languages\nHalting problem, Rice\u2019s theorem, deciding properties of computable functions\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Decidable and semidecidable languages/sets 41 (92)\nD For a language (set) L \u2286 \u03a3\u2217 define\n\u25b7 the total function (predicate) \u03c7L : \u03a3\u2217 \u2192 {0, 1} with \u03c7\u22121\nL (1) = L.\n\u03c7L is called the characteristic/indicator function of L.\n\u25b7 the partial function e\u03c7L : \u03a3\u2217 ,\u2192 {1} with e\u03c7\u22121\nL (1) = L\n! \u03c7L is total with \u03c7(L) = 0 while e\u03c7L is undefined on L = \u03a3\u2217 \\ L.\nD A language L \u2286 \u03a3\u2217 is decidable if \u03c7L is computable.\nA language L \u2286 \u03a3\u2217 is semidecidable if e\u03c7L is computable.\n(also: \u201cthe word problem for language L is (semi)decidable if . . . \u201d)\nL L is decidable iff both L and L are semidecidable.\n\u25b7 Obvious if L is decidable.\n\u25b7 Let the D1TMs ML and ML compute e\u03c7L and e\u03c7L, respectively.\nInterleave the simulations of both ML and ML on input w.\n(Two threads on one core.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Decidable and semidecidable languages/sets 41 (93)\nD For a language (set) L \u2286 \u03a3\u2217 define\n\u25b7 the total function (predicate) \u03c7L : \u03a3\u2217 \u2192 {0, 1} with \u03c7\u22121\nL (1) = L.\n\u03c7L is called the characteristic/indicator function of L.\n\u25b7 the partial function e\u03c7L : \u03a3\u2217 ,\u2192 {1} with e\u03c7\u22121\nL (1) = L\n! \u03c7L is total with \u03c7(L) = 0 while e\u03c7L is undefined on L = \u03a3\u2217 \\ L.\nD A language L \u2286 \u03a3\u2217 is decidable if \u03c7L is computable.\nA language L \u2286 \u03a3\u2217 is semidecidable if e\u03c7L is computable.\n(also: \u201cthe word problem for language L is (semi)decidable if . . . \u201d)\nT The special halting problem as language\nLh = {w \u2208 {0, 1}\u2217 | Mw halts on input w}\nis semidecidable, but not decidable (short: undecidable).\nC Lh = {w \u2208 {0, 1}\u2217 | Mw does not halt on input w} is not semidecidable.\n! Computing the (total) characteristic function of a set resp. deciding a (unary)",
      "T The special halting problem as language\nLh = {w \u2208 {0, 1}\u2217 | Mw halts on input w}\nis semidecidable, but not decidable (short: undecidable).\nC Lh = {w \u2208 {0, 1}\u2217 | Mw does not halt on input w} is not semidecidable.\n! Computing the (total) characteristic function of a set resp. deciding a (unary)\npredicate resp. computing a single bit of information can be impossible.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Decidable and semidecidable languages/sets 41 (94)\nD For a language (set) L \u2286 \u03a3\u2217 define\n\u25b7 the total function (predicate) \u03c7L : \u03a3\u2217 \u2192 {0, 1} with \u03c7\u22121\nL (1) = L.\n\u03c7L is called the characteristic/indicator function of L.\n\u25b7 the partial function e\u03c7L : \u03a3\u2217 ,\u2192 {1} with e\u03c7\u22121\nL (1) = L\n! \u03c7L is total with \u03c7(L) = 0 while e\u03c7L is undefined on L = \u03a3\u2217 \\ L.\nD A language L \u2286 \u03a3\u2217 is decidable if \u03c7L is computable.\nA language L \u2286 \u03a3\u2217 is semidecidable if e\u03c7L is computable.\n(also: \u201cthe word problem for language L is (semi)decidable if . . . \u201d)\n\u25b7 A language L can be understood as a unary predicate:\n\u25b7 Given an encoding w \u2208 {0, 1}\u2217 the object represented by w has a property iff\nw \u2208 L, e.g.: Mw halts on \u03b5 iff w \u2208 LH,\u03b5.\n\u25b7 E.g. in the simplest case, neural networks are used to \u201clearn\u201d (approximate) a\npredicate, i.e. a language (an n-bit output corresponds to n predicates).\n\u25b7 If L is undecidable, any 1TM that tries to decide L has to give the wrong",
      "w \u2208 L, e.g.: Mw halts on \u03b5 iff w \u2208 LH,\u03b5.\n\u25b7 E.g. in the simplest case, neural networks are used to \u201clearn\u201d (approximate) a\npredicate, i.e. a language (an n-bit output corresponds to n predicates).\n\u25b7 If L is undecidable, any 1TM that tries to decide L has to give the wrong\nanswer sometimes \u2013 and thus this has to be true also for neural networks.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Recognizable languages 42 (95)\nD (Reminder) The language accepted by the 1TM M = (Q, \u03a3, \u0393, \u03b4, q0, \u25a1, F) is\nL(M) := {w \u2208 \u03a3\u2217 | q0w \u2212 \u2192\u2217\nM \u03b1qf \u03b2 for some \u03b1, \u03b2\u2208 \u0393\u2217, qf \u2208 F}\nA language L \u2286 \u03a3\u2217 is recognizable if\nthere is some 1TM M that accepts L, i.e. with L(M) = L.\n! \u201crecognizable\u201d allows M to be nondeterministic:\nM accepts a word w \u2208 \u03a3\u2217 iff there is at least one run on input M that\neventually terminates in a final/accepting state.\nM rejects a word w \u2208 \u03a3\u2217 iff every run on input M either is infinite or\neventually terminates in a non-final/rejecting state.\n(Recall: By definition of 1TM, \u03b4 is undefined for all final states.)\nBut as we already know that BFS can be used to turn every 1TM into a\nD1TM, it immediately follows that \u201crecognizable\u201d and \u201csemidecidable\u201d\ncoincide.\nFurther wlog we may assume that a D1TM halts iff it accepts.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Recognizable languages 42 (96)\nD (Reminder) The language accepted by the 1TM M = (Q, \u03a3, \u0393, \u03b4, q0, \u25a1, F) is\nL(M) := {w \u2208 \u03a3\u2217 | q0w \u2212 \u2192\u2217\nM \u03b1qf \u03b2 for some \u03b1, \u03b2\u2208 \u0393\u2217, qf \u2208 F}\nA language L \u2286 \u03a3\u2217 is recognizable if\nthere is some 1TM M that accepts L, i.e. with L(M) = L.\nF L is semidecidable iff L is recognizable.\n\u25b7 Assume L is semidecidable, i.e. e\u03c7L is computed by some 1TM M.\nThen by definition L(M) = L as M does not terminate on L.\n\u25b7 Assume L is recognizable, i.e. L = L(M) for some 1TM M.\nAs M might be nondeterministic, use BFS to construct all reachable\nconfiguration for the given input w; only return 1 if some final state is\nreached, else return 0 or do not terminate.\n\u25b7 If L is semidecidable, we can write a program/define a DTM that enumerates\nexactly the elements of L.\n(For every w \u2208 L there is shortest accepting run; use BFS to simulate all runs on \u03a3\u2217 for\nincreasing time; cf. dovetailing/BFS on \u03a3\u2217)",
      "reached, else return 0 or do not terminate.\n\u25b7 If L is semidecidable, we can write a program/define a DTM that enumerates\nexactly the elements of L.\n(For every w \u2208 L there is shortest accepting run; use BFS to simulate all runs on \u03a3\u2217 for\nincreasing time; cf. dovetailing/BFS on \u03a3\u2217)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Semidecidable and computably/recursively enumerable sets 43 (97)\nL L \u2286 \u03a3\u2217 with L \u0338= \u2205 is semidecidable iff\nthere is a computable enumeration f : N \u2192\u03a3\u2217 so that f(N ) = L.\nftable = [] # f only has to be surjective\nfor k = 1, 2, . . .in N : # max depth of BFS and time bound\nfor l = 0, 1, . . . , k: # in increasing order (depth of BFS)\nfor w \u2208 \u03a3l: # in lexicographic order (front of BFS)\nif simulate_for_k_steps(\u03c7L, w, k) == 1:\nftable.append(w) # happens infinitely often for every w \u2208 L\nif len(ftable) == n: return w\nfor k = 1, 2, . . .in N :\nwk = f(k) # run M on k; has to terminate as f is total\nif w == wk: return 1\nD A semidecidable set (incl. \u2205) is also called computably/recursively\nenumerable, i.e. we can construct/reach every w \u2208 L in a finite time.\n(decidable: we can also construct/reach each w \u2208 L in finite time.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Semidecidable, recognizable, enumerable, computable, . . . 44 (98)\n! So for L \u2286 \u03a3\u2217 we have:\nL is recognizable, i.e. accepted by some TM.\niff L is semidecidable.\niff L is computably/recursively enumerable.\niff The word problem for L is semidecidable.\niff There is a computable total function f : N \u2192 \u03a3\u2217 with f(N ) = L (if L \u0338= \u2205).\niff There is a computable partial function e\u03c7L : \u03a3\u2217 ,\u2192 {1} with L = e\u03c7\u22121\nL (1).\nand\nL is decidable\niff \u03c7L : \u03a3\u2217 \u2192 {0, 1} with \u03c7\u22121\nL (1) = L is computable.\niff both L and L = \u03a3\u2217 \\ L are semidecidable/rec. enumerable/recognizable.\nand\nL is undecidable\niff at least L or L is not semidecidable/rec. enumerable/recognizable.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Languages vs. functions, semidecidable vs. computable 45 (99)\n\u25b7 By definition, \u201csemidecidable\u201d is a special case of \u201ccomputable\u201d.\n! If f : \u03a3\u2217 ,\u2192 \u03a3\u2217 is computable, then also f$ : \u03a3\u2217 ,\u2192 \u03a3\u2217{$}, w7\u2192 f(w)$ is\ncomputable where $ \u0338\u2208 \u03a3 is a \u201cfresh\u201d symbol for \u201cend of word\u201d.\n\u25b7 \u201c$\u201d can also be encoded in binary, e.g.: 0 7\u2192 00, 1 7\u2192 01, $ 7\u2192 10\n\u25b7 \u201c$\u201d is not required if we have a computable bound on |f(w)|.\nL Let f : \u03a3\u2217 ,\u2192 \u03a3\u2217{$} and consider the language (\u201cu prefix of f(w)?\u201d)\nL = {u#w | w \u2208 \u03a3\u2217, u, v\u2208 (\u03a3 \u222a {$})\u2217, f(w) = uv}\nL is semidecidable iff f is computable.\n\u25b7 See the next slide: treat \u03a3\u2217{$} as an infinite tree.\nGiven some input w we use e\u03c7L to guide a DFS from the root \u03b5 towards f(w).\nAs e\u03c7L does not terminate in general, we need to simulate e\u03c7L(ua) for all\na \u2208 \u03a3 \u222a {$} in parallel again.\n! In particular, if L is decidable, i.e. \u03c7L computable, then we can recover f(w)\nfrom L using at most |\u03a3 \u222a {$}| \u00b7 |f(w)| calls to \u03c7L.",
      "As e\u03c7L does not terminate in general, we need to simulate e\u03c7L(ua) for all\na \u2208 \u03a3 \u222a {$} in parallel again.\n! In particular, if L is decidable, i.e. \u03c7L computable, then we can recover f(w)\nfrom L using at most |\u03a3 \u222a {$}| \u00b7 |f(w)| calls to \u03c7L.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Semidecidability and computability 46 (100)\nL f : \u03a3\u2217 ,\u2192 \u03a3\u2217{$} is computable ($ \u0338\u2208 \u03a3) iff\nL = {u#w | w \u2208 \u03a3\u2217, u, v\u2208 (\u03a3 \u222a {$})\u2217, f(w) = uv} is semidecidable\n\u25b7 Assume f : \u03a3\u2217 ,\u2192 \u03a3\u2217{$} is computable:\nw\u2032 = f(w) # direct call, might not terminate\nif w\u2032.startswith(u):\nreturn 1\nwhile True:\npass\n\u25b7 Assume L = {u#w | w \u2208 \u03a3\u2217, u, v\u2208 (\u03a3 \u222a {$})\u2217, f(w) = uv} is semidecidable,\ni.e. e\u03c7L is computed by some ML; use ML to as \u201coracle\u201d to guide the search:\nu = \u03b5 # DFS path through \u03a3\u2217{$} from \u03b5 towards f(w)\nfor k = 1, 2, 3, . . .in N :\nfor a \u2208 \u03a3 \u222a {$}:\nif simulate_for_k_steps(ML, ua#w, k) == 1:\nif a == $:\nreturn u\nu = ua\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Deciding satisfiability vs. computing a satisfying assignment 47 (101)\n! If L is even decidable, then we can simply call ML and wait for the result,\nand thus remove the outer loop.\n\u25b7 For instance, consider the satisfiability of propositional formulas ( wlog as\nclause set), e.g.\nF = {{\u00acp, q,\u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acp, \u00acs}, {\u00acp, r}}\nGiven a propositional formula F with variables x1, . . . , xn\n\u25b7 the computation problem is: compute a satisfying assignment\n\u03b2 : {x1, . . . , xn} \u2192 {0, 1} of F, if there is one, else return false.\n\u25b7 the decision problem is: decide (true / false) if F is satisfiable, i.e. has at least\none satisfying assignment.\nChecking all 2n (minimal) assignments allows to solve both problems. Note:\n\u25b7 Any 1TM/algorithm that solves the computation problem also solves the\ndecision problem.\n\u25b7 An 1TM/algorithm ML that solves the decision problem can be used to guide\nDPLL using at most 2n calls.",
      "one satisfying assignment.\nChecking all 2n (minimal) assignments allows to solve both problems. Note:\n\u25b7 Any 1TM/algorithm that solves the computation problem also solves the\ndecision problem.\n\u25b7 An 1TM/algorithm ML that solves the decision problem can be used to guide\nDPLL using at most 2n calls.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example DPLL basic version 48 (102)\n{{\u00acp, q,\u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acp, \u00acs}, {\u00acp, r}}\n{{q, \u00acr, s}, {\u00acq, \u00acr, s}, {r}, {\u00acs}} {{\u00acq, \u00acr, s}, {r}}\n{{\u00acr, s}, {r}, {\u00acs}} {{\u00acr, s}, {r}, {\u00acs}}\n{{s}, {\u00acs}} {{}, {\u00acs}}\n{{}} {{}}\n{{s}, {\u00acs}} {{}, {\u00acs}}\n{{}} {{}}\n{{\u00acr, s}, {r}}\n{{s}}\n{}\np := true p := false\nq := true\nq := false\nr := true r := false\ns := true s := false\nr := true r := false\ns := true s := false\nq := true\nr := true\ns := true\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Decision vs. computation and approximation 49 (103)\n\u25b7 Given a polynomial like\nX5 \u2212 X + 1\nwe can decide that it has a real root x0 \u2208 [\u22121.1, \u22121.0] using e.g. Sturm\u2019s\ntheorem:\nx\ny\nbut in general we can only approximate the (e.g.) binary representation of x0\ne.g. by means of the reduction to a decision problem (binary search/bisection\nmethod) (Newton\u2019s method might still be the better approach) .\n! There exists (at least) one real number x s.t. there is no program/D1TM that\non input i \u2208 N 0 outputs the first i digits of the binary representation of x.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "3 Turing machines, computable functions, semidecidable languages\nTuring machines\nComputable and non-computable functions\nSemidecidable/recognizable/recursively enumerable languages\nHalting problem, Rice\u2019s theorem, deciding properties of computable functions\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem in general and Rice\u2019s theorem 51 (105)\n\u25b7 As the special halting problem is undecidable, any non-trivial question\nregarding computable functions is also undecidable:\n! As before let {0, 1}\u2217 \u220b w 7\u2192 Mw be a computable total surjective \u201cdecoding\u201d\nso that {Mw | w \u2208 {0, 1}\u2217} is the set of all (descriptions of) D1TM.\nC Halting problem:\nLH := {w#x | w, x\u2208 {0, 1}\u2217, Mw halts on input x}\nis semideciable, but not decidable (i.e. LH is not semideciable).\n\u25b7 \u201chalts\u201d: the maximal run q0w \u2212 \u2192\u2217\nM is finite\n\u25b7 semidecidable: simply simulate Mw on x.\n\u25b7 undecidable: otherwise Lh would be decidable as: w \u2208 Lh iff w#w \u2208 LH\n! The argument used in the proof is called a (computable) reduction:\n\u25b7 \u03c1(w) := w#w is a computable total function so that w \u2208 Lh iff \u03c1(w) \u2208 LH.\n\u25b7 i.e. by deciding f(w) \u2208 LH we can decide w \u2208 Lh and thus \u201creduce\u201d the\nspecial halting problem to the (general) halting problem.",
      "! The argument used in the proof is called a (computable) reduction:\n\u25b7 \u03c1(w) := w#w is a computable total function so that w \u2208 Lh iff \u03c1(w) \u2208 LH.\n\u25b7 i.e. by deciding f(w) \u2208 LH we can decide w \u2208 Lh and thus \u201creduce\u201d the\nspecial halting problem to the (general) halting problem.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem in general and Rice\u2019s theorem 51 (106)\n\u25b7 As the special halting problem is undecidable, any non-trivial question\nregarding computable functions is also undecidable:\nC Halting problem on empty tape\nLH,\u03b5 := {w | w \u2208 {0, 1}\u2217, Mw halts on input \u03b5}\nis semideciable, but not decidable (i.e. LH,\u03b5 is not semideciable).\n\u25b7 semidecidable: simply simulate Mw on \u03b5.\n\u25b7 undecidable: otherwise Lh would be decidable.\nWe define again a computable reduction \u03c1.\nGiven w construct from Mw the D1TM M\u2032\nw as follows:\n\u25b7 M\u2032\nw initially replaces any input by w.\n\u25b7 it then executes the original Mw on w.\nLet \u03c1(w) := w\u2032 be the binary encoding of M\u2032\nw.\nThen: w \u2208 Lh iff \u03c1(w) = w\u2032 \u2208 LH,\u03b5\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem in general and Rice\u2019s theorem 51 (107)\n\u25b7 As the special halting problem is undecidable, any non-trivial question\nregarding computable functions is also undecidable:\nC Halting problem for some input\nLH,? := {w | w \u2208 {0, 1}\u2217, there ex. x \u2208 {0, 1}\u2217 : Mw halts on input x}\nis semideciable, but not decidable (i.e. LH,? is not semideciable).\n\u25b7 semidecidable: enumerate all x on which Mw halts (dove-tailing/BFS)\n\u25b7 undecidable: just as for halting on empty tape\nGiven w construct from Mw the D1TM M\u2032\nw as follows:\n\u25b7 M\u2032\nw initially replaces any input by w.\n\u25b7 it then executes the original Mw on w.\nLet \u03c1(w) := w\u2032 be the binary encoding of M\u2032\nw.\nThen: w \u2208 Lh iff \u03c1(w) = w\u2032 \u2208 LH,?\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem in general and Rice\u2019s theorem 51 (108)\n\u25b7 As the special halting problem is undecidable, any non-trivial question\nregarding computable functions is also undecidable:\nL Halting problem for all inputs/nontermination: Both\nLH,\u2217 := {w | w \u2208 {0, 1}\u2217, for all x \u2208 {0, 1}\u2217 : Mw halts on x}\nLH,\u2217 = {w | w \u2208 {0, 1}\u2217, there ex. x \u2208 {0, 1}\u2217 : Mw does not halt on x}\nare not semideciable.\n\u25b7 LH,\u2217 is not semidecidable: As before, let \u03c1(w) be the encoding of the D1TM\nM\u2032\nw that simply replaces every input by w and then runs Mw on w.\nThen: w \u2208 Lh iff \u03c1(w) \u2208 LH,\u2217\n\u25b7 LH,\u2217 is not semidecidable: Given w \u2208 {0, 1}\u2217 construct M\u2032\nw as follows:\n\u25b7 On input x simulate Mw on w for n := lsbf\u22121(x1) steps.\n\u25b7 If Mw halts during the simulation, loop forever, otherwise halt.\nLet \u02c6\u03c1(w) := w\u2032 be the encoding of M\u2032\nw.\nThen: w \u2208 Lh iff \u02c6\u03c1(w) \u2208 LH,\u2217.\n(M\u2032",
      "\u25b7 LH,\u2217 is not semidecidable: Given w \u2208 {0, 1}\u2217 construct M\u2032\nw as follows:\n\u25b7 On input x simulate Mw on w for n := lsbf\u22121(x1) steps.\n\u25b7 If Mw halts during the simulation, loop forever, otherwise halt.\nLet \u02c6\u03c1(w) := w\u2032 be the encoding of M\u2032\nw.\nThen: w \u2208 Lh iff \u02c6\u03c1(w) \u2208 LH,\u2217.\n(M\u2032\nw rejects all x with |x| larger than the running time of Mw on input w.)No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem in general and Rice\u2019s theorem 51 (109)\n\u25b7 As the special halting problem is undecidable, any non-trivial question\nregarding computable functions is also undecidable:\nC Language (in)equivalence\nL= := {w#w\u2032 | w, w\u2032 \u2208 {0, 1}\u2217, L(Mw) = L(Mw\u2032)}\nL\u0338= := {w#w\u2032 | w, w\u2032 \u2208 {0, 1}\u2217, L(Mw) \u0338= L(Mw\u2032)}\nBoth L= and L\u0338= are undecidable:\n\u25b7 wlog we can assume that a D1TM either halts in a unique final state or does\nnot halt at all, i.e. wlog \u201caccepting\u201d and \u201chalting\u201d coincide. Then:\nLH,\u03b5 = {w \u2208 \u03a3\u2217 | \u03b5 \u2208 L(Mw)} is semidecidable,\nbut not LH,\u03b5 = {w \u2208 \u03a3\u2217 | \u03b5 \u0338\u2208 L(Mw)}.\nLH,? = {w \u2208 \u03a3\u2217 | L(Mw) \u0338= \u2205} is semidecidable,\nbut not LH,? = {w \u2208 \u03a3\u2217 | L(Mw) = \u2205}.\nLH,\u2217 = {w \u2208 \u03a3\u2217 | L(Mw) = \u03a3\u2217} and\nLH,\u2217 = {w \u2208 \u03a3\u2217 | L(Mw) \u0338= \u03a3\u2217} are both undecidable.\n\u25b7 Thus simply choose some D1TM Mw\u2032 with L(Mw\u2032) = \u03a3\u2217 to reduce L= to\nLH,\u2217.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem in general and Rice\u2019s theorem 51 (110)\n\u25b7 As the special halting problem is undecidable, any non-trivial question\nregarding computable functions is also undecidable:\n! The main ideas used in the preceeding proofs can be generalized to Rice\u2019s\ntheorem and the Rice-Shapiro theorem.\n! Informally, both theorems say that as soon as we try to write a \u201ccode\nanalyzer\u201d for any non-trivial property/predicate regarding the function\ncomputed by a TM/program (semantics)\n\u25b7 like termination/nontermination, total/partial, finitely many outputs/infinitely\nmany outputs, prototype vs. optimized code, . . .\nthis code analyzer will not terminate for some programs (or it has to give\nsometimes the wrong answer).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem in general and Rice\u2019s theorem 51 (111)\n\u25b7 As the special halting problem is undecidable, any non-trivial question\nregarding computable functions is also undecidable:\n! The important point to realize is that such a \u201ccode analyzer\u201d has to\nterminate and give the correct answer for all TMs/programs.\nTwo solutions:\n\u25b7 Only consider sufficiently restricted classes of TMs/programs.\nE.g. if we restrict the tape of 1TM to the length of the original input, then for\na given input x \u2208 {0, 1}\u2217, there are at most (\u0393 \u222a Q)|x|+2 configurations.\n\u25b7 Allow that the \u201ccode analyzer\u201d returns \u201cdon\u2019t know\u201d or \u201cfalse negatives\u201d or\n\u201cfalse positives\u201d depending on the application.\nE.g. software tests actually yield \u201cfail\u201d or \u201ccouldn\u2019t find an error/don\u2019t\nknow/might be correct\u201d, and thus potentially false positives.\n\u25b7 Combination of both are used in software/hardware verifcation in practice,\ne.g. for device drivers.\n(Hardware is the physical realization of TMs (see e.g. VHDL).)",
      "know/might be correct\u201d, and thus potentially false positives.\n\u25b7 Combination of both are used in software/hardware verifcation in practice,\ne.g. for device drivers.\n(Hardware is the physical realization of TMs (see e.g. VHDL).)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem, busy beavers, and Kolmogorov complexity 52 (112)\nD Busy beaver: Let En be the set of all D1TMs with \u0393 = {0, 1}, \u25a1 = 0,\n|F| = 1, |Q| = n + 1 and terminate on input \u03b5.\n\u03a3B(n) is the maximal number of 1s output by any M \u2208 En.\nSB(n) is the maximal number of steps made by any M \u2208 En.\n\u25b7 By definition \u03a3B(n) and SB(n) are always finite with\nn \u2264 \u03a3B(n) \u2264 SB(n)\nIt can be shown that SB(n) < \u03a3B(3n + 6) (cf. here).\n\u25b7 wlog every M \u2208 En always moves its head by \u00b11 until entering qf .\nAs \u03b4 \u2208 (Q \u00d7 \u0393 \u00d7 {\u00b11})Q\\{qf }\u00d7\u0393 we have |En| \u2264((n + 1)4)2n.\n\u25b7 Currently, the precise values are only known for n <5:\n\u03a3B(1) = 1 \u03a3 B(2) = 4 \u03a3 B(3) = 6 \u03a3 B(4) = 13\nSB(1) = 1 SB(2) = 6 SB(3) = 21 SB(4) = 107\n! w \u0338\u2208 LH,\u03b5 iff Mw makes at least S(|Mw|) steps on input \u03b5.\nHence: \u03a3B is not computable. In fact \u03a3B \u201coutgrows\u201d every computable\nfunction f : N \u2192 N . (cf. Rado).No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem, busy beavers, and Kolmogorov complexity 52 (113)\nD Busy beaver: Let En be the set of all D1TMs with \u0393 = {0, 1}, \u25a1 = 0,\n|F| = 1, |Q| = n + 1 and terminate on input \u03b5.\n\u03a3B(n) is the maximal number of 1s output by any M \u2208 En.\nSB(n) is the maximal number of steps made by any M \u2208 En.\n\u25b7 An important consequence is:\n\u25b7 Assume we have some open mathematical problem that can be disproved by\nmeans of a counter-example, e.g.:\nGoldbach\u2019s conjecture: For all even n \u2208 N with n >2 there exists two primes\np, qwith n = p + q.\n\u25b7 Assume further we can write a program that implements an exhaustive search\nfor a counter-example (and thus does not require any input).\n\u25b7 Then we can translate this program into a D1TM M with n states.\n\u25b7 Thus: the conjecture is valid iff M makes at least SB(n) + 1 stesp.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem, busy beavers, and Kolmogorov complexity 52 (114)\nD Busy beaver: Let En be the set of all D1TMs with \u0393 = {0, 1}, \u25a1 = 0,\n|F| = 1, |Q| = n + 1 and terminate on input \u03b5.\n\u03a3B(n) is the maximal number of 1s output by any M \u2208 En.\nSB(n) is the maximal number of steps made by any M \u2208 En.\n\u25b7 A bit simplified:\nKolmogorov complexity asks, given some word w \u2208 \u03a3\u2217:\n\u201cWhat is the shortest DTM that outputs w?\u201d\nwhere \u201cshortest\u201d is e.g. the bit-length of the string describing the DTM.\nHence, the Kolmogorov complexity of a given word w is computable,\nbut as function in general not computable resp. as decision problem\nundedicable.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "4 String rewriting, grammars, Chomsky hierarchy\nString rewriting and grammars: abstraction of \u201cmathematical calculations\u201d\nChomsky hierarchy and normal forms\nGrammars vs. Turing machines (recognizable languages)\nUndecidable problems and Post correspondence problem\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "String rewriting systems 55 (117)\nD (Effective) string rewriting system (SRS, also: semi-Thue system) (\u03a3, R):\n\u25b7 \u03a3: finite alphabet\n\u25b7 R: finite set of rewrite rules with R \u2286 \u03a3\u2217 \u00d7 \u03a3\u2217\n\u25b7 The rewrite rules generate the rewriting relation \u2212 \u2192R on \u03a3\u2217:\n\u2212 \u2192R:= {(xuy, xvy) | (u, v) \u2208 R, x, y\u2208 \u03a3\u2217}\nIts reflexive-transitive closure is denoted by \u2212 \u2192\u2217\nR.\n\u25b7 w can be rewritten to w\u2032 wrt. R if w \u2212 \u2192\u2217\nR w\u2032.\n\u25b7 SRS are a simple model of the concept of \u201ccalculating by hand\u201d.\n\u25b7 Consider e.g. for n = 5\n\u03a3 := Z n \u222a {[, ], \u00b7, +}\nR := {([x \u00b7 y], z) | x, y, z\u2208 Z n, x\u00b7 y \u2261n z}\n\u222a { ([x + y], z) | x, y, z\u2208 Z n, x+ y \u2261n z}\n[[3 + 1]\u00b7 [2 \u00b7 4]] \u2212 \u2192R [4 \u00b7 [2 \u00b7 4]] \u2212 \u2192R [4 \u00b7 3] \u2212 \u2192R 2\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "String rewriting systems 55 (118)\nD (Effective) string rewriting system (SRS, also: semi-Thue system) (\u03a3, R):\n\u25b7 \u03a3: finite alphabet\n\u25b7 R: finite set of rewrite rules with R \u2286 \u03a3\u2217 \u00d7 \u03a3\u2217\n\u25b7 The rewrite rules generate the rewriting relation \u2212 \u2192R on \u03a3\u2217:\n\u2212 \u2192R:= {(xuy, xvy) | (u, v) \u2208 R, x, y\u2208 \u03a3\u2217}\nIts reflexive-transitive closure is denoted by \u2212 \u2192\u2217\nR.\n\u25b7 w can be rewritten to w\u2032 wrt. R if w \u2212 \u2192\u2217\nR w\u2032.\n\u25b7 SRS are a simple model of the concept of \u201ccalculating by hand\u201d.\n\u25b7 Consider e.g. for n = 2\n\u03a3 := {0, 1, [, ], \u2227, \u00ac}\nR := {(\u00acx, z) | x, z\u2208 {0, 1}, z= 1 \u2212 x}\n\u222a { ([x \u2227 y], z) | x, y, z\u2208 {0, 1}, z= min(x, y)}\n[\u00ac0 \u2227 [1 \u2227 \u00ac0]] \u2212 \u2192R [\u00ac0 \u2227 [1 \u2227 1]] \u2212 \u2192R [\u00ac0 \u2227 1] \u2212 \u2192R [1 \u2227 1] \u2212 \u2192R 1\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "String rewriting systems 55 (119)\nD (Effective) string rewriting system (SRS, also: semi-Thue system) (\u03a3, R):\n\u25b7 \u03a3: finite alphabet\n\u25b7 R: finite set of rewrite rules with R \u2286 \u03a3\u2217 \u00d7 \u03a3\u2217\n\u25b7 The rewrite rules generate the rewriting relation \u2212 \u2192R on \u03a3\u2217:\n\u2212 \u2192R:= {(xuy, xvy) | (u, v) \u2208 R, x, y\u2208 \u03a3\u2217}\nIts reflexive-transitive closure is denoted by \u2212 \u2192\u2217\nR.\n\u25b7 w can be rewritten to w\u2032 wrt. R if w \u2212 \u2192\u2217\nR w\u2032.\n\u25b7 SRS are a simple model of the concept of \u201ccalculating by hand\u201d.\n\u25b7 For 1TM M the relation \u2212 \u2192M is an SRS on the configurations \u0393\u2217 \u00d7 Q \u00d7 \u0393\u2217\n\u2212 \u2192M = {(\u03b1qa\u03b2, \u03b1br\u03b2) | ((q, a), (r, b,+1)) \u2208 \u03b4, \u03b1, \u03b2\u2208 \u0393\u2217}\n\u222a {(\u03b1zqa\u03b2, \u03b1rzb\u03b2) | ((q, a), (r, b,\u22121)) \u2208 \u03b4, \u03b1, \u03b2\u2208 \u0393\u2217, z\u2208 \u0393}\n\u222a {(\u03b1qa\u03b2, \u03b1rb\u03b2) | ((q, a), (r, b,0)) \u2208 \u03b4, \u03b1, \u03b2\u2208 \u0393\u2217}\nIn the end, the CPU just rewrites binary strings.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "String rewriting systems 55 (120)\nD (Effective) string rewriting system (SRS, also: semi-Thue system) (\u03a3, R):\n\u25b7 \u03a3: finite alphabet\n\u25b7 R: finite set of rewrite rules with R \u2286 \u03a3\u2217 \u00d7 \u03a3\u2217\n\u25b7 The rewrite rules generate the rewriting relation \u2212 \u2192R on \u03a3\u2217:\n\u2212 \u2192R:= {(xuy, xvy) | (u, v) \u2208 R, x, y\u2208 \u03a3\u2217}\nIts reflexive-transitive closure is denoted by \u2212 \u2192\u2217\nR.\n\u25b7 w can be rewritten to w\u2032 wrt. R if w \u2212 \u2192\u2217\nR w\u2032.\n\u25b7 SRS are a simple model of the concept of \u201ccalculating by hand\u201d.\n\u25b7 Consider {a, b}\u2217 modulo the identities a2 \u2261 \u03b5 \u2261 b2 and ab \u2261 ba where each\nidentity w \u2261 w\u2032 gives rise to the rewrite rules (w, w\u2032), (w\u2032, w) \u2208 R s.t.:\nam1 bn1 . . . amlbnl \u2212 \u2192\u2217\nR a(m1+...+ml) mod 2b(n1+...+nl) mod 2 \u2208 {\u03b5, a, b, ab}\ni.e. this is actually a group that is isomorphic with Z 2 \u00d7 Z 2.\n\u25b7 The word problem for groups is undecidable in general, too:\nCan w be rewritten to w\u2032 wrt. the defining identities?No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "String rewriting systems 55 (121)\nD (Effective) string rewriting system (SRS, also: semi-Thue system) (\u03a3, R):\n\u25b7 \u03a3: finite alphabet\n\u25b7 R: finite set of rewrite rules with R \u2286 \u03a3\u2217 \u00d7 \u03a3\u2217\n\u25b7 The rewrite rules generate the rewriting relation \u2212 \u2192R on \u03a3\u2217:\n\u2212 \u2192R:= {(xuy, xvy) | (u, v) \u2208 R, x, y\u2208 \u03a3\u2217}\nIts reflexive-transitive closure is denoted by \u2212 \u2192\u2217\nR.\n\u25b7 w can be rewritten to w\u2032 wrt. R if w \u2212 \u2192\u2217\nR w\u2032.\n\u25b7 SRS are a simple model of the concept of \u201ccalculating by hand\u201d.\n\u25b7 Proofs based on natural deduction, e.g.:\n\u22a2 A \u2227 B\n\u22a2 A\n\u22a2 A\n\u22a2 A \u2228 B\n\u22a2 A \u22a2 A \u2192 B\n\u22a2 B\nA \u22a2 B A \u22a2 \u00acB\n\u22a2 \u00acA\ncan be enumerated by a TM and can thus be rephrased as SRS.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "MU puzzle 56 (122)\n\u25b7 The MU puzzle consists of the rewriting relation on \u03a3\u2217 = {M, I, U}\u2217:\n\u2212 \u2192R := {(xI, xIU) | x \u2208 \u03a3\u2217}\n\u222a { (My, Myy) | y \u2208 \u03a3\u2217}\n\u222a { (xIIIy, xUy) | x, y\u2208 \u03a3\u2217}\n\u222a { (xUUy, xy) | x, y\u2208 \u03a3\u2217}\ne.g.\nMI \u2212 \u2192R MII \u2212 \u2192R MIIII \u2212 \u2192R MUI \u2212 \u2192R MUIU\n\u2212 \u2192R MUIUUIU \u2212 \u2192R MUIIU\nThe question is to decide whether MI \u2212 \u2192\u2217\nR MU .\n(e.g. see the DS slides for the solution.)\n\u25b7 Note that formally \u2212 \u2192R is not an SRS.\nBut a 1TM can nondeterministically apply the rules to rewrite MI step by\nstep into any word w with MI \u2212 \u2192\u2217\nR w.\nAs we will see, any such 1TM can be translated to an SRS resp. grammar.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Formal grammars 57 (123)\nD (unrestricted formal) grammar G = (V, \u03a3, P, S)\n\u25b7 V: finite alphabet of variable/nonterminal symbols\n\u25b7 \u03a3: finite alphabet of terminal symbols with \u03a3 \u2229 V = \u2205\n\u25b7 P: finite set of production rules P \u2286 V + \u00d7 (\u03a3 \u222a V )\u2217 (with V + := V V\u2217)\n\u25b7 S: the start symbol (axiom) with S \u2208 V\n\u25b7 The production rules generate the rewriting relation on (V \u222a \u03a3)\u2217\n\u2212 \u2192G:= {(uvw, uv\u2032w) | (v, v\u2032) \u2208 P, u, w\u2208 (\u03a3 \u222a V )\u2217}\nIts reflexive-transitive closure is denoted by \u2212 \u2192\u2217\nG.\n\u25b7 A path S \u2212 \u2192G \u03b11 \u2212 \u2192G . . .\u2212 \u2192G \u03b1l \u2212 \u2192G w is called a derivation of w.\n(A word w \u2208 \u03a3\u2217 is also called a sentence, a word \u03b1i \u2208 (V \u222a \u03a3)\u2217 a sentential form.)\nThe language generated by G is L(G) := {w \u2208 \u03a3\u2217 | S \u2212 \u2192\u2217\nG w}.\n\u25b7 A grammar is a restricted from of SRS specifically for generating/producing a\nlanguage. (But this distinction is only of technical nature) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (124)\nV = {E, N, D, X} S = E\n\u03a3 = {[, ], +, \u2217, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,\u2032 }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E \u2217 E])}\n\u222a {(N, DN), (N, \u03b5)} \u222a {(D, i) | i \u2208 Z 10} \u222a {(X, X\u2032), (X, x)}\nE \u2212 \u2192G [E \u2217 E]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (125)\nV = {E, N, D, X} S = E\n\u03a3 = {[, ], +, \u2217, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,\u2032 }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E \u2217 E])}\n\u222a {(N, DN), (N, \u03b5)} \u222a {(D, i) | i \u2208 Z 10} \u222a {(X, X\u2032), (X, x)}\nE \u2212 \u2192G [E \u2217 E]\n\u2212 \u2192G [[E + E] \u2217 E]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (126)\nV = {E, N, D, X} S = E\n\u03a3 = {[, ], +, \u2217, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,\u2032 }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E \u2217 E])}\n\u222a {(N, DN), (N, \u03b5)} \u222a {(D, i) | i \u2208 Z 10} \u222a {(X, X\u2032), (X, x)}\nE \u2212 \u2192G [E \u2217 E]\n\u2212 \u2192G [[E + E] \u2217 E]\n\u2212 \u2192G [[DN + E] \u2217 E]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (127)\nV = {E, N, D, X} S = E\n\u03a3 = {[, ], +, \u2217, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,\u2032 }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E \u2217 E])}\n\u222a {(N, DN), (N, \u03b5)} \u222a {(D, i) | i \u2208 Z 10} \u222a {(X, X\u2032), (X, x)}\nE \u2212 \u2192G [E \u2217 E]\n\u2212 \u2192G [[E + E] \u2217 E]\n\u2212 \u2192G [[DN + E] \u2217 E]\n\u2212 \u2192G [[DDN + X] \u2217 X]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (128)\nV = {E, N, D, X} S = E\n\u03a3 = {[, ], +, \u2217, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,\u2032 }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E \u2217 E])}\n\u222a {(N, DN), (N, \u03b5)} \u222a {(D, i) | i \u2208 Z 10} \u222a {(X, X\u2032), (X, x)}\nE \u2212 \u2192G [E \u2217 E]\n\u2212 \u2192G [[E + E] \u2217 E]\n\u2212 \u2192G [[DN + E] \u2217 E]\n\u2212 \u2192G [[DDN + X] \u2217 X]\n\u2212 \u2192G [[DD + X] \u2217 X]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (129)\nV = {E, N, D, X} S = E\n\u03a3 = {[, ], +, \u2217, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,\u2032 }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E \u2217 E])}\n\u222a {(N, DN), (N, \u03b5)} \u222a {(D, i) | i \u2208 Z 10} \u222a {(X, X\u2032), (X, x)}\nE \u2212 \u2192G [E \u2217 E]\n\u2212 \u2192G [[E + E] \u2217 E]\n\u2212 \u2192G [[DN + E] \u2217 E]\n\u2212 \u2192G [[DDN + X] \u2217 X]\n\u2212 \u2192G [[DD + X] \u2217 X]\n\u2212 \u2192G [[D3 + X] \u2217 X]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (130)\nV = {E, N, D, X} S = E\n\u03a3 = {[, ], +, \u2217, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,\u2032 }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E \u2217 E])}\n\u222a {(N, DN), (N, \u03b5)} \u222a {(D, i) | i \u2208 Z 10} \u222a {(X, X\u2032), (X, x)}\nE \u2212 \u2192G [E \u2217 E]\n\u2212 \u2192G [[E + E] \u2217 E]\n\u2212 \u2192G [[DN + E] \u2217 E]\n\u2212 \u2192G [[DDN + X] \u2217 X]\n\u2212 \u2192G [[DD + X] \u2217 X]\n\u2212 \u2192G [[D3 + X] \u2217 X]\n\u2212 \u2192G [[23 + X] \u2217 X]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (131)\nV = {E, N, D, X} S = E\n\u03a3 = {[, ], +, \u2217, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,\u2032 }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E \u2217 E])}\n\u222a {(N, DN), (N, \u03b5)} \u222a {(D, i) | i \u2208 Z 10} \u222a {(X, X\u2032), (X, x)}\nE \u2212 \u2192G [E \u2217 E]\n\u2212 \u2192G [[E + E] \u2217 E]\n\u2212 \u2192G [[DN + E] \u2217 E]\n\u2212 \u2192G [[DDN + X] \u2217 X]\n\u2212 \u2192G [[DD + X] \u2217 X]\n\u2212 \u2192G [[D3 + X] \u2217 X]\n\u2212 \u2192G [[23 + X] \u2217 X]\n\u2212 \u2192G [[23 + X\u2032] \u2217 X]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (132)\nV = {E, N, D, X} S = E\n\u03a3 = {[, ], +, \u2217, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,\u2032 }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E \u2217 E])}\n\u222a {(N, DN), (N, \u03b5)} \u222a {(D, i) | i \u2208 Z 10} \u222a {(X, X\u2032), (X, x)}\nE \u2212 \u2192G [E \u2217 E]\n\u2212 \u2192G [[E + E] \u2217 E]\n\u2212 \u2192G [[DN + E] \u2217 E]\n\u2212 \u2192G [[DDN + X] \u2217 X]\n\u2212 \u2192G [[DD + X] \u2217 X]\n\u2212 \u2192G [[D3 + X] \u2217 X]\n\u2212 \u2192G [[23 + X] \u2217 X]\n\u2212 \u2192G [[23 + X\u2032] \u2217 X]\n\u2212 \u2192G [[23 + x\u2032] \u2217 X]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (133)\nV = {E, N, D, X} S = E\n\u03a3 = {[, ], +, \u2217, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,\u2032 }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E \u2217 E])}\n\u222a {(N, DN), (N, \u03b5)} \u222a {(D, i) | i \u2208 Z 10} \u222a {(X, X\u2032), (X, x)}\nE \u2212 \u2192G [E \u2217 E]\n\u2212 \u2192G [[E + E] \u2217 E]\n\u2212 \u2192G [[DN + E] \u2217 E]\n\u2212 \u2192G [[DDN + X] \u2217 X]\n\u2212 \u2192G [[DD + X] \u2217 X]\n\u2212 \u2192G [[D3 + X] \u2217 X]\n\u2212 \u2192G [[23 + X] \u2217 X]\n\u2212 \u2192G [[23 + X\u2032] \u2217 X]\n\u2212 \u2192G [[23 + x\u2032] \u2217 X]\n\u2212 \u2192G [[23 + x\u2032] \u2217 X]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (134)\nV = {E, N, D, X} S = E\n\u03a3 = {[, ], +, \u2217, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,\u2032 }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E \u2217 E])}\n\u222a {(N, DN), (N, \u03b5)} \u222a {(D, i) | i \u2208 Z 10} \u222a {(X, X\u2032), (X, x)}\nE \u2212 \u2192G [E \u2217 E]\n\u2212 \u2192G [[E + E] \u2217 E]\n\u2212 \u2192G [[DN + E] \u2217 E]\n\u2212 \u2192G [[DDN + X] \u2217 X]\n\u2212 \u2192G [[DD + X] \u2217 X]\n\u2212 \u2192G [[D3 + X] \u2217 X]\n\u2212 \u2192G [[23 + X] \u2217 X]\n\u2212 \u2192G [[23 + X\u2032] \u2217 X]\n\u2212 \u2192G [[23 + x\u2032] \u2217 X]\n\u2212 \u2192G [[23 + x\u2032] \u2217 X]\n\u2212 \u2192G [[23 + x\u2032] \u2217 x\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (135)\nV = {E, N, D, X} S = E\n\u03a3 = {[, ], +, \u2217, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,\u2032 }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E \u2217 E])}\n\u222a {(N, DN), (N, \u03b5)} \u222a {(D, i) | i \u2208 Z 10} \u222a {(X, X\u2032), (X, x)}\nE \u2212 \u2192G [E \u2217 E]\n\u2212 \u2192G [[E + E] \u2217 E]\n\u2212 \u2192G [[DN + E] \u2217 E]\n\u2212 \u2192G [[DDN + X] \u2217 X]\n\u2212 \u2192G [[DD + X] \u2217 X]\n\u2212 \u2192G [[D3 + X] \u2217 X]\n\u2212 \u2192G [[23 + X] \u2217 X]\n\u2212 \u2192G [[23 + X\u2032] \u2217 X]\n\u2212 \u2192G [[23 + x\u2032] \u2217 X]\n\u2212 \u2192G [[23 + x\u2032] \u2217 X]\n\u2212 \u2192G [[23 + x\u2032] \u2217 x\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (136)\nV = {E, N, D, X} S = E\n\u03a3 = {[, ], +, \u2217, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,\u2032 }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E \u2217 E])}\n\u222a {(N, DN), (N, \u03b5)} \u222a {(D, i) | i \u2208 Z 10} \u222a {(X, X\u2032), (X, x)}\n! For convenience:\n\u25b7 \u201c\u2212 \u2192G\u201d is commonly used also for denoting/defining the actual rules P.\nThis makes it also easier to use \u201c (\u201d and \u201c )\u201d as terminal symbols.\n\u25b7 \u201c|\u201d is used for grouping rules with the same left-hand side.\nBut this assumes/requires that | \u0338\u2208V \u222a \u03a3.\nAbove grammar is thus typically defined by simply writing\nE \u2212 \u2192G DN | X | [E + E] | [E \u2217 E]\nN \u2212 \u2192G DN | \u03b5\nX \u2212 \u2192G X\u2032 | x\nD \u2212 \u2192G 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (137)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (138)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (139)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (140)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (141)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (142)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\n\u2212 \u2192G ((\u00acX op X ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (143)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\n\u2212 \u2192G ((\u00acX op X ) op F)\n\u2212 \u2192G ((\u00acX op x) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (144)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\n\u2212 \u2192G ((\u00acX op X ) op F)\n\u2212 \u2192G ((\u00acX op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (145)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\n\u2212 \u2192G ((\u00acX op X ) op F)\n\u2212 \u2192G ((\u00acX op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (146)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\n\u2212 \u2192G ((\u00acX op X ) op F)\n\u2212 \u2192G ((\u00acX op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X\u2032)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (147)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\n\u2212 \u2192G ((\u00acX op X ) op F)\n\u2212 \u2192G ((\u00acX op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X\u2032)\n\u2212 \u2192G ((\u00acx\u2032 op x) op X\u2032)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (148)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\n\u2212 \u2192G ((\u00acX op X ) op F)\n\u2212 \u2192G ((\u00acX op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X\u2032)\n\u2212 \u2192G ((\u00acx\u2032 op x) op X\u2032)\n\u2212 \u2192G ((\u00acx\u2032 op x) op x\u2032)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (149)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\n\u2212 \u2192G ((\u00acX op X ) op F)\n\u2212 \u2192G ((\u00acX op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X\u2032)\n\u2212 \u2192G ((\u00acx\u2032 op x) op X\u2032)\n\u2212 \u2192G ((\u00acx\u2032 op x) op x\u2032)\n\u2212 \u2192G ((\u00acx\u2032 op x) \u2227 x\u2032)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (150)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\n\u2212 \u2192G ((\u00acX op X ) op F)\n\u2212 \u2192G ((\u00acX op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X\u2032)\n\u2212 \u2192G ((\u00acx\u2032 op x) op X\u2032)\n\u2212 \u2192G ((\u00acx\u2032 op x) op x\u2032)\n\u2212 \u2192G ((\u00acx\u2032 op x) \u2227 x\u2032)\n\u2212 \u2192G ((\u00acx\u2032 \u2228 x) \u2227 x\u2032)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (151)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\n\u25b7 The order in which we apply the production rules is not predetermined,\ni.e. rewriting is usually nondeterministic.\n\u25b7 The grammars in this and the preceding examples are called context-free as\nwe/the computer can simply replace a single nonterminal/variable without\nkeeping track of the surrounding symbols (context).\n\u25b7 The main use of context-free grammars is to define the (syntax) trees\nunderlying well-formed expressions/formulas/terms/programs:\nRecall from DS: every binary tree can be encoded into a Dyck word, every\nDyck word encodes a binary tree.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Unary-binary trees and Dyck languages 60 (152)\nV = {S} \u03a3 = {[, ]}\nS \u2212 \u2192G [] | [S] | [SS]\n\"\u0014h\u0002\n[ ]\n\u0003ih\n[ ]\ni\u0015\u0014 hi \u0015#\n\u25b7 Dyck language wrt. a single type of brackets:\nEvery node is encoded by means of a matching pair of brackets,\n\u25b7 Often, a leaf is also encoded as the empty word.\nchildren are encoded by means of nesting.\n\u25b7 E.g. using the CYK algorithm (later) we can recover the tree from the word.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars for Python variable identifiers and keywords 61 (153)\nV = {K} S = K \u03a3 = {A, . . . ,Z, a, . . . ,z}\nK \u2212 \u2192G True | False | None | and | or | not | if | else | elif\nK \u2212 \u2192G . . .\nV \u2032 = {I, J, D, L} S\u2032 = I \u03a3\u2032 = {A, . . . ,Z, a, . . . ,z, , 0, . . . ,9}\nI \u2212 \u2192G\u2032 LJ\nJ \u2212 \u2192G\u2032 LJ | DJ | \u03b5\nD \u2212 \u2192G\u2032 0 | . . .| 9\nL \u2212 \u2192G\u2032 a | . . .| z | A | . . .| Z |\n\u25b7 Actually, an identifier must not be a keyword, i.e. we rather should give a\ngrammar for L(G\u2032) \\ L(G).\n\u25b7 For these simple grammars (called regular) we know how to compute from G\nand G\u2032 a grammar for L(G\u2032) \\ L(G).\n\u25b7 But in general, this is not possible\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n \u2208 N } 62(154)\nV = {S, A, B, C, L, R} \u03a3 = {a, b, c}\nS \u2212 \u2192G ASBC | R\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR | Lc\nCL \u2212 \u2192G Lc BL \u2212 \u2192G Lb AL \u2212 \u2192G La | a\nS \u2212 \u2192G A S BC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n \u2208 N } 62(155)\nV = {S, A, B, C, L, R} \u03a3 = {a, b, c}\nS \u2212 \u2192G ASBC | R\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR | Lc\nCL \u2212 \u2192G Lc BL \u2212 \u2192G Lb AL \u2212 \u2192G La | a\nS \u2212 \u2192G A S BC\n\u2212 \u2192G AA S BCBC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n \u2208 N } 62(156)\nV = {S, A, B, C, L, R} \u03a3 = {a, b, c}\nS \u2212 \u2192G ASBC | R\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR | Lc\nCL \u2212 \u2192G Lc BL \u2212 \u2192G Lb AL \u2212 \u2192G La | a\nS \u2212 \u2192G A S BC\n\u2212 \u2192G AA S BCBC\n\u2212 \u2192G AA R BCBC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n \u2208 N } 62(157)\nV = {S, A, B, C, L, R} \u03a3 = {a, b, c}\nS \u2212 \u2192G ASBC | R\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR | Lc\nCL \u2212 \u2192G Lc BL \u2212 \u2192G Lb AL \u2212 \u2192G La | a\nS \u2212 \u2192G A S BC\n\u2212 \u2192G AA S BCBC\n\u2212 \u2192G AA R BCBC\n\u2212 \u2192G AAB R CBC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n \u2208 N } 62(158)\nV = {S, A, B, C, L, R} \u03a3 = {a, b, c}\nS \u2212 \u2192G ASBC | R\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR | Lc\nCL \u2212 \u2192G Lc BL \u2212 \u2192G Lb AL \u2212 \u2192G La | a\nS \u2212 \u2192G A S BC\n\u2212 \u2192G AA S BCBC\n\u2212 \u2192G AA R BCBC\n\u2212 \u2192G AAB R CBC\n\u2212 \u2192G AABBC R C\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n \u2208 N } 62(159)\nV = {S, A, B, C, L, R} \u03a3 = {a, b, c}\nS \u2212 \u2192G ASBC | R\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR | Lc\nCL \u2212 \u2192G Lc BL \u2212 \u2192G Lb AL \u2212 \u2192G La | a\nS \u2212 \u2192G A S BC\n\u2212 \u2192G AA S BCBC\n\u2212 \u2192G AA R BCBC\n\u2212 \u2192G AAB R CBC\n\u2212 \u2192G AABBC R C\n\u2212 \u2192G AABBC L c\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n \u2208 N } 62(160)\nV = {S, A, B, C, L, R} \u03a3 = {a, b, c}\nS \u2212 \u2192G ASBC | R\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR | Lc\nCL \u2212 \u2192G Lc BL \u2212 \u2192G Lb AL \u2212 \u2192G La | a\nS \u2212 \u2192G A S BC\n\u2212 \u2192G AA S BCBC\n\u2212 \u2192G AA R BCBC\n\u2212 \u2192G AAB R CBC\n\u2212 \u2192G AABBC R C\n\u2212 \u2192G AABBC L c\n\u2212 \u2192G AABB L cc\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n \u2208 N } 62(161)\nV = {S, A, B, C, L, R} \u03a3 = {a, b, c}\nS \u2212 \u2192G ASBC | R\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR | Lc\nCL \u2212 \u2192G Lc BL \u2212 \u2192G Lb AL \u2212 \u2192G La | a\nS \u2212 \u2192G A S BC\n\u2212 \u2192G AA S BCBC\n\u2212 \u2192G AA R BCBC\n\u2212 \u2192G AAB R CBC\n\u2212 \u2192G AABBC R C\n\u2212 \u2192G AABBC L c\n\u2212 \u2192G AABB L cc\n\u2212 \u2192G AAB L bcc\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n \u2208 N } 62(162)\nV = {S, A, B, C, L, R} \u03a3 = {a, b, c}\nS \u2212 \u2192G ASBC | R\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR | Lc\nCL \u2212 \u2192G Lc BL \u2212 \u2192G Lb AL \u2212 \u2192G La | a\nS \u2212 \u2192G A S BC\n\u2212 \u2192G AA S BCBC\n\u2212 \u2192G AA R BCBC\n\u2212 \u2192G AAB R CBC\n\u2212 \u2192G AABBC R C\n\u2212 \u2192G AABBC L c\n\u2212 \u2192G AABB L cc\n\u2212 \u2192G AAB L bcc\n\u2212 \u2192G AA L bbcc\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n \u2208 N } 62(163)\nV = {S, A, B, C, L, R} \u03a3 = {a, b, c}\nS \u2212 \u2192G ASBC | R\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR | Lc\nCL \u2212 \u2192G Lc BL \u2212 \u2192G Lb AL \u2212 \u2192G La | a\nS \u2212 \u2192G A S BC\n\u2212 \u2192G AA S BCBC\n\u2212 \u2192G AA R BCBC\n\u2212 \u2192G AAB R CBC\n\u2212 \u2192G AABBC R C\n\u2212 \u2192G AABBC L c\n\u2212 \u2192G AABB L cc\n\u2212 \u2192G AAB L bcc\n\u2212 \u2192G AA L bbcc\n\u2212 \u2192G A L abbcc\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n \u2208 N } 62(164)\nV = {S, A, B, C, L, R} \u03a3 = {a, b, c}\nS \u2212 \u2192G ASBC | R\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR | Lc\nCL \u2212 \u2192G Lc BL \u2212 \u2192G Lb AL \u2212 \u2192G La | a\nS \u2212 \u2192G A S BC\n\u2212 \u2192G AA S BCBC\n\u2212 \u2192G AA R BCBC\n\u2212 \u2192G AAB R CBC\n\u2212 \u2192G AABBC R C\n\u2212 \u2192G AABBC L c\n\u2212 \u2192G AABB L cc\n\u2212 \u2192G AAB L bcc\n\u2212 \u2192G AA L bbcc\n\u2212 \u2192G A L abbcc\n\u2212 \u2192G aabbcc\nConsider S \u2212 \u2192\u2217\nG AAA S BCBCBCand fix the error.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201cnon-decreasing\u201d grammar for {anbncn | n \u2208 N } 63(165)\nV = {S, A, B, C, R, L} \u03a3 = {a, b, c}\nS \u2212 \u2192G abc | ARBC RB \u2212 \u2192G BR RC \u2212 \u2192G LBCC\nBL \u2212 \u2192G LB AL \u2212 \u2192G AA | AAR\nA \u2212 \u2192G a B \u2212 \u2192G b C \u2212 \u2192G c\nS \u2212 \u2192G A R BC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201cnon-decreasing\u201d grammar for {anbncn | n \u2208 N } 63(166)\nV = {S, A, B, C, R, L} \u03a3 = {a, b, c}\nS \u2212 \u2192G abc | ARBC RB \u2212 \u2192G BR RC \u2212 \u2192G LBCC\nBL \u2212 \u2192G LB AL \u2212 \u2192G AA | AAR\nA \u2212 \u2192G a B \u2212 \u2192G b C \u2212 \u2192G c\nS \u2212 \u2192G A R BC\n\u2212 \u2192G AB R C\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201cnon-decreasing\u201d grammar for {anbncn | n \u2208 N } 63(167)\nV = {S, A, B, C, R, L} \u03a3 = {a, b, c}\nS \u2212 \u2192G abc | ARBC RB \u2212 \u2192G BR RC \u2212 \u2192G LBCC\nBL \u2212 \u2192G LB AL \u2212 \u2192G AA | AAR\nA \u2212 \u2192G a B \u2212 \u2192G b C \u2212 \u2192G c\nS \u2212 \u2192G A R BC\n\u2212 \u2192G AB R C\n\u2212 \u2192G AB L BCC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201cnon-decreasing\u201d grammar for {anbncn | n \u2208 N } 63(168)\nV = {S, A, B, C, R, L} \u03a3 = {a, b, c}\nS \u2212 \u2192G abc | ARBC RB \u2212 \u2192G BR RC \u2212 \u2192G LBCC\nBL \u2212 \u2192G LB AL \u2212 \u2192G AA | AAR\nA \u2212 \u2192G a B \u2212 \u2192G b C \u2212 \u2192G c\nS \u2212 \u2192G A R BC\n\u2212 \u2192G AB R C\n\u2212 \u2192G AB L BCC\n\u2212 \u2192G A L BBCC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201cnon-decreasing\u201d grammar for {anbncn | n \u2208 N } 63(169)\nV = {S, A, B, C, R, L} \u03a3 = {a, b, c}\nS \u2212 \u2192G abc | ARBC RB \u2212 \u2192G BR RC \u2212 \u2192G LBCC\nBL \u2212 \u2192G LB AL \u2212 \u2192G AA | AAR\nA \u2212 \u2192G a B \u2212 \u2192G b C \u2212 \u2192G c\nS \u2212 \u2192G A R BC\n\u2212 \u2192G AB R C\n\u2212 \u2192G AB L BCC\n\u2212 \u2192G A L BBCC\n\u2212 \u2192G AA R BBCC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201cnon-decreasing\u201d grammar for {anbncn | n \u2208 N } 63(170)\nV = {S, A, B, C, R, L} \u03a3 = {a, b, c}\nS \u2212 \u2192G abc | ARBC RB \u2212 \u2192G BR RC \u2212 \u2192G LBCC\nBL \u2212 \u2192G LB AL \u2212 \u2192G AA | AAR\nA \u2212 \u2192G a B \u2212 \u2192G b C \u2212 \u2192G c\nS \u2212 \u2192G A R BC\n\u2212 \u2192G AB R C\n\u2212 \u2192G AB L BCC\n\u2212 \u2192G A L BBCC\n\u2212 \u2192G AA R BBCC\n\u2212 \u2192G AAB R BCC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201cnon-decreasing\u201d grammar for {anbncn | n \u2208 N } 63(171)\nV = {S, A, B, C, R, L} \u03a3 = {a, b, c}\nS \u2212 \u2192G abc | ARBC RB \u2212 \u2192G BR RC \u2212 \u2192G LBCC\nBL \u2212 \u2192G LB AL \u2212 \u2192G AA | AAR\nA \u2212 \u2192G a B \u2212 \u2192G b C \u2212 \u2192G c\nS \u2212 \u2192G A R BC\n\u2212 \u2192G AB R C\n\u2212 \u2192G AB L BCC\n\u2212 \u2192G A L BBCC\n\u2212 \u2192G AA R BBCC\n\u2212 \u2192G AAB R BCC\n\u2212 \u2192G AABB R CC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201cnon-decreasing\u201d grammar for {anbncn | n \u2208 N } 63(172)\nV = {S, A, B, C, R, L} \u03a3 = {a, b, c}\nS \u2212 \u2192G abc | ARBC RB \u2212 \u2192G BR RC \u2212 \u2192G LBCC\nBL \u2212 \u2192G LB AL \u2212 \u2192G AA | AAR\nA \u2212 \u2192G a B \u2212 \u2192G b C \u2212 \u2192G c\nS \u2212 \u2192G A R BC\n\u2212 \u2192G AB R C\n\u2212 \u2192G AB L BCC\n\u2212 \u2192G A L BBCC\n\u2212 \u2192G AA R BBCC\n\u2212 \u2192G AAB R BCC\n\u2212 \u2192G AABB R CC\n\u2212 \u2192G AABB L BBCCC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201cnon-decreasing\u201d grammar for {anbncn | n \u2208 N } 63(173)\nV = {S, A, B, C, R, L} \u03a3 = {a, b, c}\nS \u2212 \u2192G abc | ARBC RB \u2212 \u2192G BR RC \u2212 \u2192G LBCC\nBL \u2212 \u2192G LB AL \u2212 \u2192G AA | AAR\nA \u2212 \u2192G a B \u2212 \u2192G b C \u2212 \u2192G c\nS \u2212 \u2192G A R BC\n\u2212 \u2192G AB R C\n\u2212 \u2192G AB L BCC\n\u2212 \u2192G A L BBCC\n\u2212 \u2192G AA R BBCC\n\u2212 \u2192G AAB R BCC\n\u2212 \u2192G AABB R CC\n\u2212 \u2192G AABB L BBCCC\n\u2212 \u2192G AAB L BBBCCC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201cnon-decreasing\u201d grammar for {anbncn | n \u2208 N } 63(174)\nV = {S, A, B, C, R, L} \u03a3 = {a, b, c}\nS \u2212 \u2192G abc | ARBC RB \u2212 \u2192G BR RC \u2212 \u2192G LBCC\nBL \u2212 \u2192G LB AL \u2212 \u2192G AA | AAR\nA \u2212 \u2192G a B \u2212 \u2192G b C \u2212 \u2192G c\nS \u2212 \u2192G A R BC\n\u2212 \u2192G AB R C\n\u2212 \u2192G AB L BCC\n\u2212 \u2192G A L BBCC\n\u2212 \u2192G AA R BBCC\n\u2212 \u2192G AAB R BCC\n\u2212 \u2192G AABB R CC\n\u2212 \u2192G AABB L BBCCC\n\u2212 \u2192G AAB L BBBCCC\n\u2212 \u2192G AA L BBBBCCC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201cnon-decreasing\u201d grammar for {anbncn | n \u2208 N } 63(175)\nV = {S, A, B, C, R, L} \u03a3 = {a, b, c}\nS \u2212 \u2192G abc | ARBC RB \u2212 \u2192G BR RC \u2212 \u2192G LBCC\nBL \u2212 \u2192G LB AL \u2212 \u2192G AA | AAR\nA \u2212 \u2192G a B \u2212 \u2192G b C \u2212 \u2192G c\nS \u2212 \u2192G A R BC\n\u2212 \u2192G AB R C\n\u2212 \u2192G AB L BCC\n\u2212 \u2192G A L BBCC\n\u2212 \u2192G AA R BBCC\n\u2212 \u2192G AAB R BCC\n\u2212 \u2192G AABB R CC\n\u2212 \u2192G AABB L BBCCC\n\u2212 \u2192G AAB L BBBCCC\n\u2212 \u2192G AA L BBBBCCC\n\u2212 \u2192G AAABBBBCCC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201cnon-decreasing\u201d grammar for {anbncn | n \u2208 N } 63(176)\nV = {S, A, B, C, R, L} \u03a3 = {a, b, c}\nS \u2212 \u2192G abc | ARBC RB \u2212 \u2192G BR RC \u2212 \u2192G LBCC\nBL \u2212 \u2192G LB AL \u2212 \u2192G AA | AAR\nA \u2212 \u2192G a B \u2212 \u2192G b C \u2212 \u2192G c\nS \u2212 \u2192G A R BC\n\u2212 \u2192G AB R C\n\u2212 \u2192G AB L BCC\n\u2212 \u2192G A L BBCC\n\u2212 \u2192G AA R BBCC\n\u2212 \u2192G AAB R BCC\n\u2212 \u2192G AABB R CC\n\u2212 \u2192G AABB L BBCCC\n\u2212 \u2192G AAB L BBBCCC\n\u2212 \u2192G AA L BBBBCCC\n\u2212 \u2192G AAABBBBCCC\n\u2212 \u2192\u2217\nG aaabbbccc\nIn every rule: |LHS| \u2264 |RHS|; such grammars are called context-sensitive.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201clinear\u201d grammar for {anbn | n \u2208 N } 64(177)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G aSb | ab\nS \u2212 \u2192G aSb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201clinear\u201d grammar for {anbn | n \u2208 N } 64(178)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G aSb | ab\nS \u2212 \u2192G aSb\n\u2212 \u2192G aaSbb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201clinear\u201d grammar for {anbn | n \u2208 N } 64(179)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G aSb | ab\nS \u2212 \u2192G aSb\n\u2212 \u2192G aaSbb\n\u2212 \u2192G aaaSbbb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201clinear\u201d grammar for {anbn | n \u2208 N } 64(180)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G aSb | ab\nS \u2212 \u2192G aSb\n\u2212 \u2192G aaSbb\n\u2212 \u2192G aaaSbbb\n\u2212 \u2192G aaaabbbb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201cright-linear\u201d grammar for {an | n \u2208 N } 65(181)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G aS | a\nS \u2212 \u2192G aS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201cright-linear\u201d grammar for {an | n \u2208 N } 65(182)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G aS | a\nS \u2212 \u2192G aS\n\u2212 \u2192G aaS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201cright-linear\u201d grammar for {an | n \u2208 N } 65(183)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G aS | a\nS \u2212 \u2192G aS\n\u2212 \u2192G aaS\n\u2212 \u2192G aaaS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A \u201cright-linear\u201d grammar for {an | n \u2208 N } 65(184)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G aS | a\nS \u2212 \u2192G aS\n\u2212 \u2192G aaS\n\u2212 \u2192G aaaS\n\u2212 \u2192G aaaa\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Graphs and right-linear grammars 66 (185)\na b\nc d\nV = {S, Xa, Xb, Xc, Xd} \u03a3 = {a, b, c, d}\nS \u2212 \u2192G aXa | bXb | cXc | dXd\nXa \u2212 \u2192G bXb | cXc | \u03b5 X b \u2212 \u2192G dXd | bXb | \u03b5 X c \u2212 \u2192G bXb | dXd | \u03b5 X d \u2212 \u2192G cXc | \u03b5\n\u25b7 Every finite directed graph G gives rise to a right-linear (regular) grammar G\nso that L(G) is the set of finite paths of G.\n\u25b7 This is one reason why \u2217 is used both for languages and the\nreflexive-transitive closure.\n\u25b7 Similarly, every regular grammar G gives rise to a graph (which can be read\nas a 1TM that \u201caccepts\u201d L(G)).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "SRS vs. grammars 67 (186)\nF Every grammar G = (V, \u03a3, P, S) is an SRS on the alphabet \u0393 = V \u222a \u03a3.\nL For every SRS (\u03a3, R) there exists a grammar G so that\nL(G) = {w#w\u2032 | w \u2212 \u2192\u2217\nR w\u2032}\ni.e. L(G) generates all pairs of words so that the first word can be rewritten\nto the second word wrt. R. (\u201c#\u201d serves as delimiter again.)\n\u25b7 For each a \u2208 \u03a3 introduce new variables Xa, Za and rules Xa \u2212 \u2192G a, Za \u2212 \u2192G a.\nFor each rule (\u03b1, \u03b2) \u2208 R introduce the rule (\u03b1\u2032, \u03b2\u2032) where \u03b1\u2032, \u03b2\u2032 are obtained\nfrom \u03b1, \u03b2by replacing every a \u2208 \u03a3 by Za.\nIntroduce a new variable S as start symbol and add the rules\nS \u2212 \u2192G XaYa# | XaYaS Y aXb \u2212 \u2192G XbYa Ya# \u2212 \u2192G #Za\ni.e. S generates Xa1 . . . Xal#Za1 . . . Zal\nS \u2212 \u2192\u2217\nG Xa1 Ya1 . . . XalYal# \u2212 \u2192\u2217\nG Xa1 . . . Xal#Za1 . . . Zal \u2212 \u2192\u2217\nG . . .\nfor some nondeterministically chosen w = a1 . . . al \u2208 \u03a3\u2217 where the modified\nrules of R can only be applied to Za1 . . . Zal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "4 String rewriting, grammars, Chomsky hierarchy\nString rewriting and grammars: abstraction of \u201cmathematical calculations\u201d\nChomsky hierarchy and normal forms\nGrammars vs. Turing machines (recognizable languages)\nUndecidable problems and Post correspondence problem\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Chomsky hierarchy 69 (188)\nD Chomsky hierarchy\n\u25b7 Type 0 or general phrase structure grammar:\nno restrictions, i.e. P \u2286 V + \u00d7 (\u03a3 \u222a V )\u2217 with |P| < \u221e.\n\u25b7 Type 1 or context-sensitive grammar:\n|v| \u2264 |v\u2032| for all (v, v\u2032) \u2208 P (so v\u2032 \u0338= \u03b5 as v \u2208 V +)\n\u25b7 Type 2 or context-free grammar:\n|v| \u2264 |v\u2032| for all (v, v\u2032) \u2208 P and v \u2208 V\n\u25b7 Type 3 or (right) regular grammar:\n|v| \u2264 |v\u2032| for all (v, v\u2032) \u2208 P and v \u2208 V and v\u2032 \u2208 \u03a3 \u222a \u03a3V\n! For grammars of type 1, 2, 3 the usual convention is to allow (S, \u03b5) \u2208 P but\nto require that S does not appear on the right of any rule.\n\u25b7 A context-free rule (v, v\u2032) \u2208 V \u00d7 (\u03a3 \u222a V )+ can be applied without taking the\ncontext into consideration, i.e. it is conceptually simpler for a human.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Chomsky hierarchy 69 (189)\nD Chomsky hierarchy\n\u25b7 Type 0 or general phrase structure grammar:\nno restrictions, i.e. P \u2286 V + \u00d7 (\u03a3 \u222a V )\u2217 with |P| < \u221e.\n\u25b7 Type 1 or context-sensitive grammar:\n|v| \u2264 |v\u2032| for all (v, v\u2032) \u2208 P (so v\u2032 \u0338= \u03b5 as v \u2208 V +)\n\u25b7 Type 2 or context-free grammar:\n|v| \u2264 |v\u2032| for all (v, v\u2032) \u2208 P and v \u2208 V\n\u25b7 Type 3 or (right) regular grammar:\n|v| \u2264 |v\u2032| for all (v, v\u2032) \u2208 P and v \u2208 V and v\u2032 \u2208 \u03a3 \u222a \u03a3V\nD A language L \u2286 \u03a3\u2217 is of type x if L = L(G) for some grammar G of type x.\n\u25b7 Examples:\n\u25b7 {an | n \u2208 N } is of type 0, 1, 2, 3 (regular).\n\u25b7 {anbn | n \u2208 N } is of type 0, 1, 2 (context-free).\n\u25b7 {anbncn . . . zn | n \u2208 N } is of type 0, 1 (context-sensitive).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Chomsky hierarchy and programming languages 70 (190)\n\u25b7 Grammars are used for defining and parsing languages.\n\u25b7 E.g. programming languages typically use grammars of both type 2 and 3:\n\u25b7 type 3 (regular) grammars are used for defining valid/correct lexemes:\n\u25b7 E.g. reserved words like class, def or variable identifiers.\n\u25b7 the lexemes form the alphabet from which the program is formed.\n\u25b7 lexemes usually only have a very simple structure, the grammar serves as\ncompressed infinite dictionary.\n\u25b7 type 2 (context-free) grammars are used for defining\nvalid/correct/well-structured/well-formed programs (= words over lexemes).\n\u25b7 As most programming languages support arithemtic expressions and\npropositional formulas (for assertions), i.e. more complex versions of Dyck\nlanguages, we actually need to use at least context-free grammars here.\n\u25b7 See lex and yacc\nAs we will see: for both types 2, 3 we can decide the word problem efficiently.",
      "propositional formulas (for assertions), i.e. more complex versions of Dyck\nlanguages, we actually need to use at least context-free grammars here.\n\u25b7 See lex and yacc\nAs we will see: for both types 2, 3 we can decide the word problem efficiently.\n\u25b7 Some natural languages use context-sensitive rules in their grammar/syntax\n(cf. here).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Normal forms for grammars 71 (191)\nT Kuroda normal form for type 0: For every grammar G = (V, \u03a3, P, S) we can\ncompute a grammar G\u2032 = (V \u2032, \u03a3, P\u2032, S\u2032) with L(G) = L(G\u2032) and\nP\u2032 \u2286 V V\u00d7 V V\u222a V \u00d7 ({\u03b5} \u222a\u03a3 \u222a V \u222a V V)\n\u25b7 First, introduce for every a \u2208 \u03a3 a variable Xa.\nNext, replace a by Xa in all rules P.\nFinally, add the rules (Xa, a) for all a \u2208 \u03a3.\nNow all rules are of the form P \u2286 V + \u00d7 V \u2217 \u222a V \u00d7 \u03a3.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Normal forms for grammars 71 (192)\nT Kuroda normal form for type 0: For every grammar G = (V, \u03a3, P, S) we can\ncompute a grammar G\u2032 = (V \u2032, \u03a3, P\u2032, S\u2032) with L(G) = L(G\u2032) and\nP\u2032 \u2286 V V\u00d7 V V\u222a V \u00d7 ({\u03b5} \u222a\u03a3 \u222a V \u222a V V)\n\u25b7 If there is at least one rule (\u03b1, \u03b2) \u2208 P with |\u03b1| > |\u03b2|:\nFirst, introduce a variable X\u03b5 and the rule (X\u03b5, \u03b5).\nNext replace every rule (\u03b1, \u03b3) \u2208 V + \u00d7 V \u2217 with |\u03b1| > |\u03b3| by\n\u03b1 \u2212 \u2192G \u03b3 X\u03b5 . . . X\u03b5| {z }\n|\u03b1|\u2212|\u03b3| copies\ns.t. G is now almost of type 1: the only exception is the rule (X\u03b5, \u03b5).\n! Can only happen, if G is of type 0.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Normal forms for grammars 71 (193)\nT Kuroda normal form for type 0: For every grammar G = (V, \u03a3, P, S) we can\ncompute a grammar G\u2032 = (V \u2032, \u03a3, P\u2032, S\u2032) with L(G) = L(G\u2032) and\nP\u2032 \u2286 V V\u00d7 V V\u222a V \u00d7 ({\u03b5} \u222a\u03a3 \u222a V \u222a V V)\n\u25b7 Replace all rules r = (A, C1C2C3 . . . Cl) \u2208 P with l \u2265 3 by\nA \u2212 \u2192G Rr;1\nRr;1 \u2212 \u2192G C1Rr;2\n...\nRr;l \u2212 \u2192G Cl\nThe newly introduced auxiliary variables Rr;i are used to write the sentential\nform Ci . . . Cl from left to right.\nBy construction Rr;1 can only produce C1 . . . Cl.\nFrequently, instead of Rr;i simply [CiCi+1 . . . Cl] is used so that\n[CiCi+1 . . . Cl] \u2212 \u2192G Ci[Ci+1 . . . Cl]\nbut this requires that [ and ] are not already used.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Normal forms for grammars 71 (194)\nT Kuroda normal form for type 0: For every grammar G = (V, \u03a3, P, S) we can\ncompute a grammar G\u2032 = (V \u2032, \u03a3, P\u2032, S\u2032) with L(G) = L(G\u2032) and\nP\u2032 \u2286 V V\u00d7 V V\u222a V \u00d7 ({\u03b5} \u222a\u03a3 \u222a V \u222a V V)\n\u25b7 Replace all rules r = (A1A2A3 . . . Ak, C1 . . . Cl) \u2208 P with k \u2265 3 by\nA1 \u2212 \u2192G Rr;1\nRr;1A2 \u2212 \u2192G C1Rr;2\n...\nRr;k\u22121Ak \u2212 \u2192G Ck\u22121Ck (if k = l)\nRr;k\u22121Ak \u2212 \u2192G Ck\u22121Rr;k (if k < l)\nRr;k \u2212 \u2192G CkRr;k+1 (k \u2264 l by construction)\n...\nRr;l \u2212 \u2192G Cl\nThe newly introduced auxiliary variables Rr;i apply the again the rule r by\nreading A1 . . . Ak and replacing it by C1 . . . CkCk+1 . . . Cl.\n(Using the bracket convention, you can e.g. also write [Ai+1 . . . Ak; Ci . . . Cl] for\nRr;i if i < k; and simply [Ci . . . Cl] if i \u2265 l; recall after the first step we have k \u2264 l.)\n! Can only happen, if G is of type 0 or 1.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Normal forms for grammars 71 (195)\nT Kuroda normal form for type 0: For every grammar G = (V, \u03a3, P, S) we can\ncompute a grammar G\u2032 = (V \u2032, \u03a3, P\u2032, S\u2032) with L(G) = L(G\u2032) and\nP\u2032 \u2286 V V\u00d7 V V\u222a V \u00d7 ({\u03b5} \u222a\u03a3 \u222a V \u222a V V)\n\u25b7 Every derivation wrt. the original grammar can be simulated by the\nconstructed grammar\nWe skip the proof that the new grammar cannot generate any additional\nwords.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Normal forms for grammars 71 (196)\nT Kuroda normal form for type 0: For every grammar G = (V, \u03a3, P, S) we can\ncompute a grammar G\u2032 = (V \u2032, \u03a3, P\u2032, S\u2032) with L(G) = L(G\u2032) and\nP\u2032 \u2286 V V\u00d7 V V\u222a V \u00d7 ({\u03b5} \u222a\u03a3 \u222a V \u222a V V)\nL Rules of the form (A, B) \u2208 V \u00d7 V can be contracted:\nConsider the the finite graph (V, {(A, C) | (A, C) \u2208 P}).\nIf A can reach A\u2032 in this graph and\n\u25b7 (\u03b1A\u2032\u03b2, C\u2032D\u2032) \u2208 P with \u03b1\u03b2 \u2208 V \u222a {\u03b5}:\nadd the rule (\u03b1A\u03b2, C\u2032D\u2032) as \u03b1A\u03b2 \u2212 \u2192\u2217\nG \u03b1A\u2032\u03b2 \u2212 \u2192G C\u2032D\u2032\n\u25b7 (A\u2032, a\u2032) \u2208 P: add the rule (A, a\u2032) as A \u2212 \u2192\u2217\nG A\u2032 \u2212 \u2192G a\u2032 (a\u2032 \u2208 \u03a3).\n\u25b7 (A\u2032, \u03b5) \u2208 P: add the rule (A, \u03b5) as A \u2212 \u2192\u2217\nG A\u2032 \u2212 \u2192G \u03b5 (only for type 0).\nFinally, remove all rules (A, B) \u2208 V \u00d7 V .\n\u25b7 These rules are sometimes called unary/unit/chain rules; the new rules simply\njump to the end of all possible \u201cchains\u201d A0 \u2212 \u2192G A1 \u2212 \u2192G A2 \u2212 \u2192G . . .\u2212 \u2192G Al.\n\u25b7 Any such chain cannot produce a terminal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Normal forms for grammars 71 (197)\nT Kuroda normal form for type 0: For every grammar G = (V, \u03a3, P, S) we can\ncompute a grammar G\u2032 = (V \u2032, \u03a3, P\u2032, S\u2032) with L(G) = L(G\u2032) and\nP\u2032 \u2286 V V\u00d7 V V\u222a V \u00d7 ({\u03b5} \u222a\u03a3 \u222a V \u222a V V)\nC Kuroda normal form for type 1: For every grammar G of type 1 we can\ncompute a grammar G\u2032 = (V \u2032, \u03a3, P\u2032, S\u2032) with L(G) = L(G\u2032) and\nP\u2032 \u2286 V V\u00d7 V V\u222a V \u00d7 (\u03a3 \u222a V \u222a V 2)\nC Kuroda normal form for type 2: For every grammar G of type 2 we can\ncompute a grammar G\u2032 = (V \u2032, \u03a3, P\u2032, S\u2032) with L(G) = L(G\u2032) and\nP\u2032 \u2286 V \u00d7 (\u03a3 \u222a V \u222a V 2)\nC Chomsky normal form for type 2: For every grammar G of type 2 we can\ncompute a grammar G\u2032 = (V \u2032, \u03a3, P\u2032, S\u2032) with L(G) = L(G\u2032) and\nP\u2032 \u2286 V \u00d7 (\u03a3 \u222a V 2)\n(contract chain/unit/unary rules)No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Normal forms for grammars 71 (198)\nT Kuroda normal form for type 0: For every grammar G = (V, \u03a3, P, S) we can\ncompute a grammar G\u2032 = (V \u2032, \u03a3, P\u2032, S\u2032) with L(G) = L(G\u2032) and\nP\u2032 \u2286 V V\u00d7 V V\u222a V \u00d7 ({\u03b5} \u222a\u03a3 \u222a V \u222a V V)\n\u25b7 Remarks:\n\u25b7 By the Kuroda normal form the only difference between type 1 and type 0 is\nthe use of (arbitrary) \u03b5-rules A \u2212 \u2192G \u03b5.\n\u25b7 And the difference between type 2 and 1 is the use of the context-sensitive\nrules AB \u2212 \u2192G CD.\n\u25b7 Using the Chomsky normal form, one can also show that\nGreibach normal form for type 1: For every grammar G of type 2 we can\ncompute a grammar G\u2032 = (V \u2032, \u03a3, P\u2032, S\u2032) with L(G) = L(G\u2032) and\nP\u2032 \u2286 V \u00d7 \u03a3V \u2217\nRecall: G is of type 3 (regular, right-linear) if P \u2286 V \u00d7 (\u03a3 \u222a \u03a3V ).\nSo, the main difference is that in type 2 we may use also nonlinear rules of the\nform X \u2212 \u2192G aX1 . . . Xk (k \u2265 2) (tail recursion vs. general recursion).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kuroda and Chomsky normal forms 72 (199)\n\u25b7 Consider the grammar with V = {S, A, B, C, R, L}, \u03a3 = {a, b, c} and\nS \u2212 \u2192G aSBC | aR R \u2212 \u2192G Lc L \u2212 \u2192G b\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR\nCL \u2212 \u2192G Lc BL \u2212 \u2192G Lb\n\u25b7 First introduce Xa \u2212 \u2192G a, Xb \u2212 \u2192G b, Xc \u2212 \u2192G c,\nXa \u2212 \u2192G a X b \u2212 \u2192G b X c \u2212 \u2192G c\nS \u2212 \u2192G XaSBC | XaR R \u2212 \u2192G LXc L \u2212 \u2192G Xb\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR\nCL \u2212 \u2192G LXc BL \u2212 \u2192G LXb\n(Rather annoying.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kuroda and Chomsky normal forms 72 (200)\n\u25b7 Consider the grammar with V = {S, A, B, C, R, L}, \u03a3 = {a, b, c} and\nS \u2212 \u2192G aSBC | aR R \u2212 \u2192G Lc L \u2212 \u2192G b\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR\nCL \u2212 \u2192G Lc BL \u2212 \u2192G Lb\n! Very often, (square) brackets are used to denote auxiliary variables:\nE.g. instead of Xa also [a] is used assuming that \u201c [, ]\u201d are not used as symbols.\n[a] \u2212 \u2192G a [b] \u2212 \u2192G b [c] \u2212 \u2192G c\nS \u2212 \u2192G [a]SBC | [a]R R \u2212 \u2192G L[c] L \u2212 \u2192G [b]\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR\nCL \u2212 \u2192G L[c] BL \u2212 \u2192G L[b]\nwhere [a], [b], [c] have to be read as three nonterminals.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kuroda and Chomsky normal forms 72 (201)\n\u25b7 Consider the grammar with V = {S, A, B, C, R, L}, \u03a3 = {a, b, c} and\nS \u2212 \u2192G aSBC | aR R \u2212 \u2192G Lc L \u2212 \u2192G b\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR\nCL \u2212 \u2192G Lc BL \u2212 \u2192G Lb\n\u25b7 Next we need to replace S \u2212 \u2192G [a]SBC :\nUsing the bracket-convention, we can actually simply unravel this rule into\nS \u2212 \u2192G [a][SBC ] [ SBC ] \u2212 \u2192G S[BC] [ BC] \u2212 \u2192G BC\nyielding\n[a] \u2212 \u2192G a [b] \u2212 \u2192G b [c] \u2212 \u2192G c\n[SBC ] \u2212 \u2192G S[BC] [ BC] \u2212 \u2192G BC\nS \u2212 \u2192G [a][SBC ] | [a]R R \u2212 \u2192G L[c] L \u2212 \u2192G [b]\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR\nCL \u2212 \u2192G L[c] BL \u2212 \u2192G L[b]\n(cf. replacing z = x3 by z = xy, y= x2; replacing x\u2032\u2032 = \u03bbx by x\u2032 = \u03bby, y\u2032 = x; the\nnew rules \u201csimulate\u201d the old rules.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kuroda and Chomsky normal forms 72 (202)\n\u25b7 Consider the grammar with V = {S, A, B, C, R, L}, \u03a3 = {a, b, c} and\nS \u2212 \u2192G aSBC | aR R \u2212 \u2192G Lc L \u2212 \u2192G b\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR\nCL \u2212 \u2192G Lc BL \u2212 \u2192G Lb\n\u25b7 Finally, we need to replace RCB \u2212 \u2192G BCR e.g. by means to\nR \u2212 \u2192G [CB; BCR] [ CB; BCR]C \u2212 \u2192G B[B; CR] [ B; CR]B \u2212 \u2192G CR\n(read [CG; BCR] as \u201creplace CG by BCR\u201d.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kuroda and Chomsky normal forms 72 (203)\n\u25b7 Consider the grammar with V = {S, A, B, C, R, L}, \u03a3 = {a, b, c} and\nS \u2212 \u2192G aSBC | aR R \u2212 \u2192G Lc L \u2212 \u2192G b\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR\nCL \u2212 \u2192G Lc BL \u2212 \u2192G Lb\n\u25b7 Kuroda normal form:\n[a] \u2212 \u2192G a [b] \u2212 \u2192G b [c] \u2212 \u2192G c\n[SBC ] \u2212 \u2192G S[BC] [ BC] \u2212 \u2192G BC\nR \u2212 \u2192G [CB; BCR]\n[CB; BCR] C \u2212 \u2192G B[B; CR] [ B; CR]B \u2212 \u2192G CR\nS \u2212 \u2192G [a][SBC ] | [a]R R \u2212 \u2192G L[c] L \u2212 \u2192G [b]\nRB \u2212 \u2192G BR RC \u2212 \u2192G CR\nCL \u2212 \u2192G L[c] BL \u2212 \u2192G L[b]\nThe main point is that the new rules for [CB; BCR] and [B; CR] will\n\u201cdeadlock\u201d if they cannot read the required symbols, and thus the derivation\ncannot reach a terminal word.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kuroda and Chomsky normal forms 72 (204)\n\u25b7 Consider the grammar with V = {S, A, B, C, R, L}, \u03a3 = {a, b, c} and\nS \u2212 \u2192G aSBC | aR R \u2212 \u2192G Lc L \u2212 \u2192G b\nRB \u2212 \u2192G BR RCB \u2212 \u2192G BCR RC \u2212 \u2192G CR\nCL \u2212 \u2192G Lc BL \u2212 \u2192G Lb\n\u25b7 Just as a reminder \u201c [CB; BCR]\u201d has to be read as a single\nnonteriminal/variable (identifier). Alternatively, we could write XCB;BCR , e.g.\nXa \u2212 \u2192G a X b \u2212 \u2192G b X c \u2212 \u2192G c\nXSBC \u2212 \u2192G SXBC XBC \u2212 \u2192G BC\nR \u2212 \u2192G XCB;BCR\nXCB;BCR C \u2212 \u2192G BXB;CR XB;CRB \u2212 \u2192G CR\nS \u2212 \u2192G XaXSBC | XaR R \u2212 \u2192G LXc L \u2212 \u2192G Xb\nRB \u2212 \u2192G BR RC \u2212 \u2192G CR\nCL \u2212 \u2192G LXc BL \u2212 \u2192G LXb\nWe could further replace the linear rule R \u2212 \u2192G XCB;BCR by\nRC \u2212 \u2192G BXB;CR\nwhich makes the nonterminal XCB;BCR unreachable so that we can remove th\nXCB;BCR and all rules in which it is used.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "\u03b5-rules in context-free grammars 73 (205)\n\u25b7 Formally, the Chomsky hierarchy does not allow \u03b5-rules (X \u2212 \u2192G \u03b5) for all\ngrammars of type greater than 0.\n! By the Kuroda normal form, \u03b5-rules are also the only difference between\ngrammars of type 0 and type 1; as we will see, the word problem is\nundecidable for type 0, but decidable for type 1.\n\u25b7 The usual convention for type 1, 2, 3 grammars is that the start symbol S\nmay be rewritten to \u03b5 with the restriction that S must not be used on\nright-hand side of any rule, i.e. the grammar looks like this:\nS \u2212 \u2192G S\u2032 | \u03b5 S \u2032 \u2212 \u2192G . . .\nwith S\u2032 the start symbol of a grammar of type 1, 2, 3 in the strict sense (no\n\u03b5-rules). (The word problem stays decidable.)\n\u25b7 Still, allowing arbitrary \u03b5-rules in grammars of type 2, 3 (not type 1) does not\nchange the type of the language.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Removing \u03b5-rules in context-free grammars 74 (206)\nL Let G = (V, \u03a3, P, S) be a grammar with P \u2286 V \u00d7 ({\u03b5} \u222a\u03a3 \u222a V \u222a V V).\nCompute inductively E := E|V | by means of\nE0 := {X | (X, \u03b5) \u2208 P}\nEk+1 := Ek \u222a {X | (X, \u03b1) \u2208 P, \u03b1\u2208 E\u2217\nk }| {z }\nX\u2212 \u2192G\u03b1\u2212 \u2192\u2217\nG\u03b5\nDefine G\u2032 = (V, \u03a3, P\u2032, S) by (all combinations; Kuroda reduces this to two cases)\nP\u2032 := P \\ {(X, \u03b5) \u2208 P}\n\u222a { (X, Y) | (X, Y Z) \u2208 P, Z\u2208 E}| {z }\nX\u2212 \u2192GY Z\u2212 \u2192\u2217\nGY\n\u222a{(X, Z) | (X, Y Z), Y\u2208 E}| {z }\nX\u2212 \u2192GY Z\u2212 \u2192\u2217\nGZ\nThen L(G) \\ {\u03b5} = L(G\u2032) and \u03b5 \u2208 L(G) iff S \u2208 E.\n\u25b7 Consider the grammar G\u2032\u2032 with P\u2032\u2032 = P\u2032 \u222a {(X, \u03b5) | X \u2208 E}.\n\u25b7 Obviously, L(G) \u2286 L(G\u2032\u2032); we show by induction/well-ordering of N that the\ncontracted \u03b5-rules {(X, \u03b5) | X \u2208 E}do not add any additional word s.t. the\nrules P\u2032 already suffice.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Removing \u03b5-rules in context-free grammars 74 (207)\nL Let G = (V, \u03a3, P, S) be a grammar with P \u2286 V \u00d7 ({\u03b5} \u222a\u03a3 \u222a V \u222a V V).\nCompute inductively E := E|V | by means of\nE0 := {X | (X, \u03b5) \u2208 P}\nEk+1 := Ek \u222a {X | (X, \u03b1) \u2208 P, \u03b1\u2208 E\u2217\nk }| {z }\nX\u2212 \u2192G\u03b1\u2212 \u2192\u2217\nG\u03b5\nDefine G\u2032 = (V, \u03a3, P\u2032, S) by (all combinations; Kuroda reduces this to two cases)\nP\u2032 := P \\ {(X, \u03b5) \u2208 P}\n\u222a { (X, Y) | (X, Y Z) \u2208 P, Z\u2208 E}| {z }\nX\u2212 \u2192GY Z\u2212 \u2192\u2217\nGY\n\u222a{(X, Z) | (X, Y Z), Y\u2208 E}| {z }\nX\u2212 \u2192GY Z\u2212 \u2192\u2217\nGZ\nThen L(G) \\ {\u03b5} = L(G\u2032) and \u03b5 \u2208 L(G) iff S \u2208 E.\n\u25b7 Consider a shortest derivation S \u2212 \u2192\u2217\nG\u2032\u2032 w of a word w \u0338\u2208 L(G\u2032) \u222a {\u03b5}\nS \u2212 \u2192\u2217\nG\u2032\u2032 \u03b1\u2032X\u2032\u03b2\u2032 \u2212 \u2192G\u2032\u2032 \u03b1\u2032\u03b1X\u03b2\u03b2 \u2032 \u2212 \u2192G\u2032\u2032 \u03b1\u2032\u03b1\u03b2\u03b2\u2032 \u2212 \u2192\u2217\nG\u2032\u2032 w\nwith X\u2032 \u2212 \u2192G \u03b5. Then X\u2032 is introduced by some rule (X\u2032, \u03b1X\u03b2) so that we can\nshorten the derivation either by (X\u2032, \u03b5) \u2208 P\u2032\u2032 (if \u03b1\u03b2 = \u03b5) or (X\u2032, X) \u2208 P\u2032 (if\n\u03b1\u03b2 = Y ).No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Removing \u03b5-rules in context-free grammars 74 (208)\nL Let G = (V, \u03a3, P, S) be a grammar with P \u2286 V \u00d7 ({\u03b5} \u222a\u03a3 \u222a V \u222a V V).\nCompute inductively E := E|V | by means of\nE0 := {X | (X, \u03b5) \u2208 P}\nEk+1 := Ek \u222a {X | (X, \u03b1) \u2208 P, \u03b1\u2208 E\u2217\nk }| {z }\nX\u2212 \u2192G\u03b1\u2212 \u2192\u2217\nG\u03b5\nDefine G\u2032 = (V, \u03a3, P\u2032, S) by (all combinations; Kuroda reduces this to two cases)\nP\u2032 := P \\ {(X, \u03b5) \u2208 P}\n\u222a { (X, Y) | (X, Y Z) \u2208 P, Z\u2208 E}| {z }\nX\u2212 \u2192GY Z\u2212 \u2192\u2217\nGY\n\u222a{(X, Z) | (X, Y Z), Y\u2208 E}| {z }\nX\u2212 \u2192GY Z\u2212 \u2192\u2217\nGZ\nThen L(G) \\ {\u03b5} = L(G\u2032) and \u03b5 \u2208 L(G) iff S \u2208 E.\nC \u03b5 \u2208 L(G) is decidable for context-free grammars.\nC L(G) = \u2205 is decidable for context-free grammars:\n\u25b7 Replace every rule (X, a) \u2208 P by (X, \u03b5); then L(G) \u0338= \u2205 iff S \u2208 E.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Removing \u03b5-rules in context-free grammars: example 75 (209)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G T T T \u2212 \u2192G \u03b5 T \u2212 \u2192G aSb\n\u25b7 First transform the grammar into Kuroda normal form:\nIntroduce variables for nonterimals and \u03b5:\nS \u2212 \u2192G T T T \u2212 \u2192G X\u03b5 T \u2212 \u2192G XaSXb\nXa \u2212 \u2192G a X b \u2212 \u2192G b X \u03b5 \u2212 \u2192G \u03b5\nThen \u201cmake right-hand sides at most quadractic\u201d:\nS \u2212 \u2192G T T T \u2212 \u2192G X\u03b5 T \u2212 \u2192G Xa[SXb] [ SXb] \u2212 \u2192G SXb\nXa \u2212 \u2192G a X b \u2212 \u2192G b X \u03b5 \u2212 \u2192G \u03b5\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Removing \u03b5-rules in context-free grammars: example 75 (210)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G T T T \u2212 \u2192G X\u03b5 T \u2212 \u2192G Xa[SXb] [ SXb] \u2212 \u2192G SXb\nXa \u2212 \u2192G a X b \u2212 \u2192G b X \u03b5 \u2212 \u2192G \u03b5\n\u25b7 Compute (\u201ccolor\u201d) the nonterminals that can produce \u03b5:\n\u25b7 E0 = {X\u03b5}\n\u25b7 E1 = {X\u03b5, T} as T \u2212 \u2192G X\u03b5 \u2212 \u2192G \u03b5.\n\u25b7 E2 = {X\u03b5, T, S} as S \u2212 \u2192G T T\u2212 \u2192G X\u03b5T \u2212 \u2192G T \u2212 \u2192G X\u03b5 \u2212 \u2192G \u03b5.\nRemove the \u03b5-rule X\u03b5 \u2212 \u2192G \u03b5 and adapt the rules\nS \u2212 \u2192G T T T \u2212 \u2192G X\u03b5 T \u2212 \u2192G Xa[SXb] [ SXb] \u2212 \u2192G SXb\nS \u2212 \u2192G T T \u2212 \u2192G Xa[SXb] [ SXb] \u2212 \u2192G Xb\nXa \u2212 \u2192G a X b \u2212 \u2192G b\nS\u2032 \u2212 \u2192G S S \u2032 \u2212 \u2192G \u03b5\nwith S\u2032 the new start symbol.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Removing \u03b5-rules in context-free grammars: example 75 (211)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G T T T \u2212 \u2192G X\u03b5 T \u2212 \u2192G Xa[SXb] [ SXb] \u2212 \u2192G SXb\nS \u2212 \u2192G T T \u2212 \u2192G Xa[SXb] [ SXb] \u2212 \u2192G Xb\nXa \u2212 \u2192G a X b \u2212 \u2192G b S \u2032 \u2212 \u2192G S S \u2032 \u2212 \u2192G \u03b5\n! In general, the resulting grammar will contain \u201cnonproductive\u201d nonterminals:\nFor a context-free grammar these can be computed by replacing (conceptually)\nall terminals by \u03b5 and then determining the nonterminals that can produce \u03b5,\ni.e.:\nI.B. Every terminal is productive (P0 = \u03a3).\nI.S. A nonterminal X is productive iff (X, \u03b3) \u2208 P with \u03b3 consisting only of\nproductive symbols (Pk+1 = Pk \u222a {X \u2208 V | (X, \u03b3) \u2208 P, \u03b3\u2208 P\u2217\nk}).\nAll nonproductive nonterminals (including all rules that use an unproductive\nnonterminal) can be discarded.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Removing \u03b5-rules in context-free grammars: example 75 (212)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G T T T \u2212 \u2192G X\u03b5 T \u2212 \u2192G Xa[SXb] [ SXb] \u2212 \u2192G SXb\nS \u2212 \u2192G T T \u2212 \u2192G Xa[SXb] [ SXb] \u2212 \u2192G Xb\nXa \u2212 \u2192G a X b \u2212 \u2192G b S \u2032 \u2212 \u2192G S S \u2032 \u2212 \u2192G \u03b5\n! Finally, the context-free grammar might contain variables that are\nunreachable from the start symbol, i.e. cannot occur in any derivation.\nThese can be computed by considering the \u201ccall graph\u201d\n(V, {(X, Y) | (X, Y) \u2208 P} \u222a {(X, Y), (X, Z) | (X, Y Z) \u2208 P})\nI.B. The axiom S is reachable (R0 = {S}).\nI.S. A nonterminal Y is reachable iff (X, \u03b1Y \u03b3) \u2208 P for some \u03b1, \u03b3\u2208 (V \u222a \u03a3)\u2217\n(Rk+1 = Rk \u222a {Y \u2208 V | (X, \u03b1Y \u03b3) \u2208 P, X\u2208 R\u2217\nk}).\nAny nonterminal that is not reachable starting from S in this graph can be\nremoved (including all rules in which it occurs) .\n(Note: For \u03b5-rules and productive nonterminals we move backwards along the rules, for\nreachable nonterminals we move forwards; but all are variants of reachability/refl.-transitive",
      "removed (including all rules in which it occurs) .\n(Note: For \u03b5-rules and productive nonterminals we move backwards along the rules, for\nreachable nonterminals we move forwards; but all are variants of reachability/refl.-transitive\nclosure, see also Horn clause resolution/exercises.)No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Remving \u03b5-rules in context-free grammars: Dyck grammar 76 (213)\n\u25b7 The usual definition of the Dyck grammar wrt. one pair of brackets is:\nS \u2212 \u2192G \u03b5 | [S] | SS\n(The Kuroda normal makes it much simpler in general to remove \u03b5-rules (e.g. consider a\nrule X \u2212 \u2192G Y Y . . . Y) but is actually not required.)\n\u25b7 As S \u2212 \u2192G \u03b5, we add the rules\n\u25b7 S \u2212 \u2192G [] as shortcut for S \u2212 \u2192G [S] \u2212 \u2192G []\n\u25b7 S \u2212 \u2192G S as shortcut for S \u2212 \u2192G SS \u2212 \u2192G S\nDiscarding S \u2212 \u2192G \u03b5 yields\nS \u2212 \u2192G [S] | SS | [] | S\nThe chain rule (cycle) S \u2212 \u2192G S can trivally be discarded:\nS \u2212 \u2192G [S] | SS | []\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Simplifying context-free grammars 77 (214)\n1 Introduce auxiliary variables s.t. P \u2286 V \u00d7 ({\u03b5} \u222a\u03a3 \u222a V \u222a V V)\n\u25b7 This includes of course regular grammars.\n2 Contract \u03b5-rules:\n\u25b7 This might introduce additional chain rules.\n\u25b7 If \u03b5 \u2208 L(G), add a new start symbol S\u2032 and the new rule S\u2032 \u2212 \u2192G \u03b5.\n3 For Chomsky normal form: contract all chain/unary/unit rules.\n4 Discard all unproductive nonterminals:\n\u25b7 This only removes rules and nonterminals, but might make some nonterminals\nunreachable.\n5 Discard all unreachable nonterminals:\n\u25b7 This only removes rules and nonterminals, and cannot re-introduce\nunproductive nonterminals, chain rules or \u03b5-rules.\n! Removing \u03b5-rules before transforming the grammar into Kuroda normal form\ncan lead to an exponential blow up.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "4 String rewriting, grammars, Chomsky hierarchy\nString rewriting and grammars: abstraction of \u201cmathematical calculations\u201d\nChomsky hierarchy and normal forms\nGrammars vs. Turing machines (recognizable languages)\nUndecidable problems and Post correspondence problem\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Formal grammars and recognizable languages 79 (216)\nT A language is recognizable iff it can be generated by a type- 0 grammar.\n\u25b7 Assume L = L(G) for some grammar G:\nBy using a vector alphabet we can assume the TM has two tapes.\nThe first tape remembers the input; the second tape is used to\nnondeterministically simulate every derivation wrt. G.\nThe TM accepts iff the second tape contains also the input.\n! If G is of type 1, then every simulation that tries to access tape cells not\ncontaining the original input can terminated.\nThat is: a context-sensitive language can be accepted by a 1TM that only\nmakes use of the cells that initially stored the input.\n\u25b7 For the other direction, we treat the transition rules of the TM as rewrite rules\non the configurations (next).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Formal grammars and recognizable languages 79 (217)\nT A language is recognizable iff it can be generated by a type- 0 grammar.\n\u25b7 Assume L = L(M), i.e. there is a deterministic 1TM\nM = (Q, \u03a3, \u0393, \u03b4, q0, \u25a1, {qf }) that accepts L.\nFor the grammar, we use the nonterminals {\n\u0014qx\ny\n\u0015\n,\n\u0014x\ny\n\u0015\n| q \u2208 Q, x\u2208 \u0393, y\u2208 \u03a3}.\nThe lower component remembers the original word, the upper component\nsimulates the tape of the 1TM.\nThe start symbol first nondeterministically a word w \u2208 L(M) with the\nmaximal space required by a run of M on w:\nS \u2212 \u2192G BIB |\n\u0014q0\u25a1\n\u25a1\n\u0015\nB \u2212 \u2192G\n\u0014\u25a1\n\u25a1\n\u0015\nB |\n\u0014\u25a1\n\u25a1\n\u0015\nI \u2212 \u2192G I\n\u0014a\na\n\u0015\n|\n\u0014q0a\na\n\u0015\n(a \u2208 \u03a3)\nFor each transition rule ((q, a), (r, b, d)) \u2208 \u03b4 introduce the production rules:\n\u0014qa\nx\n\u0015\u0014z\ny\n\u0015\n\u2212 \u2192G\n\u0014b\nx\n\u0015\u0014rz\ny\n\u0015\n(d = +1)\n\u0014qa\nx\n\u0015\n\u2212 \u2192G\n\u0014rb\nx\n\u0015\n(d = 0)\n\u0014z\ny\n\u0015\u0014qa\nx\n\u0015\n\u2212 \u2192G\n\u0014rz\ny\n\u0015\u0014b\nx\n\u0015\n(d = \u22121)\nFinally, we can always terminate the simulation by means of the rules\n\u0014z\nx\n\u0015\n\u2212 \u2192G x\n\u0014qf z\nx\n\u0015\n\u2212 \u2192G x\n\u0014z\n\u25a1\n\u0015\n\u2212 \u2192G \u03b5\n\u0014qf z\n\u25a1\n\u0015\n\u2212 \u2192G \u03b5 (z \u2208 \u0393, x\u2208 \u03a3)\nNote that we only allow to \u201cforget\u201d the final state.",
      "\u0015\n\u2212 \u2192G\n\u0014b\nx\n\u0015\u0014rz\ny\n\u0015\n(d = +1)\n\u0014qa\nx\n\u0015\n\u2212 \u2192G\n\u0014rb\nx\n\u0015\n(d = 0)\n\u0014z\ny\n\u0015\u0014qa\nx\n\u0015\n\u2212 \u2192G\n\u0014rz\ny\n\u0015\u0014b\nx\n\u0015\n(d = \u22121)\nFinally, we can always terminate the simulation by means of the rules\n\u0014z\nx\n\u0015\n\u2212 \u2192G x\n\u0014qf z\nx\n\u0015\n\u2212 \u2192G x\n\u0014z\n\u25a1\n\u0015\n\u2212 \u2192G \u03b5\n\u0014qf z\n\u25a1\n\u0015\n\u2212 \u2192G \u03b5 (z \u2208 \u0393, x\u2208 \u03a3)\nNote that we only allow to \u201cforget\u201d the final state.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Formal grammars and recognizable languages 79 (218)\nT A language is recognizable iff it can be generated by a type- 0 grammar.\nC There is no type- 0 grammar for L\u03b5.\nC The word problem for a grammar G = (V, \u03a3, P, S) asks to decide whether\nS \u2212 \u2192\u2217\nG w for a given word w. It is thus only semideciable (for type 0).\nC G is of type 1 iff it can be recognized by a 1TM that only accesses the tape\non cells originally containing input symbols.\n\u25b7 Follows from the sketched constructions:\nIf G is type 1, we only need to nondeterministically simulate derivations\nS \u2212 \u2192G \u03b11 \u2212 \u2192G \u03b12 \u2212 \u2192G . . .\u2212 \u2192G \u03b1l\nas long as |s| \u2264 |\u03b10| \u2264 |\u03b11| \u2264. . .\u2264 |\u03b1l| \u2264 |w|\nIf M only uses the space given by the original input, we do not introduce or\nremove\n\u0014\u25a1\n\u25a1\n\u0015\nand the grammar is immediately of type 1.\nD A 1TM that only uses the space originally occupied by the input\n\u25b7 i.e. if q0w \u2212 \u2192\u2217\nM \u03b1q\u03b2, then |\u03b1\u03b2| \u2264 |w|.",
      "as long as |s| \u2264 |\u03b10| \u2264 |\u03b11| \u2264. . .\u2264 |\u03b1l| \u2264 |w|\nIf M only uses the space given by the original input, we do not introduce or\nremove\n\u0014\u25a1\n\u25a1\n\u0015\nand the grammar is immediately of type 1.\nD A 1TM that only uses the space originally occupied by the input\n\u25b7 i.e. if q0w \u2212 \u2192\u2217\nM \u03b1q\u03b2, then |\u03b1\u03b2| \u2264 |w|.\nis called an linear bounded automaton (LBA).No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-sensitive (type 1) languages 80 (219)\nT L(G) is decidable if G is at least of type 1 (resp. accepted by an LBA).\n\u25b7 Again, use BFS to enumerate the words \u03b1 \u2208 (V \u222a \u03a3)\u2217 with S \u2212 \u2192k\nG \u03b1 for\nincreasing k. Along any path (derivation)\nS \u2212 \u2192G \u03b11 \u2212 \u2192G \u03b12 \u2212 \u2192G . . .\nwe have |\u03b1i| \u2264 |\u03b1i+1| by definition of type 1.\nGiven some word w \u2208 \u03a3\u2217 thus terminate the BFS for all |\u03b1i| with |w| < |\u03b1i|.\n\u25b7 Using BFS can again lead to an exponential runtime and also space/memory\nconsumption as we might traverse the perfect tree \u03a3\u2264|w|.\n\u25b7 As we will see, for type 2 and 3 much more efficient decision procedures exists,\ni.e. we can parse such grammars efficiently.\n\u25b7 Note that this is a purely syntactic criterion.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-sensitive (type 1) languages 80 (220)\nT L(G) is decidable if G is at least of type 1 (resp. accepted by an LBA).\n\u25b7 Again, use BFS to enumerate the words \u03b1 \u2208 (V \u222a \u03a3)\u2217 with S \u2212 \u2192k\nG \u03b1 for\nincreasing k. Along any path (derivation)\nS \u2212 \u2192G \u03b11 \u2212 \u2192G \u03b12 \u2212 \u2192G . . .\nwe have |\u03b1i| \u2264 |\u03b1i+1| by definition of type 1.\nGiven some word w \u2208 \u03a3\u2217 thus terminate the BFS for all |\u03b1i| with |w| < |\u03b1i|.\n! Some important results on LBAs/context-sensitive languages (w/o proofs):\n\u25b7 Linear speedup theorem: A vector/tuple alphabet allows an LBA to actually\nuse O(|w|) space and thus also to simulate a constant number of tapes.\n\u25b7 Savitch\u2019s theorem: An LBA can be simulated by a deterministic 1TM that uses\nat most O(|w|2) space; it is still open if deterministic LBAs suffice.\nSo, it might be the case the deterministic LBAs can do less than\n(nondeterministic) LBAs.\n\u25b7 Immerman\u2013Szelepcs\u00b4 enyi theorem: for a given LBAM we can construct an\nLBA M\u2032 with L(M\u2032) = L(M), i.e. languages of type 1 are closed wrt.",
      "at most O(|w|2) space; it is still open if deterministic LBAs suffice.\nSo, it might be the case the deterministic LBAs can do less than\n(nondeterministic) LBAs.\n\u25b7 Immerman\u2013Szelepcs\u00b4 enyi theorem: for a given LBAM we can construct an\nLBA M\u2032 with L(M\u2032) = L(M), i.e. languages of type 1 are closed wrt.\ncomplement in contrast to languages of type 0.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-sensitive (type 1) languages 80 (221)\nT L(G) is decidable if G is at least of type 1 (resp. accepted by an LBA).\n\u25b7 Again, use BFS to enumerate the words \u03b1 \u2208 (V \u222a \u03a3)\u2217 with S \u2212 \u2192k\nG \u03b1 for\nincreasing k. Along any path (derivation)\nS \u2212 \u2192G \u03b11 \u2212 \u2192G \u03b12 \u2212 \u2192G . . .\nwe have |\u03b1i| \u2264 |\u03b1i+1| by definition of type 1.\nGiven some word w \u2208 \u03a3\u2217 thus terminate the BFS for all |\u03b1i| with |w| < |\u03b1i|.\n! There are infinitely many decidable languages of type 0 but not of type 1:\n\u25b7 Informally: L \u2208 NSPACE(nk) if there is some (nondeterministic) 1TM M\nwith L = L(M) using at most O(nk) space for every input w (with k \u2208 N 0).\nNSPACE(nk) \u228a NSPACE(nk+1) for every k \u2208 N 0 (cf. complexity zoo).\nIf L \u2208 NSPACE(nk), we can \u201cinflate\u201d the input L\u2032 = {w10|w|k\n| w \u2208 L} s.t.\nL\u2032 \u2208 NSPACE(n), i.e. these languages are \u201cpolynomially-bounded expansive\u201d.\n\u25b7 We can also bound the length of any run on input w by f(|w|): as a 1TM can\nonly write a single symbol in one step, this also bounds the space by f(|w|).",
      "| w \u2208 L} s.t.\nL\u2032 \u2208 NSPACE(n), i.e. these languages are \u201cpolynomially-bounded expansive\u201d.\n\u25b7 We can also bound the length of any run on input w by f(|w|): as a 1TM can\nonly write a single symbol in one step, this also bounds the space by f(|w|).\n\u25b7 Reachability in Petri nets (via TMs a special case of SRS) is decidable but\nonly in non-elementary time and space (Mayr, 1981).No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammar vs. machine 81 (222)\n! Some things are easier to do with a grammar than with a 1TM and v.v.:\nL If L1, L2 are both of type \u03c4 \u2208 {0, 1, 2, 3}, then so is L1 \u222a L2.\nIf L1, L2 are both of type \u03c4 \u2208 {0, 1}, then so is L1 \u2229 L2.\nIf L1, L2 are both of type 1, then so is L1 \\ L2.\n\u25b7 L1 \u222a L2:\nFor i = 1, 2 let Gi = (Vi, \u03a3, Pi, Si) be a grammar with Li = L(Gi).\nwlog V1 \u2229 V2 = \u2205 (simply tag variables with i) and S \u0338\u2208 V1 \u222a V2 \u222a \u03a3.\nThen (V1 \u222a V2 \u222a {S}, \u03a3, P1 \u222a P2 \u222a {(S, S1), (S, S2)}, S) produces L1 \u222a L2.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammar vs. machine 81 (223)\n! Some things are easier to do with a grammar than with a 1TM and v.v.:\nL If L1, L2 are both of type \u03c4 \u2208 {0, 1, 2, 3}, then so is L1 \u222a L2.\nIf L1, L2 are both of type \u03c4 \u2208 {0, 1}, then so is L1 \u2229 L2.\nIf L1, L2 are both of type 1, then so is L1 \\ L2.\n\u25b7 L1 \u2229 L2:\nFor i = 1, 2 let Mi be an 1TM (LBA) with Li = L(Mi).\nLet M be the following TM:\n\u25b7 Run M1 on input w; if M1 does not accept w, loop forever.\n\u25b7 Run M2 on input w; if M2 does not accept w, loop forever.\n\u25b7 Accept w.\nIf M1, M2 are both LBAs, then also M will be linearly bounded.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammar vs. machine 81 (224)\n! Some things are easier to do with a grammar than with a 1TM and v.v.:\nL If L1, L2 are both of type \u03c4 \u2208 {0, 1, 2, 3}, then so is L1 \u222a L2.\nIf L1, L2 are both of type \u03c4 \u2208 {0, 1}, then so is L1 \u2229 L2.\nIf L1, L2 are both of type 1, then so is L1 \\ L2.\n\u25b7 L1 \\ L2:\nFor i = 1, 2 let Mi be an LBA with Li = L(Mi).\nBy the Immerman\u2013Szelepcs\u00b4 enyi theorem we can compute an LBA M\u2032\n2 from\nM2 with L(M\u2032\n2) = L2, and thus \u201c M1; M\u2032\n2\u201d yields an LBA M with\nL(M) = L1 \u2229 L2 = L1 \\ L.\n\u25b7 Recall: if L and L are both recognizable/semidecidable/type 0/rec.\nenumerable, then L is decidable: but L = \u03a3\u2217 \\ L.\n\u25b7 Moving from grammar to \u201cmachine\u201d to grammar is conceptually a simple\napproach for computing a grammar for L(G1) \\ L(G2).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammar vs. machine 81 (225)\n! Some things are easier to do with a grammar than with a 1TM and v.v.:\nL If L1, L2 are both of type \u03c4 \u2208 {0, 1, 2, 3}, then so is L1 \u222a L2.\nIf L1, L2 are both of type \u03c4 \u2208 {0, 1}, then so is L1 \u2229 L2.\nIf L1, L2 are both of type 1, then so is L1 \\ L2.\n\u25b7 As we will see:\n\u25b7 The intersection of two languages of type 2 is in general only of type 1.\n\u25b7 Languages of type 3 are also closed wrt. boolean (set) operations.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "4 String rewriting, grammars, Chomsky hierarchy\nString rewriting and grammars: abstraction of \u201cmathematical calculations\u201d\nChomsky hierarchy and normal forms\nGrammars vs. Turing machines (recognizable languages)\nUndecidable problems and Post correspondence problem\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Undecidable problems for grammars of type 0 83(227)\nF As unrestricted grammars (type 0) can simulate 1TMs, the following\nproblems are undecidable for unrestricted grammars G, G1, G2 in general:\n\u25b7 w \u2208 L(G), specifically \u03b5 \u2208 L(G) (see LH and LH,\u03b5)\n\u25b7 L(G) = \u03a3\u2217 (see LH,\u2217)\n\u25b7 L(G) = \u2205 (see LH,?)\n\u25b7 L(G1) = L(G2) (see L=)\n\u25b7 L(G1) \u2286 L(G2) (e.g. as we may choose L(G1) = \u03a3\u2217)\n\u25b7 |L(G)| = \u221e (see LH,\u2217)\n\u25b7 there exists a grammar G\u2032 of type 1 with L(G) = L(G\u2032). (Rice)\n\u25b7 |L(G1) \u2229 L(G2)| = \u221e (as we may choose L(G1) = \u03a3\u2217)\n\u25b7 |L(G1) \u2229 L(G2)| = 0 (as we may choose L(G1) = \u03a3\u2217)\nRemarks:\n\u25b7 Type 3: all of these problems are decidable (later).\n\u25b7 Type 2: \u03b5 \u2208 L(G) and L(G) = \u2205 are decidable (as seen); also |L(G)| = \u221e.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Undecidable problems for grammars of type 0 83(228)\nF As unrestricted grammars (type 0) can simulate 1TMs, the following\nproblems are undecidable for unrestricted grammars G, G1, G2 in general:\n\u25b7 w \u2208 L(G), specifically \u03b5 \u2208 L(G) (see LH and LH,\u03b5)\n\u25b7 L(G) = \u03a3\u2217 (see LH,\u2217)\n\u25b7 L(G) = \u2205 (see LH,?)\n\u25b7 L(G1) = L(G2) (see L=)\n\u25b7 L(G1) \u2286 L(G2) (e.g. as we may choose L(G1) = \u03a3\u2217)\n\u25b7 |L(G)| = \u221e (see LH,\u2217)\n\u25b7 there exists a grammar G\u2032 of type 1 with L(G) = L(G\u2032). (Rice)\n\u25b7 |L(G1) \u2229 L(G2)| = \u221e (as we may choose L(G1) = \u03a3\u2217)\n\u25b7 |L(G1) \u2229 L(G2)| = 0 (as we may choose L(G1) = \u03a3\u2217)\nRemarks:\n\u25b7 Type 1: L(G) = \u2205 and |L(G)| = \u221e are undecidable.\n\u25b7 We cannot remove \u03b5-rules, unproductive or unreachable nonterminals for type\n0, 1 in general.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Post correspondence problem 84 (229)\nD An instance of the Post correspondence problem (PCP) (cf. Emil Post)\nconsists of a finite number of word pairs\n(u(1), v(1)), . . . ,(u(n), v(n)) \u2208 \u03a3\u2217 \u00d7 \u03a3\u2217\nand asks whether there is a nonempty sequence i1, i2 . . . , il \u2208 [n] (l >1) s.t.\nu(i1)u(i2) . . . u(il) = v(i1)v(i2) . . . v(il)\n! A PCP instance has either no or infinitely many solutions.\n\u25b7 E.g. consider the PCP instance\n(u(1), v(1)) := (1, 101) ( u(2), v(2)) := (10, 00) ( u(3), v(3)) := (011, 11)\nA solution is 1, 3, 2, 3 as:\nu(1)u(3)u(2)u(3) = 1 011 10 011 = 101110011\nv(1)v(3)v(2)v(3) = 101 11 00 11 = 101110011\n\u25b7 Try to solve the following using a computer (or cf. wikipedia (de))\n(001, 0) (01 , 011) (01 , 101) (10 , 001)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Post correspondence problem and grammars 85 (230)\nT For every (deterministic) 1TM M we can compute a PCP instance\n(u(1), v(1)), . . . ,(u(n), v(n)) \u2208 \u03a3\u2217 \u00d7 \u03a3\u2217\nso that \u03b5 \u2208 L(M) iff the PCP instance has a solution.\n\u25b7 The main idea is that we use both components to simulate the same run\nq0\u03b5 \u2212 \u2192G \u03b11q1\u03b21 \u2212 \u2192G . . .\u2212 \u2192G \u03b1lql\u03b2l \u2212 \u2192G qf \u03b5\nwhere the run is encoded as: (\u201c#\u201d as delimiter; wlog the TM empties its tape)\n#q0#\u03b11q1\u03b21# . . .#\u03b1lql\u03b2l#qf #\nThe central idea is to let the first component trail behind by 1 step:\n\u0014 #\n#q0#\n\u0015\u0014 q0#\n\u03b11q1\u03b21#\n\u0015\n. . .\n\u0014 \u03b1iqi\u03b2i#\n\u03b1i+1qi+1\u03b2i+1#\n\u0015\n=\n\u0014 #q0# . . .#\u03b1iqi\u03b2i#\n#q0# . . .#\u03b1iqi\u03b2i#\u03b1i+1qi+1\u03b2i+1#\n\u0015\nwhere the word pairs are represented as vectors.\nAs the first component trails behind, we can choose the PCP word pairs so\nthat the upper (first) component \u201creads qia from below\u201d in order to correctly\nencode the successor in the lower component again.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Post correspondence problem and grammars 85 (231)\nT For every (deterministic) 1TM M we can compute a PCP instance\n(u(1), v(1)), . . . ,(u(n), v(n)) \u2208 \u03a3\u2217 \u00d7 \u03a3\u2217\nso that \u03b5 \u2208 L(M) iff the PCP instance has a solution.\n\u25b7 This leads to the following PCP instance (wlog M empties its tape, F = {qf }):\n{(#, #q0#), (qf ##, #), (aqf , qf ), (qf a, qf ) | a \u2208 \u0393}\n\u222a {(x, x) | x \u2208 \u0393 \u222a {#}}\n\u222a {(qa, q\u2032a\u2032) | ((q, a), (q\u2032, a\u2032, 0)) \u2208 \u03b4}\n\u222a {(q#, q\u2032a\u2032#) | ((q, \u25a1), (q\u2032, a\u2032, 0)) \u2208 \u03b4}\n\u222a {(qa, a\u2032q\u2032) | ((q, a), (q\u2032, a\u2032, +1)) \u2208 \u03b4}\n\u222a {(q#, a\u2032q\u2032#) | ((q, \u25a1), (q\u2032, a\u2032, +1)) \u2208 \u03b4}\n\u222a {(bqa, q\u2032ba\u2032) | ((q, a), (q\u2032, a\u2032, \u22121)) \u2208 \u03b4, b\u2208 \u0393}\n\u222a {(bq#, q\u2032ba\u2032#) | ((q, \u25a1), (q\u2032, a\u2032, \u22121)) \u2208 \u03b4, b\u2208 \u0393}\n\u222a {(#qa, #q\u2032\u25a1a\u2032) | ((q, a), (q\u2032, a\u2032, \u22121)) \u2208 \u03b4}\nWe still need to enforce that a solution starts with (#, #q0#):\nThe main idea is to replace each pair (u(i), v(i)) = (a1 . . . ak, b1 . . . , bl) by\n(a1|a2|. . .|ak|, |b1|b2|. . .|bl) except for (#, #q0#) which becomes",
      "\u222a {(#qa, #q\u2032\u25a1a\u2032) | ((q, a), (q\u2032, a\u2032, \u22121)) \u2208 \u03b4}\nWe still need to enforce that a solution starts with (#, #q0#):\nThe main idea is to replace each pair (u(i), v(i)) = (a1 . . . ak, b1 . . . , bl) by\n(a1|a2|. . .|ak|, |b1|b2|. . .|bl) except for (#, #q0#) which becomes\n(|#|, |#|q0|#); and add the additional pair ($, |$) as \u201cterminator\u201d (cf. Sipser).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Post correspondence problem and grammars 85 (232)\nT For every (deterministic) 1TM M we can compute a PCP instance\n(u(1), v(1)), . . . ,(u(n), v(n)) \u2208 \u03a3\u2217 \u00d7 \u03a3\u2217\nso that \u03b5 \u2208 L(M) iff the PCP instance has a solution.\nC The PCP is only semidecidable, but not decidable.\n! An PCP instance can be described as the intersection of two cf. languages:\n\u25b7 Assume that a1, . . . , an, # \u0338\u2208 \u03a3 and add them to \u03a3.\nDefine the two grammars Gu, Gv as follows ( i = 1, 2 . . . , l)\nSu \u2212 \u2192G u(i)Suai | u(i)#ai resp. Sv \u2212 \u2192G v(i)Svai | v(i)#ai\nL(Gu) \u2229 L(Gv) = {u(i1) . . . u(il)#ail . . . ai1 | i1, . . . , il is a solution }\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Post correspondence problem and grammars 85 (233)\nT For every (deterministic) 1TM M we can compute a PCP instance\n(u(1), v(1)), . . . ,(u(n), v(n)) \u2208 \u03a3\u2217 \u00d7 \u03a3\u2217\nso that \u03b5 \u2208 L(M) iff the PCP instance has a solution.\n\u25b7 Recall: a PCP instance has one solution iff it has infinitely many solutions.\nC Already for (linear) context-free grammars G1, G2 it is undecidable if\n\u25b7 L(G1) \u2229 L(G2) = \u2205 resp. |L(G1) \u2229 L(G2)| = 0\n\u25b7 |L(G1) \u2229 L(G2)| < \u221e resp. |L(G1) \u2229 L(G2)| = \u221e\nC For a context-sensitive grammar G it is undecidable if\n\u25b7 L(G) = \u2205 resp. |L(G)| = 0\n\u25b7 |L(G)| < \u221e resp. |L(G)| = \u221e\nas the intersection L(G1) \u2229 L(G2) of grammars of type 2 is still of type 1.\n\u25b7 These problems are still decidable for a single context-free grammar (using\ngraph reachability).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Post correspondence problem and grammars 85 (234)\nT For every (deterministic) 1TM M we can compute a PCP instance\n(u(1), v(1)), . . . ,(u(n), v(n)) \u2208 \u03a3\u2217 \u00d7 \u03a3\u2217\nso that \u03b5 \u2208 L(M) iff the PCP instance has a solution.\nC Let g, h: \u0393\u2217 \u2192 \u03a3\u2217 be two monoid homomorphisms with |\u0393| < \u221e.\n\u25b7 i.e. h(uv) = h(u)h(v) and h(\u03b5) = \u03b5 and analogously for g.\nThen it is undecidable if g(w) = h(w) for some w \u2208 \u0393\u2217.\n! Some remarks:\n\u25b7 If |\u03a3| = 1, then every PCP can be decided.\n\u25b7 Every PCP instance with |\u03a3| > 2 can be \u201cinflated\u201d to a PCP instance with\n|\u03a3| = 2 using unary encoding (e.g. {a1, . . . , al} \u2192 {0, 1}, ai 7\u2192 01i).\n\u25b7 Every PCP instance with only n = 2 word pairs is decidable.\n\u25b7 PCP instances with at least n \u2265 5 word pairs are undecidable in general.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "5 Regular languages\nRegular grammars and finite automata\nMoving between regular grammars and finite automata\nClosure properties\nRegular expressions\nMinimizing DFAs and Myhill-Nerode\nBasics of probabilistic finite automata (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular languages and regular grammars (reminder) 88 (237)\nD A grammar G = (V, \u03a3, P, S) is regular if P \u2286 V \u00d7 ({\u03b5} \u222a\u03a3 \u222a V \u222a \u03a3V ).\n\u25b7 Recall: the Chomsky hierarchy requires P \u2286 V \u00d7 (\u03a3 \u222a \u03a3V ), but \u03b5-rules only\nadd \u03b5 to L(G) and both \u03b5-rules and chain rules can be removed.\nA language L \u2286 \u03a3\u2217 is regular if L = L(G) for some regular grammar G.\n\u25b7 Regular grammars can be conveniently represented as labeled directed graphs\n(formally: nondeterministic finite automata with \u03b5-transitions), e.g.\nV = {S, T} S \u2212 \u2192G aS | T T \u2212 \u2192G bT | S | \u03b5\nas\nS T\na\n\u03b5\nb\n\u03b5\nor S T qf\na\n\u03b5\nb\n\u03b5\n\u03b5\nwhich can be understood as the succinct representation of a 1TM that reads\nthe input completely from left to right till the first blank \u25a1 is read.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular languages and regular grammars (reminder) 88 (238)\nD A grammar G = (V, \u03a3, P, S) is regular if P \u2286 V \u00d7 ({\u03b5} \u222a\u03a3 \u222a V \u222a \u03a3V ).\n\u25b7 Recall: the Chomsky hierarchy requires P \u2286 V \u00d7 (\u03a3 \u222a \u03a3V ), but \u03b5-rules only\nadd \u03b5 to L(G) and both \u03b5-rules and chain rules can be removed.\nA language L \u2286 \u03a3\u2217 is regular if L = L(G) for some regular grammar G.\n\u25b7 Regular grammars can be conveniently represented as labeled directed graphs\n(formally: nondeterministic finite automata with \u03b5-transitions), e.g.\nV = {S, T} S \u2212 \u2192G aS | T T \u2212 \u2192G bT | S | \u03b5\nas\nS T qf\na: a/+ 1\na: a/0; b: b/0\nb: b/+ 1\na: a/0; b: b/0\n\u25a1: \u25a1/0\nwhich can be understood as the succinct representation of a 1TM that reads\nthe input completely from left to right till the first blank \u25a1 is read.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Finite automata 89 (239)\nD Finite automata (FA) (also: finite-state machine (FSM)): (Q, \u03a3, \u03b4, q0, F)\n\u25b7 Q: finite set of states\n\u25b7 \u03a3: the finite input alphabet, e.g. \u03a3 = {0, 1}\n\u25b7 q0: the initial state with q0 \u2208 Q\n\u25b7 F: the final states with F \u2286 Q\n\u25b7 \u03b4: the transition rules with \u03b4 \u2286 (Q \u00d7 (\u03a3 \u222a {\u03b5})) \u00d7 Q.\nA transition rule ((q, a), r) \u2208 \u03b4 with a \u2208 \u03a3 stands for the instruction:\nif state == \u2019q\u2019 and tape[pos] == \u2019a\u2019:\nstate = \u2019r\u2019; pos += 1 # head moves to the right\nAn \u03b5-transition rule ((q, \u03b5), r) \u2208 \u03b4 stands for the instruction:\nif state == \u2019q\u2019:\nstate = \u2019r\u2019 # head does not move\n(A 1TM does not have \u03b5-transitions.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "FA: configuration, run, accepted language 90 (240)\nD Let M = (Q, \u03a3, \u03b4, q0, F) be an FA.\nAn FA is almost a 1TM except that\n\u25b7 it must not write to the tape,\n\u25b7 it must read/consume its input completely from left to right,\n\u25b7 it may continue from a final state.\n(If we introduce an \u201cend of input\u201d marker, then the definitions for 1TMs can be directly\nre-used.)\nA configuration\nq\u03b2 with q \u2208 Q, \u03b2\u2208 \u03a3\u2217\nof an FA consists simply of the current state q and the remaining input \u03b2.\n\u25b7 Because of its restrictions, the future behavior of an FA can only depend on\nthe current state q and the remaining input \u03b2.\n\u25b7 This is sometimes called \u201cmemoryless\u201d (or \u201cmarkovian\u201d).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "FA: configuration, run, accepted language 90 (241)\nD Let M = (Q, \u03a3, \u03b4, q0, F) be an FA.\nThe rules \u03b4 generate the binary relation \u2212 \u2192M on Q \u00d7 \u03a3\u2217:\n\u2212 \u2192M := {(qa\u03b2, r\u03b5\u03b2) | ((q, a\n\u2208\u03a3\u222a{\u03b5}\n), r) \u2208 \u03b4, \u03b2\u2208 \u03a3\u2217}\nA run of the FA is a path q0\u03b20 \u2212 \u2192M q1\u03b21 \u2212 \u2192M . . .wrt. \u2212 \u2192M .\n\u25b7 A run on input x1 . . . xl \u2208 \u03a3l is a path wrt. \u2212 \u2192M starting at q0x1 . . . xl\n! Infinite runs can only arise from \u03b5-transitions in case of FAs.\n\u25b7 \u03b4 is deterministic iff every configuration has at most one successor wrt. \u2212 \u2192M\niff for every configuration there is exactly one maximal run.\nThe language accepted/recognized by M is\nL(M) = {w \u2208 \u03a3\u2217 | q0w \u2212 \u2192\u2217\nM qf \u03b5 for some qf \u2208 F}\ni.e. as before it suffices it there is at least one run that is accepting, but now\nthe input has to be read completely from left to right; note that the run is\nnot required to be maximal which only mattes in case of \u03b5-transitions.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "FA: configuration, run, accepted language 90 (242)\nD Let M = (Q, \u03a3, \u03b4, q0, F) be an FA.\nThe rules \u03b4 generate the binary relation \u2212 \u2192M on Q \u00d7 \u03a3\u2217:\n\u2212 \u2192M := {(qa\u03b2, r\u03b5\u03b2) | ((q, a\n\u2208\u03a3\u222a{\u03b5}\n), r) \u2208 \u03b4, \u03b2\u2208 \u03a3\u2217}\nA run of the FA is a path q0\u03b20 \u2212 \u2192M q1\u03b21 \u2212 \u2192M . . .wrt. \u2212 \u2192M .\nAs \u03b5-transitions can also lead to nondeterminism, the following abbreviations\nfor restricted sublasses of FAs are used:\n\u25b7 \u03b5-NFA: an FA with \u03b5-transitions, i.e. a general FA.\n\u25b7 NFA: an FA without \u03b5-transitions.\n\u25b7 DFA: an FA without \u03b5-transitions and |(q, a)\u03b4| \u22641 for all (q, a) \u2208 Q \u00d7 \u03a3,\ni.e. for every state q there is at most one r with qa \u2212 \u2192M r (a-successor).\n\u25b7 complete DFA: a DFA with |(q, a)\u03b4| = 1 for all (q, a) \u2208 Q \u00d7 \u03a3,\ni.e. for every state q there is exactly one r with qa \u2212 \u2192M r.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: \u201cends with bc\u201d 91 (243)\n\u03a3 = {a, b} S \u2212 \u2192G aS | bS | cS | bT T \u2212 \u2192G cU U \u2212 \u2192G \u03b5\nSstart T U\na, b, c\nb c\nS\n. . .\u25a1 a b b c \u25a1 . . .\nSabbc \u2212 \u2192M Sbbc \u2212 \u2192M T bc (wrong guess)\n\u25b7 FA \u201c=\u201d no-write-no-going-back-1TM\n\u25b7 Behavior depends only on state and remaining input\n\u25b7 Nondeterminism can \u201cguess\u201d when to read the last two letters, but might\nguess wrong.\n\u25b7 Regular grammar writes letters (S \u2212 \u2192G aS), FA reads letters (Sa \u2212 \u2192M S)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: \u201cends with bc\u201d 91 (244)\n\u03a3 = {a, b} S \u2212 \u2192G aS | bS | cS | bT T \u2212 \u2192G cU U \u2212 \u2192G \u03b5\nSstart T U\na, b, c\nb c\nS\n. . .\u25a1 a b b c \u25a1 . . .\nSabbc \u2212 \u2192M Sbbc \u2212 \u2192M T bc (wrong guess)\n\u25b7 FA \u201c=\u201d no-write-no-going-back-1TM\n\u25b7 Behavior depends only on state and remaining input\n\u25b7 Nondeterminism can \u201cguess\u201d when to read the last two letters, but might\nguess wrong.\n\u25b7 Regular grammar writes letters (S \u2212 \u2192G aS), FA reads letters (Sa \u2212 \u2192M S)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: \u201cends with bc\u201d 91 (245)\n\u03a3 = {a, b} S \u2212 \u2192G aS | bS | cS | bT T \u2212 \u2192G cU U \u2212 \u2192G \u03b5\nSstart T U\na, b, c\nb c\nT\n. . .\u25a1 a b b c \u25a1 . . .\nSabbc \u2212 \u2192M Sbbc \u2212 \u2192M T bc (wrong guess)\n\u25b7 FA \u201c=\u201d no-write-no-going-back-1TM\n\u25b7 Behavior depends only on state and remaining input\n\u25b7 Nondeterminism can \u201cguess\u201d when to read the last two letters, but might\nguess wrong.\n\u25b7 Regular grammar writes letters (S \u2212 \u2192G aS), FA reads letters (Sa \u2212 \u2192M S)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: \u201cends with bc\u201d 91 (246)\n\u03a3 = {a, b} S \u2212 \u2192G aS | bS | cS | bT T \u2212 \u2192G cU U \u2212 \u2192G \u03b5\nSstart T U\na, b, c\nb c\nS\n. . .\u25a1 a b b c \u25a1 . . .\nSabbc \u2212 \u2192M Sbbc \u2212 \u2192M Sbc \u2212 \u2192M T c\u2212 \u2192M U (correct guess)\n\u25b7 FA \u201c=\u201d no-write-no-going-back-1TM\n\u25b7 Behavior depends only on state and remaining input\n\u25b7 Nondeterminism can \u201cguess\u201d when to read the last two letters, but might\nguess wrong.\n\u25b7 Regular grammar writes letters (S \u2212 \u2192G aS), FA reads letters (Sa \u2212 \u2192M S)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: \u201cends with bc\u201d 91 (247)\n\u03a3 = {a, b} S \u2212 \u2192G aS | bS | cS | bT T \u2212 \u2192G cU U \u2212 \u2192G \u03b5\nSstart T U\na, b, c\nb c\nS\n. . .\u25a1 a b b c \u25a1 . . .\nSabbc \u2212 \u2192M Sbbc \u2212 \u2192M Sbc \u2212 \u2192M T c\u2212 \u2192M U (correct guess)\n\u25b7 FA \u201c=\u201d no-write-no-going-back-1TM\n\u25b7 Behavior depends only on state and remaining input\n\u25b7 Nondeterminism can \u201cguess\u201d when to read the last two letters, but might\nguess wrong.\n\u25b7 Regular grammar writes letters (S \u2212 \u2192G aS), FA reads letters (Sa \u2212 \u2192M S)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: \u201cends with bc\u201d 91 (248)\n\u03a3 = {a, b} S \u2212 \u2192G aS | bS | cS | bT T \u2212 \u2192G cU U \u2212 \u2192G \u03b5\nSstart T U\na, b, c\nb c\nS\n. . .\u25a1 a b b c \u25a1 . . .\nSabbc \u2212 \u2192M Sbbc \u2212 \u2192M Sbc \u2212 \u2192M T c\u2212 \u2192M U (correct guess)\n\u25b7 FA \u201c=\u201d no-write-no-going-back-1TM\n\u25b7 Behavior depends only on state and remaining input\n\u25b7 Nondeterminism can \u201cguess\u201d when to read the last two letters, but might\nguess wrong.\n\u25b7 Regular grammar writes letters (S \u2212 \u2192G aS), FA reads letters (Sa \u2212 \u2192M S)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: \u201cends with bc\u201d 91 (249)\n\u03a3 = {a, b} S \u2212 \u2192G aS | bS | cS | bT T \u2212 \u2192G cU U \u2212 \u2192G \u03b5\nSstart T U\na, b, c\nb c\nT\n. . .\u25a1 a b b c \u25a1 . . .\nSabbc \u2212 \u2192M Sbbc \u2212 \u2192M Sbc \u2212 \u2192M T c\u2212 \u2192M U (correct guess)\n\u25b7 FA \u201c=\u201d no-write-no-going-back-1TM\n\u25b7 Behavior depends only on state and remaining input\n\u25b7 Nondeterminism can \u201cguess\u201d when to read the last two letters, but might\nguess wrong.\n\u25b7 Regular grammar writes letters (S \u2212 \u2192G aS), FA reads letters (Sa \u2212 \u2192M S)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: \u201cends with bc\u201d 91 (250)\n\u03a3 = {a, b} S \u2212 \u2192G aS | bS | cS | bT T \u2212 \u2192G cU U \u2212 \u2192G \u03b5\nSstart T U\na, b, c\nb c\nU\n. . .\u25a1 a b b c \u25a1 . . .\nSabbc \u2212 \u2192M Sbbc \u2212 \u2192M Sbc \u2212 \u2192M T c\u2212 \u2192M U (correct guess)\n\u25b7 FA \u201c=\u201d no-write-no-going-back-1TM\n\u25b7 Behavior depends only on state and remaining input\n\u25b7 Nondeterminism can \u201cguess\u201d when to read the last two letters, but might\nguess wrong.\n\u25b7 Regular grammar writes letters (S \u2212 \u2192G aS), FA reads letters (Sa \u2212 \u2192M S)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: \u201cends with bc\u201d (DFA) 92 (251)\nSstart T U\na, b, c\nb c\npstart q r\na, c\nb c\na, c\nba\nb\ndef p(w):\nx = w[0]\nif x in [\u2019a\u2019,\u2019c\u2019]:\nreturn p(w[1:])\nif x == \u2019b\u2019:\nreturn q(w[1:])\nreturn False\ndef q(w):\nx = w[0]\nif x == \u2019a\u2019:\nreturn p(w[1:])\nif x == \u2019b\u2019:\nreturn q(w[1:])\nif x == \u2019c\u2019:\nreturn r(w[1:])\nreturn False\ndef r(w):\nif w == \u2019\u2019:\nreturn True\nx = w[0]\nif x == \u2019b\u2019:\nreturn q(w[1:])\nif x in [\u2019a\u2019,\u2019c\u2019]:\nreturn p(w[1:])\nreturn False\n\u25b7 Nondeterminism can lead to much simpler/smaller FAs, whereas DFAs can be\neasily translated e.g. into tail-recursive programs.\n\u25b7 The DFA is essentially the call graph; recall that tail-recursion is directly\nequivalent to a while-loop.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "FA vs. 1TM 93 (252)\n\u25b7 As in the case of 1TMs any kind of finite information can be encoded in the\nstates, e.g. also a \u201cwork tape\u201d of fixed length L, roughly:\n\u25b7 Every possible content of this work tape is described by \u0393L.\nAs this is a finite set, we can use it as states for an FA.\n(We still need an unbounded \u201cinput tape\u201d which is read-only, and has to be read\nfrom left to right.)\n\u25b7 For instance, controllers/drivers/RNNs can be described by FAs.\n\u25b7 Output can e.g. be encoded in the states or modeled by means of interleaving\ninput and output ( i1o1i2o2i3o3 . . . ilol).\n\u25b7 FAs with output are called finite-state transducers. Special cases: Mealy\nmachines, Moore machines, recurrent NNs (finite \u03a3 & Q due to quantization) .\nIn practice, a central question is the synthesis problem, e.g.:\n\u25b7 Given: some definition of a regular language L describing/specifying the\nbehavior of the system.\n\u25b7 Goal: compute a minimal DFA M with L(M) = L, i.e. M is an\nimplementation/realization of the specification.",
      "In practice, a central question is the synthesis problem, e.g.:\n\u25b7 Given: some definition of a regular language L describing/specifying the\nbehavior of the system.\n\u25b7 Goal: compute a minimal DFA M with L(M) = L, i.e. M is an\nimplementation/realization of the specification.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Turing on finite memory 94 (253)\n[...] Consider first the more accurate form of the question. I believe that\nin about fifty years\u2019 time it will be possible to programme computers,\nwith a storage capacity of about 109, to make them play the imitation\ngame so well that an average interrogator will not have more than 70\nper cent, chance of making the right identification after five minutes of\nquestioning. The original question, \u2018Can machines think!\u2019 I believe to be\ntoo meaningless to deserve discussion. Nevertheless I believe that at the\nend of the century the use of words and general educated opinion will\nhave altered so much that one will be able to speak of machines thinking\nwithout expecting to be contradicted. I believe further that no useful\npurpose is served by concealing these beliefs. [...] (A. Turing, \u201cCOMPUTING\nMACHINERY AND INTELLIGENCE\u201d, Mind, 1950)\n(Cf. ChatGPT based on GPT-3 which requires 800GB of storage.)",
      "without expecting to be contradicted. I believe further that no useful\npurpose is served by concealing these beliefs. [...] (A. Turing, \u201cCOMPUTING\nMACHINERY AND INTELLIGENCE\u201d, Mind, 1950)\n(Cf. ChatGPT based on GPT-3 which requires 800GB of storage.)\n(Related interactive polynomial time: \u201cPSPACE is almighty for PTIME\u201d)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "FAs and rational numbers 95 (254)\ndef binrep(x: float):\nassert 0 <= x < 1\nyield \u2019.\u2019\nwhile True:\nx = 2 * x\nif x >= 1:\nx -= 1\nyield \u20191\u2019\nelse:\nyield \u20190\u2019\nJ.1001K2 = P\u221e\ni=0(2\u22121+i\u00b74 + 2\u22124+i\u00b74)\n= (2 \u22121 + 2\u22124) \u00b7 P\u221e\ni=0(2\u22124)i\n= (2 \u22121 + 2\u22124) 1\n1\u22122\u22124\n= 23+20\n24\u22121 = 9\n15 = 3\n5 = J.6K10\n0.6start 1.2 0.4 0.8 1.6. 1 0 0\n1\n\u25b7 For a fixed rational number x \u2208 [0, 1) the program becomes an FA that\naccepts the prefixes of the binary representation of x. (Similarly, for other bases.)\n\u25b7 Regular languages are closely related to rational numbers Q .\n! As for TMs: we can always add a sink/trap/rejecting state to complete a\nDFA.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "NDDs: FAs for compression of infinite sets 96 (255)\nJx0x1 . . . xlK2 \u2212 Jy0y1 . . . ylK2 \u2264 c\niff (x0 + 2Jx1 . . . xlK2) \u2212 (y0 + 2Jy1 . . . ylK2) \u2264 c\niff Jx1 . . . xlK2 \u2212 Jy1 . . . ylK2 \u2264 \u230ac\u2212x0+y0\n2 \u230b\n1start 0 \u2212100,10,11\n01\n10\n00,01,11\n01\n00,10,11\n\u25b7 FAs can represent solutions of linear inequalities like ax + by \u2264 c over Z for\nfixed coefficients a, b, c\u2208 Z using LSBF or MSBF encoding and bit\ntuples/vectors as alphabet.\n\u25b7 States remember the \u201cslack\u201d.\n\u25b7 I.e. FAs can be used as finite representation of certain infinite sets of integers.\n\u25b7 Using the closure properties of regular languages, this is the basis for a\ndecision procedure for Presburger arithmetic.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "BBDs: FAs for compression of finite sets 97 (256)\nw \u2227 (\u00acx \u2228 (\u00acy \u2227 z)) {10}{0, 1}2 \u222a {11}{01}\nstart 1\n0\n0,1\n0,1\n1\n0\n1\n\u25b7 FAs are also used to compress e.g. finite sets of bit vectors (cf. BDD).\n\u25b7 E.g. the exponential set {0, 1}n can be represented by a DFA with n states.\n\u25b7 The closure properties of regular languages allow to efficiently work with the\ncompressed representation which is e.g. used in the verification of software\nand hardware.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: Suffix trees 98 (257)\nstart\n5 3 1\n4 2\n0\na n a n a\n$ $ $\nn\na n a\n$ $\nb\na n a n a\n$\n\u25b7 Suffix trees are finite state transducers that recognize the suffixes of a given\nword w and encode in their final states the position at which a suffix starts.\n\u25b7 \u201c$\u201d is used as \u201cend of string\u201d marker again.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: Markov language models 99 (258)\npstart q r\na: 0.2, c: 0.3\nb: 0.5 c: 0.1\na: 0.6\nb: 0.3\na: 0.8,c: 0.1\nb: 0.1\n\u25b7 DFAs can be used to assign probabilities to words resp. to randomly generate\nwords from the accepted language:\n\u25b7 The \u201crandom surfer\u201d starts in the initial state q0 and generates a \u201crandom\nwalk\u201d q0q1q2 . . .by choosing in state qi a letter a \u2208 \u03a3 with the defined\nconditional probability.\n\u25b7 An explicit \u201cend of word\u201d/\u201cterminator\u201d symbol like $ can be used to explicitly\nterminate the generating process.\n\u25b7 This allows e.g. to encode that a blank \u25a1 is more likely to follow after \u201cthe\u201d\nthen e.g. the letter l.\n\u25b7 The stationary distribution (or more generally, the Cesaro limit) yield the\naverage distribution of letters \u03a3.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Examples: Bots (Controllers, Drivers, . . . ) 100 (259)\n\u25b7 Animation and behavior of \u201cnon player characters\u201d and \u201cbots\u201d is often\ndescribed using finite automatas (=finite state machines).\n\u25b7 E.g. in Unity3d and in Unreal (image source).\n\u25b7 In general, FAs are used to describe any kind of systems with only finitely\nmany states like vending machines, clocks, controllers/robots, protocols, . . . .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "5 Regular languages\nRegular grammars and finite automata\nMoving between regular grammars and finite automata\nClosure properties\nRegular expressions\nMinimizing DFAs and Myhill-Nerode\nBasics of probabilistic finite automata (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Of regular grammars and \u03b5-NFA 102 (261)\nF From every regular grammar G (without \u03b5-rules and chain rules) we can\ncompute an \u03b5-NFA (resp. NFA) M with L(G) = L(M), and vice versa.\n\u25b7 Informally:\nA regular grammar describes how to construct a \u201cregularly structured\u201d word,\nwhereas the FA describes how to check if a word is \u201cregularly structured\u201d.\nWe thus simply need to turn \u201cread a\u201d into \u201cproduce a\u201d and v.v.:\n\u25b7 A production rule X \u2212 \u2192G aY becomes the transition rule Xa \u2212 \u2192M Y\nA production rule X \u2212 \u2192G a becomes the transition rule Xa \u2212 \u2192M qf\nwith qf the only final state of M.\n\u25b7 A transition rule pa \u2212 \u2192M q becomes the production rule p \u2212 \u2192G aq;\nif q \u2208 F is final, then also add the production rule p \u2212 \u2192G a.\n\u25b7 I.e. states and nonterminals essentially coincide (except for the introduction of\nqf ).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Of regular grammars and \u03b5-NFA 102 (262)\nF From every regular grammar G (without \u03b5-rules and chain rules) we can\ncompute an \u03b5-NFA (resp. NFA) M with L(G) = L(M), and vice versa.\n\u25b7 Let G = (V, \u03a3, P, S) be a regular grammar (with \u03b5-rules).\nThe \u03b5-NFA is M = (V \u222a {qf }, \u03a3, \u03b4, S, F) with\nF := {qf } \u222a {X | (X, \u03b5) \u2208 P} (X \u2212 \u2192G \u03b5 and stop)\n\u03b4 := {((X, a), Y) | (X, aY) \u2208 P} (X \u2212 \u2192G aY )\n\u222a { ((X, a), qf ) | (X, a) \u2208 P} (X \u2212 \u2192G a and stop)\n\u222a { ((X, \u03b5), Y) | (X, Y) \u2208 P, a\u2208 \u03a3} (X \u2212 \u2192G Y )\nwhere we assume that qf \u0338\u2208 V .\nBy construction: if G has no chain rules, then M has no \u03b5-transitions,\n\u25b7 At the cost of additional \u03b5-transitions, a single final state suffices:\nF := {qf }\n\u03b4 := {((X, a), Y) | (X, aY) \u2208 P} (X \u2212 \u2192G aY )\n\u222a { ((X, a), qf ) | (X, a) \u2208 P} (X \u2212 \u2192G a and stop)\n\u222a { ((X, \u03b5), Y) | (X, Y) \u2208 P, a\u2208 \u03a3} (X \u2212 \u2192G Y )\n\u222a { ((X, \u03b5), qf ) | (X, \u03b5) \u2208 P} (X \u2212 \u2192G \u03b5 and stop)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Of regular grammars and \u03b5-NFA 102 (263)\nF From every regular grammar G (without \u03b5-rules and chain rules) we can\ncompute an \u03b5-NFA (resp. NFA) M with L(G) = L(M), and vice versa.\n\u25b7 Let M = (Q, \u03a3, \u03b4, q0, F) be an \u03b5-NFA (with \u03b5-rules).\nThe regular grammar is G = (Q, \u03a3, P, q0) with\nP := {(q, ar) | ((q, a), r) \u2208 \u03b4} (qa \u2212 \u2192M r)\n\u222a { (q, a) | ((q, a), qf ) \u2208 \u03b4, qf \u2208 F} (qa \u2212 \u2192M qf with qf \u2208 F)\n\u222a { (q, r) | ((q, \u03b5), r) \u2208 \u03b4} (q \u2212 \u2192M r)\n\u222a { (q, \u03b5) | ((q, \u03b5), qf ) \u2208 \u03b4, qf \u2208 F} (q \u2212 \u2192M qf with qf \u2208 F)\n\u222a { (q, \u03b5) | ((q, \u03b5), qf ) \u2208 \u03b4, qf \u2208 F} (q \u2212 \u2192M qf with qf \u2208 F)\n\u222a { (q0, \u03b5) | q0 \u2208 F} (only if q0 \u2208 F)\nBy construction: if M has no \u03b5-transitions, then q0 \u2212 \u2192G \u03b5 is the only potential\n\u03b5-rule in G.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Of regular grammars and \u03b5-NFA 102 (264)\nF From every regular grammar G (without \u03b5-rules and chain rules) we can\ncompute an \u03b5-NFA (resp. NFA) M with L(G) = L(M), and vice versa.\nC For every \u03b5-NFA we can construct an NFA which accepts the same language.\n\u25b7 E.g. (i) represent the \u03b5-NFA M as a regular grammar G, (ii) remove chain\nrules in G to obtain G\u2032, (iii) represent G\u2032 as NFA M\u2032.\n\u25b7 Exercise: combine these steps into a single algorithm.\n(The main advantage of \u03b5-NFA is that they are less restricted and thus usually more\nconvenient to \u201cdraw\u201d.)\n\u25b7 Example: from regular grammar with chain rules to \u03b5-NFA.\nG: S \u2192 aS | T | \u03b5 T \u2212 \u2192G aS | bT | b\nSstart T\nqf\na b\n\u03b5\na\nb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Of regular grammars and \u03b5-NFA 102 (265)\nF From every regular grammar G (without \u03b5-rules and chain rules) we can\ncompute an \u03b5-NFA (resp. NFA) M with L(G) = L(M), and vice versa.\nC For every \u03b5-NFA we can construct an NFA which accepts the same language.\n\u25b7 E.g. (i) represent the \u03b5-NFA M as a regular grammar G, (ii) remove chain\nrules in G to obtain G\u2032, (iii) represent G\u2032 as NFA M\u2032.\n\u25b7 Exercise: combine these steps into a single algorithm.\n(The main advantage of \u03b5-NFA is that they are less restricted and thus usually more\nconvenient to \u201cdraw\u201d.)\n\u25b7 Example: Removing chain rules in\nG: S \u2192 aS | T | \u03b5 T \u2212 \u2192G aS | bT | b\nyields (replace S \u2212 \u2192G T by S \u2212 \u2192G aS | bT | b)\nG\u2032 : S \u2192 aS | bT | b | \u03b5 T \u2212 \u2192G aS | bT | b\n(Removing \u03b5-rules in G is the same as contracting \u03b5-transitions in the FA (=call graph)) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Of regular grammars and \u03b5-NFA 102 (266)\nF From every regular grammar G (without \u03b5-rules and chain rules) we can\ncompute an \u03b5-NFA (resp. NFA) M with L(G) = L(M), and vice versa.\nC For every \u03b5-NFA we can construct an NFA which accepts the same language.\n\u25b7 E.g. (i) represent the \u03b5-NFA M as a regular grammar G, (ii) remove chain\nrules in G to obtain G\u2032, (iii) represent G\u2032 as NFA M\u2032.\n\u25b7 Exercise: combine these steps into a single algorithm.\n(The main advantage of \u03b5-NFA is that they are less restricted and thus usually more\nconvenient to \u201cdraw\u201d.)\n\u25b7 Example: Without chain rules, we obtain an NFA from\nG\u2032 : S \u2192 aS | bT | b | \u03b5 T \u2212 \u2192G aS | bT | b\nSstart T\nqf\na b\n\u03b5\na\nb\nbecomes Sstart T\nqf\na b\nb\na\nb b\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From NFA to DFA 103 (267)\nT From an NFA M = (Q, \u03a3, \u03b4, q0, F) we can compute a DFA\nM\u2032 = (Q\u2032, \u03a3, \u03b4\u2032, q\u2032\n0, F\u2032) with L(M) = L(M\u2032).\n\u25b7 As M is an NFA, it reads its input strictly one letter at a time from left to\nright, and every run on w = a1 . . . al has the form\nq0a1a2 . . . al \u2212 \u2192M q1a2 . . . al \u2212 \u2192M . . .\nand it is accepting iff ql \u2208 F. (A run might also deadlock.)\nWe can thus superpose all possible runs by tracking the states we have\nreached after reading a1 . . . ai e.g. using sets of states or, more efficiently,\nusing bit vectors.\nq0start q1 q2\na, b, c\nb c\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From NFA to DFA 103 (268)\nT From an NFA M = (Q, \u03a3, \u03b4, q0, F) we can compute a DFA\nM\u2032 = (Q\u2032, \u03a3, \u03b4\u2032, q\u2032\n0, F\u2032) with L(M) = L(M\u2032).\n\u25b7 As M is an NFA, it reads its input strictly one letter at a time from left to\nright, and every run on w = a1 . . . al has the form\nq0a1a2 . . . al \u2212 \u2192M q1a2 . . . al \u2212 \u2192M . . .\nand it is accepting iff ql \u2208 F. (A run might also deadlock.)\nWe can thus superpose all possible runs by tracking the states we have\nreached after reading a1 . . . ai e.g. using sets of states or, more efficiently,\nusing bit vectors.\nq0start q1 q2\na, b, c\nb c\nq0babc \u2212 \u2192M q0abc \u2212 \u2192M q0bc \u2212 \u2192M q0c \u2212 \u2192M q0\nq0babc \u2212 \u2192M q1abc\nq0babc \u2212 \u2192M q0abc \u2212 \u2192M q0bc \u2212 \u2192M q1c\nq0babc \u2212 \u2192M q0abc \u2212 \u2192M q0bc \u2212 \u2192M q1c \u2212 \u2192M q2\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From NFA to DFA 103 (269)\nT From an NFA M = (Q, \u03a3, \u03b4, q0, F) we can compute a DFA\nM\u2032 = (Q\u2032, \u03a3, \u03b4\u2032, q\u2032\n0, F\u2032) with L(M) = L(M\u2032).\n\u25b7 As M is an NFA, it reads its input strictly one letter at a time from left to\nright, and every run on w = a1 . . . al has the form\nq0a1a2 . . . al \u2212 \u2192M q1a2 . . . al \u2212 \u2192M . . .\nand it is accepting iff ql \u2208 F. (A run might also deadlock.)\nWe can thus superpose all possible runs by tracking the states we have\nreached after reading a1 . . . ai e.g. using sets of states or, more efficiently,\nusing bit vectors.\nq0start q1 q2\na, b, c\nb c\n\"1\n0\n0\n#\nbabc \u2212 \u2192M\n\"1\n1\n0\n#\nabc \u2212 \u2192M\n\"1\n0\n0\n#\nbc \u2212 \u2192M\n\"1\n1\n0\n#\nc \u2212 \u2192M\n\"1\n0\n1\n#\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From NFA to DFA 103 (270)\nT From an NFA M = (Q, \u03a3, \u03b4, q0, F) we can compute a DFA\nM\u2032 = (Q\u2032, \u03a3, \u03b4\u2032, q\u2032\n0, F\u2032) with L(M) = L(M\u2032).\n\u25b7 As M is an NFA, it reads its input strictly one letter at a time from left to\nright, and every run on w = a1 . . . al has the form\nq0a1a2 . . . al \u2212 \u2192M q1a2 . . . al \u2212 \u2192M . . .\nand it is accepting iff ql \u2208 F. (A run might also deadlock.)\nWe can thus superpose all possible runs by tracking the states we have\nreached after reading a1 . . . ai e.g. using sets of states or, more efficiently,\nusing bit vectors.\nq0start q1 q2\na, b, c\nb c\nq100start q110 q101\na, c\nb c\na, c\nba\nb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From NFA to DFA 103 (271)\nT From an NFA M = (Q, \u03a3, \u03b4, q0, F) we can compute a DFA\nM\u2032 = (Q\u2032, \u03a3, \u03b4\u2032, q\u2032\n0, F\u2032) with L(M) = L(M\u2032).\n\u25b7 As M is an NFA, it reads its input strictly one letter at a time from left to\nright, and every run on w = a1 . . . al has the form\nq0a1a2 . . . al \u2212 \u2192M q1a2 . . . al \u2212 \u2192M . . .\nand it is accepting iff ql \u2208 F. (A run might also deadlock.)\nWe can thus superpose all possible runs by tracking the states we have\nreached after reading a1 . . . ai e.g. using sets of states or, more efficiently,\nusing bit vectors.\nThis leads to the \u201cbrute-force\u201d definition called power-set construction:\nQ\u2032 := 2 Q = {C \u2286 Q} \u223c= {0, 1}|Q|\n\u03b4\u2032 := {((C, a), {r | ((q, a), r) \u2208 \u03b4, q\u2208 C, C\u2208 2Q})}\nq\u2032\n0 := {q0}\nF\u2032 := {C \u2286 Q | C \u2229 F \u0338= \u2205}\n(Do not take this literally, use BFS or DFS instead!)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From NFA to DFA 104 (272)\ndef bfs_power_set_construction(Q,\u03a3,\u03b4,q0, F):\nC = {q0} # current states we might be in\ntodo = {C}\nQ\u2032 = todo # copy\n\u03b4\u2032 = \u2205\nwhile todo \u0338= \u2205:\nnext_todo = \u2205\nfor C \u2208 todo:\nfor a \u2208 \u03a3:\nN = \u2205 # superposition of all possible successors wrt. a\nfor q \u2208 C:\nfor r \u2208 Q:\nif ((q, a), r) \u2208 \u03b4:\nN = N \u222a {r}.\n\u03b4\u2032 = \u03b4\u2032 \u222a {((C, a), N)}\nif N \u0338\u2208 Q\u2032:\nQ\u2032 = Q\u2032 \u222a {N}\nnext_todo = next_todo \u222a {C}\ntodo = next_todo\nreturn (Q\u2032, \u03a3, \u03b4\u2032, {q0}, {C \u2208 Q\u2032 | C \u2229 F \u0338= \u2205})\n! This extends to NFA with a set of I \u2286 Q of initial states: simply start the\nBFS from I instead of {q0}.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "5 Regular languages\nRegular grammars and finite automata\nMoving between regular grammars and finite automata\nClosure properties\nRegular expressions\nMinimizing DFAs and Myhill-Nerode\nBasics of probabilistic finite automata (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Closure properties 106 (274)\nC Regular languages are closed wrt. complement, union, intersection, difference,\nconcatenation and Kleene star:\n\u25b7 Complement: Let M = (Q, \u03a3, \u03b4, q0, F) be a DFA with L = L(M).\nAs \u03b4 might not be defined for all a \u2208 \u03a3, add a sink/trap state:\nQ := Q \u222a {q\u22a5} \u03b4 := \u03b4 \u222a {((q, a), q\u22a5): |(q, a)\u03b4| = 0, q\u2208 Q, a\u2208 \u03a3}\nIn case of a deadend, we now move to q\u22a5 and loop there forever.\nNow, define M\u2032 by simply making final states nonfinal and v.v.:\nM\u2032 = (Q, \u03a3, \u03b4, q0, Q\\ F)\nAs M is deterministic, so is M\u2032, and thus there is for every word exactly one\nrun. By construction: a run in M is accepting iff it is rejecting in M\u2032.\nstart a start \u22a5a a\na\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Closure properties 106 (275)\nC Regular languages are closed wrt. complement, union, intersection, difference,\nconcatenation and Kleene star:\n\u25b7 Union: let Mi = (Qi, \u03a3, \u03b4i, q0,i, Fi) be FA with Li = L(Mi) for i = 1, 2.\n(Assume Q1 \u2229 Q2 = \u2205.)\nAdd a new initial state that can nondeterministically choose to run M1 or M2:\n(Q1 \u222a Q2 \u222a {q0}, \u03a3, \u03b41 \u222a \u03b42 \u222a {((q0, \u03b5), q0,1), ((q0, \u03b5), q0,2)}, q0, F1 \u222a F2)\n(The same approach as for grammars.)\nstart a\nstart b\na, b a, b\na, b a, b start\na\nb\n\u03b5\n\u03b5\na, b a, b\na, b a, b\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Closure properties 106 (276)\nC Regular languages are closed wrt. complement, union, intersection, difference,\nconcatenation and Kleene star:\n\u25b7 All other Boolean operations can be reduced to union and complement, e.g.:\nL1 \u2229 L2 = L1 \u222a L2 L1 \\ L2 = L1 \u2229 (\u03a3\u2217 \\ L2)\nThe complement requires to move to a DFA in general.\nIf M1, M2 are both DFAs, then their product can be used:\n(Q1 \u00d7 Q2, \u03a3, {((q1, q2, a), (r1, r2)) | ((qi, a), ri) \u2208 \u03b4i, i= 1, 2}, (q0,1, q0,2), F\u00d7)\ni.e. we let the two DFAs run in parallel/\u201clockstep\u201d. The definition of F\u00d7 then\ndepends on the Boolean operation (see the tutorials) .\npstart qa\nsstart tb\nb a, b\na a, b psstart\nqs\npt\nqt\na\nb\na\nb\nb\na\na, b\n(Again, use BFS/DFS to only construct the required states.)No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Closure properties 106 (277)\nC Regular languages are closed wrt. complement, union, intersection, difference,\nconcatenation and Kleene star:\n\u25b7 Concatenation: Let Mi = (Qi, \u03a3, \u03b4i, q0,i, Fi) be FA with Li = L(Mi) for\ni = 1, 2. (Assume Q1 \u2229 Q2 = \u2205.)\nSimply connect the NFAs in series using \u03b5-transitions:\n(Q1 \u222a Q2, \u03a3, \u03b41 \u222a \u03b42 \u222a {((qf,1, \u03b5), q0,2) | qf,1 \u2208 F1}, q0,1, F2)\nstart a\nstart b\na, b a, b\na, b a, b\nstart a\nb\n\u03b5\na, b a, b\na, b a, b\n(For comparison, try to prove this using regular grammars, and try to understand why\nthe final state is helpful here.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Closure properties 106 (278)\nC Regular languages are closed wrt. complement, union, intersection, difference,\nconcatenation and Kleene star:\n\u25b7 Kleene star: Let M = (Q, \u03a3, \u03b4, q0, F) be an FA with L = L(M).\nAdd a \u201cfeedback loop\u201d to M1, and a new initial state so that \u03b5 can be\naccepted, too:\n(Q \u222a {q0,\u2217}, \u03a3, \u03b4\u222a {((qf , \u03b5), q0), ((q0,\u2217, \u03b5), q0) | qf \u2208 F}, q0,\u2217, F\u222a {q0,\u2217})\nstart a\na, b a, b\nstart a\n\u03b5\n\u03b5\na, b a, b\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "5 Regular languages\nRegular grammars and finite automata\nMoving between regular grammars and finite automata\nClosure properties\nRegular expressions\nMinimizing DFAs and Myhill-Nerode\nBasics of probabilistic finite automata (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions 108 (280)\nD Regular expression (RegEx): For a given alphabet \u03a3, a regular expression is a\nword produced by the following context-free grammar:\nS \u2212 \u2192G 0 | 1 | a (a \u2208 \u03a3) S \u2212 \u2192G (SS) S \u2212 \u2192G (S + S) S \u2212 \u2192G S\u2217\nThe language L(\u03c1) described by a regular expression \u03c1 is defined inductively:\nL(1) := {\u03b5} L((\u03c1 + \u03c1\u2032)) := L(\u03c1) \u222a L(\u03c1\u2032)\nL(a) := {a} L((\u03c1\u03c1\u2032)) := L(\u03c1)L(\u03c1\u2032)\nL(0) := \u2205 L(\u03c1\u2217) := L(\u03c1)\u2217\nThe usual convention to use the operator precedence\nstar/repetition before product/concatenation before addition/choice\nin order to reduce the number of parentheses, e.g.:\nL(ab\u2217c) = {a}{b}\u2217{c} L(a\u2217bc) = {a}\u2217{bc} L((ab)\u2217c) = {ab}\u2217{c}\nL((a + b)\u2217c) = {a, b}\u2217{c} L(a + b\u2217c) = {a} \u222a {b}\u2217{c}\n(L(\u03c1) is the interpretation/meaning of the expression \u03c1 as a language; but we can also give\n\u03c1 the meaning/semantics e.g. of an arithmetic expression with A(\u03c1\u2217) := (1 \u2212 A(\u03c1))\u22121.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions 108 (281)\nD Regular expression (RegEx): For a given alphabet \u03a3, a regular expression is a\nword produced by the following context-free grammar:\nS \u2212 \u2192G 0 | 1 | a (a \u2208 \u03a3) S \u2212 \u2192G (SS) S \u2212 \u2192G (S + S) S \u2212 \u2192G S\u2217\nThe language L(\u03c1) described by a regular expression \u03c1 is defined inductively:\nL(1) := {\u03b5} L((\u03c1 + \u03c1\u2032)) := L(\u03c1) \u222a L(\u03c1\u2032)\nL(a) := {a} L((\u03c1\u03c1\u2032)) := L(\u03c1)L(\u03c1\u2032)\nL(0) := \u2205 L(\u03c1\u2217) := L(\u03c1)\u2217\n! Often also \u201c |\u201d for \u201c +\u201d (choice) and \u201c \u2205\u201d for \u201c 0\u201d and \u201c \u03b5\u201d for \u201c 1\u201d.\nNotation here is standard for Kleene algebras and semirings.\n(Most programming languages and tools like grep use deviating syntax anyways.)\n\u25b7 \u03c1n is defined as usual as shorthand for \u201cproduct of n copies of \u03c1\u201d for n \u2208 N 0.\n(It actually makes a difference whether we are given \u03c1n as n copies or only as pair of \u03c1 and\nn with n in binary \u2013 complexity theory later.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions 108 (282)\nD Regular expression (RegEx): For a given alphabet \u03a3, a regular expression is a\nword produced by the following context-free grammar:\nS \u2212 \u2192G 0 | 1 | a (a \u2208 \u03a3) S \u2212 \u2192G (SS) S \u2212 \u2192G (S + S) S \u2212 \u2192G S\u2217\nThe language L(\u03c1) described by a regular expression \u03c1 is defined inductively:\nL(1) := {\u03b5} L((\u03c1 + \u03c1\u2032)) := L(\u03c1) \u222a L(\u03c1\u2032)\nL(a) := {a} L((\u03c1\u03c1\u2032)) := L(\u03c1)L(\u03c1\u2032)\nL(0) := \u2205 L(\u03c1\u2217) := L(\u03c1)\u2217\nF Let \u201c\u03c1 \u2261 \u03c1\u2032\u201d denote \u201c L(\u03c1) = L(\u03c1\u2032)\u201d (same language/semantics).\nThen \u2261 is an equivalence relation on the regular expressions, and the\nfollowing identities hold (as we compute in (2\u03a3\u2217\n, \u222a, \u25e6, \u2217, \u2205, {\u03b5})):\n\u03c1 + 0 \u2261 \u03c1 \u2261 0 + \u03c1 \u03c1 1 \u2261 \u03c1 \u2261 1\u03c1\n(\u03c1 + \u03c1\u2032) + \u03c1\u2032\u2032 \u2261 \u03c1 + (\u03c1\u2032 + \u03c1\u2032\u2032) ( \u03c1\u03c1\u2032)\u03c1\u2032\u2032 \u2261 \u03c1(\u03c1\u2032\u03c1\u2032\u2032)\n(\u03c1 + \u03c1\u2032)\u03c1\u2032\u2032 \u2261 \u03c1\u03c1\u2032\u2032 + \u03c1\u2032\u03c1\u2032\u2032 \u03c1(\u03c1\u2032 + \u03c1\u2032\u2032) \u2261 \u03c1\u03c1\u2032 + \u03c1\u03c1\u2032\u2032\n\u03c1 + \u03c1\u2032 \u2261 \u03c1\u2032 + \u03c1 0\u03c1 \u2261 0 \u2261 \u03c10\n\u03c1 + \u03c1 \u2261 \u03c1 0\u2217 \u2261 1 \u2261 1\u2217\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kozen\u2019s characterization of the Kleene star 109 (283)\nT For all languages R, S, X\u2286 \u03a3\u2217 (cf. DS, Kozen):\n\u25b7 R\u2217 = {\u03b5} \u222aR R\u2217 = {\u03b5} \u222aR\u2217 R.\n\u25b7 If S \u222a R X\u2286 X, then R\u2217 S \u2286 X.\n\u25b7 If S \u222a X R\u2286 X, then S R\u2217 \u2286 X.\nC For all R, S, X\u2286 \u03a3\u2217:\nD R+ := RR\u2217\n\u25b7 R\u2217 = R\u2217R\u2217 = (R\u2217)\u2217 = (R+)\u2217 = (R\u2217)+\n\u25b7 R\u2217S is the \u2286-least solution of S \u222a R X= X (symmetrically for S R\u2217)\n\u25b7 If R \u2286 S, then R\u2217 \u2286 S\u2217 and R+ \u2286 S+.\n\u25b7 (R \u222a S)\u2217 = R\u2217(SR\u2217)\u2217 = (R\u2217S)\u2217R\u2217\n\u25b7 If RX = XS, then R\u2217X = XS\u2217.\n(In DS these results were already discussed for relations R, S, X\u2286 A \u00d7 A. As L \u2286 \u03a3\u2217 can\nbe represented as RL = {(u, uw) | u \u2208 \u03a3\u2217, w\u2208 L} \u2286\u03a3\u2217 \u00d7 \u03a3\u2217 with L = \u03b5RL and\nL\u2217 = \u03b5R\u2217\nL, the results can be transferred.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kozen\u2019s characterization of the Kleene star 109 (284)\nT For all languages R, S, X\u2286 \u03a3\u2217 (cf. DS, Kozen):\n\u25b7 R\u2217 = {\u03b5} \u222aR R\u2217 = {\u03b5} \u222aR\u2217 R.\n\u25b7 If S \u222a R X\u2286 X, then R\u2217 S \u2286 X.\n\u25b7 If S \u222a X R\u2286 X, then S R\u2217 \u2286 X.\nC Let \u201c\u03c1 \u2291 \u03c1\u2032\u201d abbreviate \u201c L(\u03c1) \u2286 L(\u03c1\u2032).\nThen for all regular expressions \u03c1, \u03c3, \u03bewrt. \u03a3:\n\u25b7 \u03c1\u2217 \u2261 1 + \u03c1\u03c1\u2217 \u2261 1 + \u03c1\u2217\u03c1.\n\u25b7 If \u03c3 + \u03c1\u03be \u2291 \u03be, then \u03c1\u2217 \u03c3 \u2291 \u03be.\n\u25b7 If \u03c3 + \u03be \u03c1\u2291 \u03be, then \u03c3 \u03c1\u2217 \u2291 \u03be.\nand thus\n(\u03c1\u2217)\u2217 \u2261 \u03c1\u2217\u03c1\u2217 \u2261 \u03c1\u2217 \u03c1\u2217\u03c1 \u2261 \u03c1\u03c1\u2217 (\u03c1 + \u03c3)\u2217 \u2261 \u03c1\u2217(\u03c3\u03c1\u2217)\u2217 \u2261 (\u03c1\u2217\u03c3)\u2217\u03c1\u2217\nand \u03c1\u2217\u03c3 is the \u2291-least solution of \u03c3 + \u03c1X \u2261 X.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expression to \u03b5-NFA 110 (285)\n! By definition we start from the simplest possible regular languages\nL(0) = \u2205 L(1) = {\u03b5} L(a) = {a}\nwhich are then combined using union, concatenation and the Kleene star.\nThe inductive definition of L(\u03c1) thus can be extended to a recursive\nprocedure which computes an \u03b5-NFA M with L(\u03c1) = L(M).\nL A regular expression \u03c1 describes a regular language L(\u03c1).\n\u25b7 Proof/construction by induction/recursion over term structure:\n\u25b7 Base cases: \u03c1 = 0, \u03c1 = 1, \u03c1 = a \u2208 \u03a3\nReturn the obvious NFAs for L(0) = \u2205, L(1) = {\u03b5}, and L(a) = {a},\nrespectively.\n\u25b7 Recursive cases: \u03c1 = (\u03c11 + \u03c12), \u03c1 = (\u03c11\u03c12), \u03c1 = \u03c1\u2217\n1:\n(Use \u03c10 \u2261 0 \u2261 0\u03c1 and \u03c1 + 0 \u2261 \u03c1 \u2261 0 + \u03c1 to simplify the expression.)\nRecursively compute FAs Mi = (Qi, \u03a3, \u03b4i, q0,i, Fi) with L(Mi) = L(\u03c1i).\nApply the corresponding construction discussed under \u201cclosure properties\u201d.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expression to \u03b5-NFA 110 (286)\n! By definition we start from the simplest possible regular languages\nL(0) = \u2205 L(1) = {\u03b5} L(a) = {a}\nwhich are then combined using union, concatenation and the Kleene star.\nThe inductive definition of L(\u03c1) thus can be extended to a recursive\nprocedure which computes an \u03b5-NFA M with L(\u03c1) = L(M).\nL A regular expression \u03c1 describes a regular language L(\u03c1).\n\u03c1 = ((a + b)\u2217c)\u2217\nstart a\nstart b\nstart c\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expression to \u03b5-NFA 110 (287)\n! By definition we start from the simplest possible regular languages\nL(0) = \u2205 L(1) = {\u03b5} L(a) = {a}\nwhich are then combined using union, concatenation and the Kleene star.\nThe inductive definition of L(\u03c1) thus can be extended to a recursive\nprocedure which computes an \u03b5-NFA M with L(\u03c1) = L(M).\nL A regular expression \u03c1 describes a regular language L(\u03c1).\n\u03c1 = ((a + b)\u2217c)\u2217\na\nb\nstart cstart\n\u03b5\n\u03b5\n\u03b5\n\u03b5\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expression to \u03b5-NFA 110 (288)\n! By definition we start from the simplest possible regular languages\nL(0) = \u2205 L(1) = {\u03b5} L(a) = {a}\nwhich are then combined using union, concatenation and the Kleene star.\nThe inductive definition of L(\u03c1) thus can be extended to a recursive\nprocedure which computes an \u03b5-NFA M with L(\u03c1) = L(M).\nL A regular expression \u03c1 describes a regular language L(\u03c1).\n\u03c1 = ((a + b)\u2217c)\u2217\na\nb\nstart c\n\u03b5\n\u03b5\n\u03b5\n\u03b5\nstart\n\u03b5\n\u03b5\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expression to \u03b5-NFA 110 (289)\n! By definition we start from the simplest possible regular languages\nL(0) = \u2205 L(1) = {\u03b5} L(a) = {a}\nwhich are then combined using union, concatenation and the Kleene star.\nThe inductive definition of L(\u03c1) thus can be extended to a recursive\nprocedure which computes an \u03b5-NFA M with L(\u03c1) = L(M).\nL A regular expression \u03c1 describes a regular language L(\u03c1).\n\u03c1 = ((a + b)\u2217c)\u2217\na\nb\nc\n\u03b5\n\u03b5\n\u03b5\n\u03b5\nstart\n\u03b5\n\u03b5 \u03b5\n\u03b5\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expression to \u03b5-NFA 110 (290)\n! By definition we start from the simplest possible regular languages\nL(0) = \u2205 L(1) = {\u03b5} L(a) = {a}\nwhich are then combined using union, concatenation and the Kleene star.\nThe inductive definition of L(\u03c1) thus can be extended to a recursive\nprocedure which computes an \u03b5-NFA M with L(\u03c1) = L(M).\nL A regular expression \u03c1 describes a regular language L(\u03c1).\n\u03c1 = ((a + b)\u2217c)\u2217\na\nb\nc\n\u03b5\n\u03b5\n\u03b5\n\u03b5\u03b5\n\u03b5 \u03b5\n\u03b5\nstart \u03b5 \u03b5\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expressions to NFAs (*) 111 (291)\n\u25b7 \u03b5-transitions are convenient for the basic construction, but usually need to be\nremoved at some point, e.g. for complementing.\n\u25b7 An alternative is to use NFAs with (i) multiple initial states, and (ii) (at most)\none final state without outgoing transitions (and no \u03b5-transitions)1:\n\u25b7 Base cases \u03c1 = 0, \u03c1 = 1, \u03c1 = a \u2208 \u03a3: Return the obvious NFAs.\n\u25b7 Recursive cases: \u03c1 = (\u03c11 + \u03c12), \u03c1 = (\u03c11\u03c12), \u03c1 = \u03c1\u2217\n1:\nRecursively compute FAs Mi = (Qi, \u03a3, \u03b4i, Ii, {qf,i}) with L(Mi) = L(\u03c1i).\nCase \u03c1 = (\u03c11 + \u03c12): treat/collapse qf,1 and qf,2 as/into a single state qf ; if\nqf,1 or qf,2 were initial, also qf is initial; all initial states stay initial.\nCase \u03c1 = (\u03c11\u03c12): for each transition ((q, a), qf,1) (a \u2208 \u03a3) leading into qf,1 add\na transition ((q, a), q\u2032) to every q\u2032 \u2208 I2; then delete qf,1; if qf,1 \u2208 I1, both\nI1 \\ {qf,1} and I2 stay initial, else only I1 \\ {qf,1}.\nCase \u03c1 = \u03c1\u2217\n1: for each transition ((q, a), qf,1) (a \u2208 \u03a3) leading into qf,1 add a",
      "a transition ((q, a), q\u2032) to every q\u2032 \u2208 I2; then delete qf,1; if qf,1 \u2208 I1, both\nI1 \\ {qf,1} and I2 stay initial, else only I1 \\ {qf,1}.\nCase \u03c1 = \u03c1\u2217\n1: for each transition ((q, a), qf,1) (a \u2208 \u03a3) leading into qf,1 add a\ntransition ((q, a), q\u2032) to every q\u2032 \u2208 I1; further add qf,1 to the initial states.\n! When determinizing simply start with the set of all initial states.\n1as noted by Klara J. Meyer and Salomon Sickert.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expressions to NFAs (*) 111 (292)\n\u25b7 \u03b5-transitions are convenient for the basic construction, but usually need to be\nremoved at some point, e.g. for complementing.\n\u25b7 An alternative is to use NFAs with (i) multiple initial states, and (ii) (at most)\none final state without outgoing transitions (and no \u03b5-transitions)1:\n\u03c1 = ((a + b)\u2217c)\u2217\nstart a\nstart b\nstart c\n1as noted by Klara J. Meyer and Salomon Sickert.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expressions to NFAs (*) 111 (293)\n\u25b7 \u03b5-transitions are convenient for the basic construction, but usually need to be\nremoved at some point, e.g. for complementing.\n\u25b7 An alternative is to use NFAs with (i) multiple initial states, and (ii) (at most)\none final state without outgoing transitions (and no \u03b5-transitions)1:\n\u03c1 = ((a + b)\u2217c)\u2217\nstart a\nstart\nb start c\n1as noted by Klara J. Meyer and Salomon Sickert.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expressions to NFAs (*) 111 (294)\n\u25b7 \u03b5-transitions are convenient for the basic construction, but usually need to be\nremoved at some point, e.g. for complementing.\n\u25b7 An alternative is to use NFAs with (i) multiple initial states, and (ii) (at most)\none final state without outgoing transitions (and no \u03b5-transitions)1:\n\u03c1 = ((a + b)\u2217c)\u2217\nstart\nstart\na\na\nstart\nb\nbab start c\n1as noted by Klara J. Meyer and Salomon Sickert.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expressions to NFAs (*) 111 (295)\n\u25b7 \u03b5-transitions are convenient for the basic construction, but usually need to be\nremoved at some point, e.g. for complementing.\n\u25b7 An alternative is to use NFAs with (i) multiple initial states, and (ii) (at most)\none final state without outgoing transitions (and no \u03b5-transitions)1:\n\u03c1 = ((a + b)\u2217c)\u2217\nstart\na\na\nstart\nb\nbab\nstart\nc\n1as noted by Klara J. Meyer and Salomon Sickert.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expressions to NFAs (*) 111 (296)\n\u25b7 \u03b5-transitions are convenient for the basic construction, but usually need to be\nremoved at some point, e.g. for complementing.\n\u25b7 An alternative is to use NFAs with (i) multiple initial states, and (ii) (at most)\none final state without outgoing transitions (and no \u03b5-transitions)1:\n\u03c1 = ((a + b)\u2217c)\u2217\nstart\na\na\nstart\nb\nbab\nstart start\nc\nc\nc\nc\n1as noted by Klara J. Meyer and Salomon Sickert.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (297)\nV = {S, T} S \u2212 \u2192G aS | T T \u2212 \u2192G bT | S | \u03b5\nS \u2261 aS + T\nT \u2261 bT + S + 1 fG(0) \u2261\n\u00120\n1\n\u0013\n! It remains to show that regular expressions can describe every regular\nlanguages.\n\u25b7 To see this, it is useful to read a regular grammar as systems of linear\nfixed-point equations x = fG(x) whose (least) solution is the language\ngenerated by its nonterminals/variables:\nfG(0) \u2291 f2\nG(0) \u2291 . . .\u2291 fk\nG(0) \u2291 fk+1\nG (0) = fG(fk\nG(0) \u2291 . . .\nor using matrices (the matrix A is also the transition matrix of the \u03b5-NFA)\nfk+1\nG (0) \u2261\n\u0012\na 1\n1 b\n\u00130 \u0012\n0\n1\n\u0013\n+\n\u0012\na 1\n1 b\n\u00131 \u0012\n0\n1\n\u0013\n+ . . .+\n\u0012\na 1\n1 b\n\u0013k \u0012\n0\n1\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (298)\nV = {S, T} S \u2212 \u2192G aS | T T \u2212 \u2192G bT | S | \u03b5\nS \u2261 aS + T\nT \u2261 bT + S + 1 f2\nG(0) \u2261\n\u0012 1\nb + 1\n\u0013\n! It remains to show that regular expressions can describe every regular\nlanguages.\n\u25b7 To see this, it is useful to read a regular grammar as systems of linear\nfixed-point equations x = fG(x) whose (least) solution is the language\ngenerated by its nonterminals/variables:\nfG(0) \u2291 f2\nG(0) \u2291 . . .\u2291 fk\nG(0) \u2291 fk+1\nG (0) = fG(fk\nG(0) \u2291 . . .\nor using matrices (the matrix A is also the transition matrix of the \u03b5-NFA)\nfk+1\nG (0) \u2261\n\u0012\na 1\n1 b\n\u00130 \u0012\n0\n1\n\u0013\n+\n\u0012\na 1\n1 b\n\u00131 \u0012\n0\n1\n\u0013\n+ . . .+\n\u0012\na 1\n1 b\n\u0013k \u0012\n0\n1\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (299)\nV = {S, T} S \u2212 \u2192G aS | T T \u2212 \u2192G bT | S | \u03b5\nS \u2261 aS + T\nT \u2261 bT + S + 1 f3\nG(0) \u2261\n\u0012a + b + 1\nbb + b + 1\n\u0013\n! It remains to show that regular expressions can describe every regular\nlanguages.\n\u25b7 To see this, it is useful to read a regular grammar as systems of linear\nfixed-point equations x = fG(x) whose (least) solution is the language\ngenerated by its nonterminals/variables:\nfG(0) \u2291 f2\nG(0) \u2291 . . .\u2291 fk\nG(0) \u2291 fk+1\nG (0) = fG(fk\nG(0) \u2291 . . .\nor using matrices (the matrix A is also the transition matrix of the \u03b5-NFA)\nfk+1\nG (0) \u2261\n\u0012\na 1\n1 b\n\u00130 \u0012\n0\n1\n\u0013\n+\n\u0012\na 1\n1 b\n\u00131 \u0012\n0\n1\n\u0013\n+ . . .+\n\u0012\na 1\n1 b\n\u0013k \u0012\n0\n1\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (300)\nV = {S, T} S \u2212 \u2192G aS | T T \u2212 \u2192G bT | S | \u03b5\nS \u2261 aS + T\nT \u2261 bT + S + 1 f4\nG(0) \u2261\n\u0012 aa + ab + a + bb + b + 1\nbbb + bb + b + 1 +a + b + 1\n\u0013\n! It remains to show that regular expressions can describe every regular\nlanguages.\n\u25b7 To see this, it is useful to read a regular grammar as systems of linear\nfixed-point equations x = fG(x) whose (least) solution is the language\ngenerated by its nonterminals/variables:\nfG(0) \u2291 f2\nG(0) \u2291 . . .\u2291 fk\nG(0) \u2291 fk+1\nG (0) = fG(fk\nG(0) \u2291 . . .\nor using matrices (the matrix A is also the transition matrix of the \u03b5-NFA)\nfk+1\nG (0) \u2261\n\u0012\na 1\n1 b\n\u00130 \u0012\n0\n1\n\u0013\n+\n\u0012\na 1\n1 b\n\u00131 \u0012\n0\n1\n\u0013\n+ . . .+\n\u0012\na 1\n1 b\n\u0013k \u0012\n0\n1\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (301)\nV = {S, T} S \u2212 \u2192G aS | T T \u2212 \u2192G bT | S | \u03b5\nS \u2261 aS + T\nT \u2261 bT + S + 1 f\u221e\nG (0) \u2261\n\u0012a\u2217(b + a\u2217)\u2217\n(b + a\u2217)\u2217\n\u0013\n! It remains to show that regular expressions can describe every regular\nlanguages.\n\u25b7 To see this, it is useful to read a regular grammar as systems of linear\nfixed-point equations x = fG(x) whose (least) solution is the language\ngenerated by its nonterminals/variables:\nfG(0) \u2291 f2\nG(0) \u2291 . . .\u2291 fk\nG(0) \u2291 fk+1\nG (0) = fG(fk\nG(0) \u2291 . . .\nor using matrices (the matrix A is also the transition matrix of the \u03b5-NFA)\nfk+1\nG (0) \u2261\n\u0012\na 1\n1 b\n\u00130 \u0012\n0\n1\n\u0013\n+\n\u0012\na 1\n1 b\n\u00131 \u0012\n0\n1\n\u0013\n+ . . .+\n\u0012\na 1\n1 b\n\u0013k \u0012\n0\n1\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (302)\nV = {S, T} S \u2212 \u2192G aS | T T \u2212 \u2192G bT | S | \u03b5\nS \u2261 aS + T\nT \u2261 bT + S + 1 f\u221e\nG (0) \u2261\n\u0012a\u2217(b + a\u2217)\u2217\n(b + a\u2217)\u2217\n\u0013\nL For two languages L1, L2 \u2286 \u03a3\u2217 the least solution of X = L1X \u222a L2 is\nX = L\u2217\n1L2 =\n[\nk\u2208N 0\nLk\n1L2 = L2 \u222a L1L2 \u222a L1L1L2 . . .\n(cf. Kozen\u2019s characterization; sometimes called \u201cArden\u2019s lemma\u201d.)\nC Rephrased wrt. regular expressions: For two regular expressions \u03b1, \u03b2the least\nsolution of X \u2261 \u03b1X + \u03b2 is thus X \u2261 \u03b1\u2217\u03b2.\n! E.g. X \u2261 X + 1 has infinitely many solutions:\nAny \u03c1 with \u03b5 \u2208 L(\u03c1) is a solution, but 1\u22171 = 1 is the least solution.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (303)\nV = {S, T} S \u2212 \u2192G aS | T T \u2212 \u2192G bT | S | \u03b5\nS \u2261 aS + T\nT \u2261 bT + S + 1 f\u221e\nG (0) \u2261\n\u0012a\u2217(b + a\u2217)\u2217\n(b + a\u2217)\u2217\n\u0013\n\u25b7 This allows to compute a regular expression for L(G) by solving the linear\nequation system for the start symbol by means of substitution:\n(1)S \u2261 aS + T\n(2)T \u2261 bT + S + 1\n(1\u2032)S \u2261 a\u2217T (solve (1) for S)\n(2\u2032)T \u2261 (b + a\u2217)T + 1 ( subs. (1\u2032) in (2))\n(2\u2032\u2032)T \u2261 (b + a\u2217)\u2217 (solve (2\u2032) for (T))\n(1\u2032\u2032)S \u2261 a\u2217(b + a\u2217)\u2217 (subs. (2\u2032\u2032) in (1\u2032))\n(As regular expression do not allow complement/difference, they have to describe all\naccepting runs of an FA which might get lengthy sometimes.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (304)\nV = {S, T} S \u2212 \u2192G aS | T T \u2212 \u2192G bT | S | \u03b5\nS \u2261 aS + T\nT \u2261 bT + S + 1 f\u221e\nG (0) \u2261\n\u0012a\u2217(b + a\u2217)\u2217\n(b + a\u2217)\u2217\n\u0013\nL For every regular language L there is a regular expression \u03c1 with L = L(\u03c1).\n\u25b7 Proof sketch:\n\u25b7 Let G = (V, \u03a3, P, S) be a regular grammar. For each X \u2208 V define:\nX \u2261 P\n(X,aY )\u2208P aY + P\n(X,Y )\u2208P Y + P\n(X,a)\u2208P a + P\n(X,\u03b5)\u2208P 1\nThe corresponding fixed-point iteration defined by X(0) := 0 and\nX(i+1) \u2261 P\n(X,aY )\u2208P aY (i) + P\n(X,Y )\u2208P Y (i) + P\n(X,a)\u2208P a + P\n(X,\u03b5)\u2208P 1\nconverges to the languages generated by the nonterminals which is the least\nsolution of resulting (right-)linear equation system.\n\u25b7 Solve this system for S using that \u03b1\u2217\u03b2 is the least solution of X \u2261 \u03b1X + \u03b2 for\nall regular expressions \u03b1, \u03b2(resp. for regular langauges L(\u03b1), L(\u03b2)).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (305)\nV = {S, T} S \u2212 \u2192G aS | T T \u2212 \u2192G bT | S | \u03b5\nS \u2261 aS + T\nT \u2261 bT + S + 1 f\u221e\nG (0) \u2261\n\u0012a\u2217(b + a\u2217)\u2217\n(b + a\u2217)\u2217\n\u0013\n\u25b7 This also proves that we can allow regular expressions as abbreviations on the\nedges of an FA, e.g.:\npstart q ra\u2217b b+c\nabc\nThe represented/accepted language is still regular:\nXp \u2261 a\u2217bXq Xq \u2261 (b + c)Xr + 1 Xr \u2261 abcXp\nThis of course also means that, if we allow regular expressions as \u201cterminals\u201d\nin regular grammars, we still generate a regular language (but this requires that\n0, 1, +, \u2217 are treated as metasymbols and not as terminals) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (306)\nV = {S, T} S \u2212 \u2192G aS | T T \u2212 \u2192G bT | S | \u03b5\nS \u2261 aS + T\nT \u2261 bT + S + 1 f\u221e\nG (0) \u2261\n\u0012a\u2217(b + a\u2217)\u2217\n(b + a\u2217)\u2217\n\u0013\n\u25b7 We also obtain from this that left-linear grammars also generate regular\nlanguages, e.g.:\nS \u2212 \u2192G Sa | T b T \u2212 \u2192G T b| c\ncorresponds to\nS \u2261 Sa + T b T \u2261 T b+ c\nwith\nT \u2261 c b\u2217 S \u2261 c b\u2217ba\u2217\n! But linear grammars in general generate non-regular languages:\nS \u2212 \u2192G\u2032 aT | \u03b5 T \u2212 \u2192G\u2032 Sb L (G\u2032) = {anbn | n \u2208 N 0}\n(See Myhill-Nerode for why {anbn | n \u2208 N 0} is not regular.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions and matrix iteration (*) 113 (307)\nF Given a matrix A of regular expressions\nA =\n\uf8eb\n\uf8ec\uf8ed\na1,1 a1,2 . . .\na2,1 a2,2 . . .\n... ... ...\n\uf8f6\n\uf8f7\uf8f8 =\n\u0012 a1,1 A1,2\nA2,1 A2,2\n\u0013\nwe can recursively compute A\u2217 by partitioning A wrt. a1,1\nA\u2217 =\n\u0012a\u2217\n1,1 + a\u2217\n1,1A1,2\u0393A2,1a\u2217\n1,1 a\u2217\n1,1A1,2\u0393\n\u0393A2,1a\u2217\n1,1 \u0393\n\u0013\n\u0393 := (A2,2 + A2,1a\u2217\n1,1A1,2)\u2217\nso that the least solution of x = Ax + y is A\u2217y.\nV = {S, T} S \u2212 \u2192G aS | T T \u2212 \u2192G bT | S | \u03b5\nS \u2261 aS + T\nT \u2261 bT + S + 1\n\u0012\nS\nT\n\u0013\n\u2261\n\u0012\na 1\n1 b\n\u0013\u0012\nS\nT\n\u0013\n+\n\u0012\n0\n1\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions and matrix iteration (*) 113 (308)\nF Given a matrix A of regular expressions\nA =\n\uf8eb\n\uf8ec\uf8ed\na1,1 a1,2 . . .\na2,1 a2,2 . . .\n... ... ...\n\uf8f6\n\uf8f7\uf8f8 =\n\u0012 a1,1 A1,2\nA2,1 A2,2\n\u0013\nwe can recursively compute A\u2217 by partitioning A wrt. a1,1\nA\u2217 =\n\u0012a\u2217\n1,1 + a\u2217\n1,1A1,2\u0393A2,1a\u2217\n1,1 a\u2217\n1,1A1,2\u0393\n\u0393A2,1a\u2217\n1,1 \u0393\n\u0013\n\u0393 := (A2,2 + A2,1a\u2217\n1,1A1,2)\u2217\nso that the least solution of x = Ax + y is A\u2217y.\nV = {S, T} S \u2212 \u2192G aS | T T \u2212 \u2192G bT | S | \u03b5\nS \u2261 aS + T\nT \u2261 bT + S + 1\n\u0012\nS\nT\n\u0013\n\u2261\n\u0012\na\u2217 + a\u2217(b + a\u2217)\u2217a\u2217 a\u2217(b + a\u2217)\u2217\n(b + a\u2217)\u2217a\u2217 (b + a\u2217)\u2217\n\u0013\u0012\n0\n1\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions and matrix iteration (*) 113 (309)\nF Given a matrix A of regular expressions\nA =\n\uf8eb\n\uf8ec\uf8ed\na1,1 a1,2 . . .\na2,1 a2,2 . . .\n... ... ...\n\uf8f6\n\uf8f7\uf8f8 =\n\u0012 a1,1 A1,2\nA2,1 A2,2\n\u0013\nwe can recursively compute A\u2217 by partitioning A wrt. a1,1\nA\u2217 =\n\u0012a\u2217\n1,1 + a\u2217\n1,1A1,2\u0393A2,1a\u2217\n1,1 a\u2217\n1,1A1,2\u0393\n\u0393A2,1a\u2217\n1,1 \u0393\n\u0013\n\u0393 := (A2,2 + A2,1a\u2217\n1,1A1,2)\u2217\nso that the least solution of x = Ax + y is A\u2217y.\nV = {S, T} S \u2212 \u2192G aS | T T \u2212 \u2192G bT | S | \u03b5\nS \u2261 aS + T\nT \u2261 bT + S + 1\n\u0012\nS\nT\n\u0013\n\u2261\n\u0012\na\u2217(b + a\u2217)\u2217\n(b + a\u2217)\u2217\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions and matrix iteration (*) 113 (310)\nF Given a matrix A of regular expressions\nA =\n\uf8eb\n\uf8ec\uf8ed\na1,1 a1,2 . . .\na2,1 a2,2 . . .\n... ... ...\n\uf8f6\n\uf8f7\uf8f8 =\n\u0012 a1,1 A1,2\nA2,1 A2,2\n\u0013\nwe can recursively compute A\u2217 by partitioning A wrt. a1,1\nA\u2217 =\n\u0012a\u2217\n1,1 + a\u2217\n1,1A1,2\u0393A2,1a\u2217\n1,1 a\u2217\n1,1A1,2\u0393\n\u0393A2,1a\u2217\n1,1 \u0393\n\u0013\n\u0393 := (A2,2 + A2,1a\u2217\n1,1A1,2)\u2217\nso that the least solution of x = Ax + y is A\u2217y.\n! The expressions a\u2217\n1,1 + a\u2217\n1,1A1,2\u0393A2,1a\u2217\n1,1 simply describes how to go from\nstate 1 to state 1 in finitely (but unbounded) many steps:\n\u25b7 a1,1: stay/loop in 1.\n\u25b7 A1,2: enter some state k >1.\n\u25b7 \u0393: spend some time away from 1.\n\u25b7 A2,1: return to 1.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions and matrix iteration (*) 113 (311)\nF Given a matrix A of regular expressions\nA =\n\uf8eb\n\uf8ec\uf8ed\na1,1 a1,2 . . .\na2,1 a2,2 . . .\n... ... ...\n\uf8f6\n\uf8f7\uf8f8 =\n\u0012 a1,1 A1,2\nA2,1 A2,2\n\u0013\nwe can recursively compute A\u2217 by partitioning A wrt. a1,1\nA\u2217 =\n\u0012a\u2217\n1,1 + a\u2217\n1,1A1,2\u0393A2,1a\u2217\n1,1 a\u2217\n1,1A1,2\u0393\n\u0393A2,1a\u2217\n1,1 \u0393\n\u0013\n\u0393 := (A2,2 + A2,1a\u2217\n1,1A1,2)\u2217\nso that the least solution of x = Ax + y is A\u2217y.\n\u25b7 In general:\nGiven a finite directed graph ([n], E), we can turn it into a DFA by assigning\neach edge (i, j) a unique terminal ai,j.\nThen the (i, j)-entry of A\u2217 will be a regular expression that describes\n(\u201csummarizes\u201d) all finite i-j-paths.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions and matrix iteration (*) 113 (312)\nF Given a matrix A of regular expressions\nA =\n\uf8eb\n\uf8ec\uf8ed\na1,1 a1,2 . . .\na2,1 a2,2 . . .\n... ... ...\n\uf8f6\n\uf8f7\uf8f8 =\n\u0012 a1,1 A1,2\nA2,1 A2,2\n\u0013\nwe can recursively compute A\u2217 by partitioning A wrt. a1,1\nA\u2217 =\n\u0012a\u2217\n1,1 + a\u2217\n1,1A1,2\u0393A2,1a\u2217\n1,1 a\u2217\n1,1A1,2\u0393\n\u0393A2,1a\u2217\n1,1 \u0393\n\u0013\n\u0393 := (A2,2 + A2,1a\u2217\n1,1A1,2)\u2217\nso that the least solution of x = Ax + y is A\u2217y.\n\u25b7 Analogously, every NFA (Q, \u03a3, \u03b4, q0, F) with |Q| = n gives rise to adjacency\nmatrices Aa \u2208 {0, 1}n\u00d7n (a \u2208 \u03a3). Set A = P\na\u2208\u03a3 aAa, then\nAk =\nX\na1...ak\u2208\u03a3k\na1 . . . akAa1 \u00b7\u00b7\u00b7 Aak\nsummarizes the behavior of the NFA for all words of length k, and\nA\u2217 = P\u221e\nk=0 Ak summarizes the complete behavior of the NFA.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions and matrix iteration (*) 113 (313)\nF Given a matrix A of regular expressions\nA =\n\uf8eb\n\uf8ec\uf8ed\na1,1 a1,2 . . .\na2,1 a2,2 . . .\n... ... ...\n\uf8f6\n\uf8f7\uf8f8 =\n\u0012 a1,1 A1,2\nA2,1 A2,2\n\u0013\nwe can recursively compute A\u2217 by partitioning A wrt. a1,1\nA\u2217 =\n\u0012a\u2217\n1,1 + a\u2217\n1,1A1,2\u0393A2,1a\u2217\n1,1 a\u2217\n1,1A1,2\u0393\n\u0393A2,1a\u2217\n1,1 \u0393\n\u0013\n\u0393 := (A2,2 + A2,1a\u2217\n1,1A1,2)\u2217\nso that the least solution of x = Ax + y is A\u2217y.\n\u25b7 And in case of an \u03b5-NFA we further have a adjacency matrix A\u03b5 wrt.\n\u03b5-transitions.\nRemoving \u03b5-transitions means to replace Aa by A\u2217\n\u03b5AaA\u2217\n\u03b5:\nAs A\u03b5 is just a boolean adjacency matrix ( 1 + 1 \u2261 1), A\u2217\n\u03b5 describes if some\nstate p can reach some state q only by means of \u03b5-transitions.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Generalized Floyd-Warshall algorithm, Kleene\u2019s algorithm, . . . 114 (314)\n\u25b7 The recursive computation of A\u2217 without the use of idempotence\nA\u2217 =\n\u0012a\u2217\n1,1 + a\u2217\n1,1A1,2\u0393A2,1a\u2217\n1,1 a\u2217\n1,1A1,2\u0393\n\u0393A2,1a\u2217\n1,1 \u0393\n\u0013\n\u0393 := (A2,2 + A2,1a\u2217\n1,1A1,2)\u2217\nunifies several important algorithms.\n\u25b7 The Floyd-Warshall algorithm for shortest paths.\n\u25b7 Kleene\u2019s algorithm for computing regular expressions.\n\u25b7 Solving systems of linear fixed-point equations over R via the\nNeumann/geometric series.\n\u25b7 Computing the generating functions for counting the paths in the graph.\n\u25b7 Computing the Cesaro mean by means of the Neumann series/generating\nfunction. Special case: computing the stationary distribution and expected\ntimes of first return/arrival.\n\u25b7 . . .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Generalized Floyd-Warshall algorithm, Kleene\u2019s algorithm, . . . 114 (315)\n\u25b7 The recursive computation of A\u2217 without the use of idempotence\nA\u2217 =\n\u0012a\u2217\n1,1 + a\u2217\n1,1A1,2\u0393A2,1a\u2217\n1,1 a\u2217\n1,1A1,2\u0393\n\u0393A2,1a\u2217\n1,1 \u0393\n\u0013\n\u0393 := (A2,2 + A2,1a\u2217\n1,1A1,2)\u2217\nunifies several important algorithms.\n\u25b7 In general, (right/left) linear fixed-point systems\nAx + y = x resp x\u22a4A + y = x\u22a4\ndescribe the semantics/behavior/equilibrium of some abstract \u201cflow\u201d of\ndynamic linear systems.\n\u25b7 In fact, also linear grammars can be represented as linear fixed-point systems\nusing \u201ccontexts\u201d instead of words:\n\u25b7 A context is a pair of words (u, v) \u2208 \u03a3 \u00d7 \u03a3 with the concatenation of contexts\nnow defined by (u, v) \u25e6 (x, y) := (ux, yv).\n\u25b7 E.g. L = {anbn | n \u2208 N 0} is then the least solution of X \u2261 (a, b)X + (\u03b5, \u03b5)\nwhich can be described by (a, b)\u2217(\u03b5, \u03b5).\n\u25b7 This allows to approximate context-free languages by means of Newton\u2019s",
      "now defined by (u, v) \u25e6 (x, y) := (ux, yv).\n\u25b7 E.g. L = {anbn | n \u2208 N 0} is then the least solution of X \u2261 (a, b)X + (\u03b5, \u03b5)\nwhich can be described by (a, b)\u2217(\u03b5, \u03b5).\n\u25b7 This allows to approximate context-free languages by means of Newton\u2019s\nmethod.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rational expressions for fixed-points (*) 115 (316)\n\u25b7 Example: Consider for simplicity the case n = 2:\nx1\ny1\nx2\ny2\na b d\nc1 1\n\u0012\nx1\nx2\n\u0013\n=\n\u0012\na b\nc d\n\u0013\n\u00b7 x +\n\u0012\ny1\ny2\n\u0013\n\u25b7 Using z\u2217 as abbreviation for \u201cleast solution of X = 1 + zX\u201d we can solve\nx1 = ax1 + bx2 + y1 \u21dd x1 = (1 \u2212 a)\u22121(bx2 + e) = a\u2217(bx2 + y1)\nx2 = ca\u2217(bx2 + y1) + dx2 + y2 \u21dd x2 = (d + ca\u2217b)\u2217(ca\u2217y1 + y2)\nor in matrix form\nx =\n\u0012a\u2217 + a\u2217b(d + ca\u2217b)\u2217ca\u2217 a\u2217b(d + ca\u2217b)\u2217\n(d + ca\u2217b)\u2217ca\u2217 (d + ca\u2217b)\u2217\n\u0013\n\u00b7\n\u0012y1\ny2\n\u0013\n(assuming that all expressions are defined)\n! For rational expressions, addition is not idempotent, i.e. 1 + 1 = 2 \u0338= 1 and in\ngeneral \u03c1 + \u03c1 \u0338= \u03c1 and (\u03c1\u2217)\u2217 \u0338= \u03c1\u2217.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rational expressions for fixed-points (*) 115 (317)\n\u25b7 Example: Consider for simplicity the case n = 2:\nx1\ny1\nx2\ny2\na b d\nc1 1\n\u0012\nx1\nx2\n\u0013\n=\n\u0012\na\u2217 + a\u2217b(d + ca\u2217b)\u2217ca\u2217 a\u2217b(d + ca\u2217b)\u2217\n(d + ca\u2217b)\u2217ca\u2217 (d + ca\u2217b)\u2217\n\u0013\n\u00b7\n\u0012\ny1\ny2\n\u0013\n\u25b7 (+, \u00b7, \u2217, 0, 1) can be instantiated in different ways:\n\u25b7 (2\u03a3\u2217\n, \u222a, \u25e6, \u2205, {\u03b5}) with L\u2217 = S\nk\u2208N 0 Lk\n\u25b7 (2V \u00d7V , \u222a, \u25e6, \u2205, IdV ) with R\u2217 = S\nk\u2208N 0 Rk.\n\u25b7 (R , +, \u00b7, 0, 1) with z\u2217 := P\u221e\nk=0 zk\n\u25b7 ({0, 1}, max, min, 0, 1) with z\u2217 = 1.\n\u25b7 (R \u222a {\u00b1\u221e}, min, +, +\u221e, 0) with z\u2217 = \u2212\u221e, if z <0, else z\u2217 = 0.\n\u25b7 (R \u222a {\u00b1\u221e}, max, +, \u2212\u221e, 0) with z\u2217 = \u221e, if z >0, else z\u2217 = 0.\n\u25b7 ([0, 1], max, \u00b7, 0, 1) with z\u2217 = 1.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rational expressions for fixed-points (*) 115 (318)\n\u25b7 Example: Consider for simplicity the case n = 2:\nx1\ny1\nx2\ny2\na b d\nc1 1\n\u0012\nx1\nx2\n\u0013\n=\n\u0012\na\u2217 + a\u2217b(d + ca\u2217b)\u2217ca\u2217 a\u2217b(d + ca\u2217b)\u2217\n(d + ca\u2217b)\u2217ca\u2217 (d + ca\u2217b)\u2217\n\u0013\n\u00b7\n\u0012\ny1\ny2\n\u0013\n\u25b7 Without the use of idempotence ( 1 + 1 \u0338= 1) an expression like\n(d + ca\u2217b)\u2217\nis called a rational expression:\n\u25b7 It summarizes the effect of all finite cycles leading from the node x2 to x2\nagain: Every such cycle can be partitioned wrt. the returns to x2,\ni.e. either return directly by means of the edge d\nor return by means of a detour to x1 as described by ca\u2217b.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rational expressions for fixed-points (*) 115 (319)\n\u25b7 Example: Consider for simplicity the case n = 2:\nx1\ny1\nx2\ny2\na b d\nc1 1\n\u0012\nx1\nx2\n\u0013\n=\n\u0012\na\u2217 + a\u2217b(d + ca\u2217b)\u2217ca\u2217 a\u2217b(d + ca\u2217b)\u2217\n(d + ca\u2217b)\u2217ca\u2217 (d + ca\u2217b)\u2217\n\u0013\n\u00b7\n\u0012\ny1\ny2\n\u0013\n\u25b7 E.g. when computing the generating function for the Fibonacci numbers, we\nhave to solve the following system over the field of rational functions in z:\n\u0012gF (z)\ngG(z)\n\u0013\n=\n\u0012z z\nz 0\n\u0013\n\u00b7\n\u0012gF (z)\ngG(z)\n\u0013\n+\n\u00120\n1\n\u0013\nSetting a = b = c = z and d = 0 in our generic solution, we obtain\n\u0012gF (z)\ngG(z)\n\u0013\n=\n\u0012z\u2217 + z\u2217z(z2z\u2217)\u2217zz\u2217 z\u2217z(z2z\u2217)\u2217\n(z2z\u2217)\u2217zz\u2217 (z2z\u2217)\u2217\n\u0013\n\u00b7\n\u00120\n1\n\u0013\nWith z\u2217 = (1 \u2212 z)\u22121 we obtain e.g. gF (z) = z\n1\u2212z\u2212z2\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rational expressions for fixed-points (*) 115 (320)\n\u25b7 Example: Consider for simplicity the case n = 2:\nx1\ny1\nx2\ny2\na b d\nc1 1\n\u0012\nx1\nx2\n\u0013\n=\n\u0012\na\u2217 + a\u2217b(d + ca\u2217b)\u2217ca\u2217 a\u2217b(d + ca\u2217b)\u2217\n(d + ca\u2217b)\u2217ca\u2217 (d + ca\u2217b)\u2217\n\u0013\n\u00b7\n\u0012\ny1\ny2\n\u0013\n\u25b7 As another application, recall that, if the Ces` aro mean exists, then\nlim\nl\u2192\u221e\n1\n1 + l\nlX\nk=0\nPk = lim\nz\u21921\u2212\n(1 \u2212 z)\n\u221eX\nk=0\n(zP )k\nFor instance, for the \u201cFibonacci graph\u201d with d = 0\nzP =\n\u0012z/2 z/2\nz 0\n\u0013\nwe have a\u2217 = 2\n2\u2212z and (d + ca\u2217b)\u2217 = 2\u2212z\n(2+z)(1\u2212z) for 0 \u2264 z <1 so that:\n(1 \u2212 z)(zP )\u2217 =\n 2(1\u2212z)\n(2\u2212z) + 2z2\n(2\u2212z)(2+z)\nz\n(2+z)\n2z\n(2+z)\n2\u2212z\n(2+z)\n!\nz\u21921\u2212\n\u2212 \u2212 \u2212 \u2212 \u2192\n\u00122\n3\n1\n32\n3\n1\n3\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rational expressions for fixed-points (*) 115 (321)\n\u25b7 Example: Consider for simplicity the case n = 2:\nx1\ny1\nx2\ny2\na b d\nc1 1\n\u0012\nx1\nx2\n\u0013\n=\n\u0012\na\u2217 + a\u2217b(d + ca\u2217b)\u2217ca\u2217 a\u2217b(d + ca\u2217b)\u2217\n(d + ca\u2217b)\u2217ca\u2217 (d + ca\u2217b)\u2217\n\u0013\n\u00b7\n\u0012\ny1\ny2\n\u0013\n\u25b7 For instance, consider again the adjacency matrix underlying the Fibonacci\nnumbers with the d-loop disabled:\nA =\n\u00121 1\n1 0\n\u0013\nThen the standard reflexive-transitive closure is obtained wrt. the Boolean\nsemiring ({0, 1}, max, min, 0, 1) with z\u2217 = 1\nA\u2217 =\n\u00121 1\n1 1\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rational expressions for fixed-points (*) 115 (322)\n\u25b7 Example: Consider for simplicity the case n = 2:\nx1\ny1\nx2\ny2\na b d\nc1 1\n\u0012\nx1\nx2\n\u0013\n=\n\u0012\na\u2217 + a\u2217b(d + ca\u2217b)\u2217ca\u2217 a\u2217b(d + ca\u2217b)\u2217\n(d + ca\u2217b)\u2217ca\u2217 (d + ca\u2217b)\u2217\n\u0013\n\u00b7\n\u0012\ny1\ny2\n\u0013\n\u25b7 If we instead treat the parameters a, b, c, das distances and compute wrt. the\nmin-tropical semiring (R \u222a {\u00b1\u221e}, min, +, +\u221e, 0)\nwith z\u2217 = \u2212\u221e, if z <0, else z\u2217 = 0, then e.g. for\nD =\n\u00121 2\n3 + \u221e\n\u0013\nD\u2217 =\n\u00120 2\n3 0\n\u0013\nbut for\nD =\n\u00121 2\n3 \u22121\n\u0013\nD\u2217 =\n\u0012\u2212\u221e \u2212\u221e\n\u2212\u221e \u2212\u221e\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions in practice 116 (323)\n\u25b7 Tools like grep and also most programming languages support extended forms\nof regular expressions (Python/re, C++11/regex) with deviating syntax.\n\u25b7 E.g. usually \u201c +\u201d and \u201c \u2217\u201d have different semantics, escapes have to be used,\nabbreviations for frequently used character classes, (complement), . . .\n\u25b7 The typical use here is pattern-matching/searching; often the power-set\nconstruction is only applied \u201con-the-fly\u201d for the given input.\n\u25b7 Regular expressions allow to specify the desired behavior of a system from\nwhich then an implementation in form of a DFA can be \u201csynthesized\u201d.\n\u25b7 Typical webpage:\nmainpage (advertisement content advertisement)\u2217\n\u25b7 Input should be a valid number in decimal representation:\n(As we need 0, 1, +, \u2212 as alphabet symbols \u03b5, \u2205, | instead.)\n0 | (+ | \u2212 |\u03b5)0\u2217(1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9)(1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9)\u2217\n\u25b7 As we will see, we can compute \u201coptimal\u201d DFAs for a given regular\nexpressions.",
      "\u25b7 Input should be a valid number in decimal representation:\n(As we need 0, 1, +, \u2212 as alphabet symbols \u03b5, \u2205, | instead.)\n0 | (+ | \u2212 |\u03b5)0\u2217(1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9)(1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9)\u2217\n\u25b7 As we will see, we can compute \u201coptimal\u201d DFAs for a given regular\nexpressions.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Remark: \u03c9-regular expressions (*) 117 (324)\n\u25b7 Let L \u2286 \u03a3\u2217 be a language of finite words:\nL\u2217 is obtained by finite repetition of words of L\nL\u03c9 is obtained by infinite repetition of words of L assuming \u03b5 \u0338\u2208 L:\nL\u03c9 = {w(1)w(2)w(3) . . . w(i) . . .| for all i \u2208 N : w(i) \u2208 L \\ {\u03b5}} \u2286\u03a3\u03c9\n\u201c\u03c9-regular expressions = regular expressions with \u03c9-operator\u201d\n\u25b7 While regular expressions can be used to describe the behavior terminating\nprograms, \u03c9-regular expressions are used to specify the behavior of systems\nthat are supposed to run forever like processes, e.g.:\n\u25b7 L((a + b + c)\u2217(a + b)\u03c9): \u201cw \u2208 {a, b, c}\u03c9 must contain only finitely many cs\u201d\n\u25b7 L(((a + b)\u2217c)\u03c9): \u201cw \u2208 {a, b, c}\u03c9 must contain infinitely many cs\u201d\nAlso \u03c9-regular expressions can be compiled to finite automata but now an\ninfinite word w \u2208 \u03a3\u03c9 is accepted if a final state is visited infinitely often.\nThis is used both for the verification and synthesis of reactive systems (like\ncontrollers/robots/processes/circuits).",
      "Also \u03c9-regular expressions can be compiled to finite automata but now an\ninfinite word w \u2208 \u03a3\u03c9 is accepted if a final state is visited infinitely often.\nThis is used both for the verification and synthesis of reactive systems (like\ncontrollers/robots/processes/circuits).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "5 Regular languages\nRegular grammars and finite automata\nMoving between regular grammars and finite automata\nClosure properties\nRegular expressions\nMinimizing DFAs and Myhill-Nerode\nBasics of probabilistic finite automata (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Minimization of DFAs 119 (326)\n\u25b7 The automatic translation of an regular expression \u03c1 to a finite automaton\nyields in general an \u03b5-NFA M\u03c1.\n\u25b7 For the implementation as a program, we actually want the FA to be\ndeterministic, i.e. we need to apply the power-set construction to M\u03c1 to\nobtain a DFA M\u2032\n\u03c1.\n\u25b7 In general, M\u2032\n\u03c1 might contain\n\u25b7 states unreachable from the initial state.\n\u25b7 states that cannot reach any final state.\n\u25b7 states that actually \u201cdo the same\u201d.\n\u25b7 The first two kinds of useless states can be removed by treating the\nautomaton as finite graph (Q, E\u03b4) with\nE\u03b4 = {(q, r) | ((q, a), r) \u2208 \u03b4} (i.e. forget the input)\nThen restrict the automaton to the states q0E\u2217\n\u03b4 \u2229 E\u2217\n\u03b4 F.\n\u25b7 For the third kind, the Myhill-Nerode relation is important.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Equivalent states in a DFA 120 (327)\n\u25b7 Example: Consider the following DFA for L(a\u2217b\u2217):\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n\u25b7 A DFA is the \u201ctail-recursive implementation\u201d of a characteristic\nfunction/predicate, i.e. it can only return 1 (accept input) or 0 (reject input).\nEvery state corresponds to a recursive characteristic function that gets some\nword w \u2208 \u03a3\u2217 as input.\nTwo states/functions p, qare \u201cequivalent wrt. the DFA M\u201d iff they\nimplement the same characteristic function iff both states accept the same\nlanguage.\n! In the following it is important that the DFA is complete, i.e. if necessary add\na rejecting state (as in the example) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Equivalent states in a DFA 120 (328)\n\u25b7 Example: Consider the following DFA for L(a\u2217b\u2217):\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\nD Define the relation \u223cM on the states Q of a (complete) DFA M by\np \u223cM q if for all z \u2208 \u03a3\u2217 : pz \u2212 \u2192\u2217\nM p\u2032 \u2208 F iff qz \u2212 \u2192\u2217\nM q\u2032 \u2208 F\n(i.e. for every word w either both states reach a final state or none reaches a final state.)\nF \u223cM is an equivalence relation on Q by the properties of the biconditional.\n(|= (A \u2194 A), |= ((A \u2194 B) \u2192 (B \u2194 A)), |= (((A \u2194 B) \u2227 (B \u2194 C)) \u2192 (A \u2194 C)).)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Equivalent states in a DFA 120 (329)\n\u25b7 Example: Consider the following DFA for L(a\u2217b\u2217):\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n0start\n1 4\na\nb\nb\na\na, b\n\u25b7 As \u223cM -equvialent states implement the same characteristic function, it\nsuffices to keep one representative of each equivalence class (colors): all\ntransitions (calls) can be rerouted to the chosen representative.\nD The quotient of M wrt. \u223cM is then\n(Q/ \u223cM , \u03a3, {(([p]\u223cM , a), [q]\u223cM ) | pa \u2212 \u2192\u2217\nM q}, [q0]\u223cM , F/\u223cM )\nTrivially, |Q/ \u223cM | \u2264 |Q|.\n\u25b7 Closely related to \u223cM is the Myhill-Nerode relation:\nWhile \u223cM depends on a specific DFA M, the Myhill-Nerode relation only\ndepends on the language.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Equivalent states in a DFA 120 (330)\n\u25b7 Example: Consider the following DFA for L(a\u2217b\u2217):\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n0start\n1 4\na\nb\nb\na\na, b\n\u25b7 wlog assume that all states are reachable from the initial state q0.\nThen we can fix for every state q of M a word wq \u2208 \u03a3\u2217 s.t. q0wq \u2212 \u2192\u2217\nM q.\nWrt. these words, the definition of \u223cM\np \u223cM q if for all z \u2208 \u03a3\u2217 : pz \u2212 \u2192\u2217\nM p\u2032 \u2208 F iff qz \u2212 \u2192\u2217\nM q\u2032 \u2208 F\nbecomes\np \u223cM q if for all z \u2208 \u03a3\u2217 : wpz \u2208 L iff wqz \u2208 L\n(The actual choice of wq does not matter as for any z there is only one computation\nstarting at q.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Equivalent states in a DFA 120 (331)\n\u25b7 Example: Consider the following DFA for L(a\u2217b\u2217):\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n0start\n1 4\na\nb\nb\na\na, b\nD The Myhill-Nerode relation \u223cL on \u03a3\u2217 is defined by\nu \u223cL v if for all z \u2208 \u03a3\u2217 : uz \u2208 L iff vz \u2208 L\nIt only depends on L but not on a specific DFA.\nF By choice/definition of wp, wq: p \u223cM q iff wp \u223cL wq.\n\u25b7 As we will see: If L is regular, than |\u03a3\u2217/ \u223cL | = |Q/ \u223cM | \u2264 |Q| for any\ncomplete DFA M with L(M) = L.\n\u25b7 First how to decide \u201c p \u223cM q\u201d for a given DFA M in order to minimize M.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Minimizing DFAs 121 (332)\n\u25b7 Let M = (Q, \u03a3, \u03b4, q0, F) be a complete DFA with L := L(M).\nwlog all states of M are reachable from q0.\nBy definition:\np \u0338\u223cM q\niff there ex. z \u2208 \u03a3\u2217 s.t. (wlog) pz \u2212 \u2192\u2217\nM p\u2032 \u2208 F and qz \u2212 \u2192\u2217\nM q\u2032 \u0338\u2208 F\niff there ex. z \u2208 \u03a3\u2217 that is accepted by the automaton with\nstates the unordered pairs\n\u0000Q\n2\n\u0001\n= {{r, s} \u2286Q | r \u0338= s}\ntransitions {(({r, s}, a), {r\u2032, s\u2032}) | a \u2208 \u03a3, ra\u2212 \u2192M r\u2032, sa\u2212 \u2192M s\u2032, r\u2032 \u0338= s\u2032}\nfinal states {{r, s} |r \u2208 F, s\u0338\u2208 F}\nand initial state {p, q}.\n(Product construction of the symmetric difference of M with itself.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Minimizing DFAs 121 (333)\n\u25b7 Let M = (Q, \u03a3, \u03b4, q0, F) be a complete DFA with L := L(M).\nwlog all states of M are reachable from q0.\nL p \u0338\u223cM q can thus be decided by backwards reachability:\nS0 :=\nn\n{r, s} \u2208\n\u0000Q\n2\n\u0001\n| r \u2208 F, s\u0338\u2208 F\no\nSk+1 := Sk\n\u222a\nn\n{r, s} \u2208\n\u0000Q\n2\n\u0001\n\\ Sk | a \u2208 \u03a3, ra\u2212 \u2192M r\u2032, sa\u2212 \u2192M s\u2032, {r\u2032, s\u2032} \u2208Sk\no\nThen: p \u0338\u223cM q iff wp \u0338\u223cL wq iff {p, q} \u2208S|Q|2 .\n(If Sk = Sk+1, also Sk = S|Q|2 .)\nBy remembering any a \u2208 \u03a3 that led to the inclusion of {p, q} \u2208Sk+1, we can\nalso compute a separating word for p and q (resp. wp and wq).\n\u25b7 The quotient of M wrt. \u223cM is then\n(Q/ \u223cM , \u03a3, {(([p]\u223cM , a), [q]\u223cM ) | pa \u2212 \u2192\u2217\nM q}, [q0]\u223cM , F/\u223cM )\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Minimizing DFAs: example 122 (334)\nS0 :=\nn\n{p, q} \u2208\n\u0000Q\n2\n\u0001\n| p \u2208 F, q\u0338\u2208 F\no\nSk+1 := Sk\n\u222a\nn\n{p, q} \u2208\n\u0000Q\n2\n\u0001\n\\ Sk | a \u2208 \u03a3, pa\u2212 \u2192M p\u2032, qa\u2212 \u2192M q\u2032, {p\u2032, q\u2032} \u2208Sk\no\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n0 \u223c\n0\n1 \u223c\n1\n2 \u223c\n2\n3 \u223c\n3\n4 \u223c\n4\n\u25b7 We only need to consider unordered pairs\n\u0000Q\n2\n\u0001\n,\nbut usually it is simpler to include {q, q} into the table; trivially, q \u223cL q.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Minimizing DFAs: example 122 (335)\nS0 :=\nn\n{p, q} \u2208\n\u0000Q\n2\n\u0001\n| p \u2208 F, q\u0338\u2208 F\no\nSk+1 := Sk\n\u222a\nn\n{p, q} \u2208\n\u0000Q\n2\n\u0001\n\\ Sk | a \u2208 \u03a3, pa\u2212 \u2192M p\u2032, qa\u2212 \u2192M q\u2032, {p\u2032, q\u2032} \u2208Sk\no\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n0 \u223c\n0\n1 \u223c\n1\n2 \u223c\n2\n3 \u223c\n3\n4 \u223c\n4\n\u03b5, 0 \u03b5, 0 \u03b5, 0 \u03b5, 0\n\u25b7 Initially mark all unordered final-non-final-pairs by \u03b5\nas \u03b5 separates every final state from any non-final state.\n\u25b7 Sometimes it helps to also add a time stamp.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Minimizing DFAs: example 122 (336)\nS0 :=\nn\n{p, q} \u2208\n\u0000Q\n2\n\u0001\n| p \u2208 F, q\u0338\u2208 F\no\nSk+1 := Sk\n\u222a\nn\n{p, q} \u2208\n\u0000Q\n2\n\u0001\n\\ Sk | a \u2208 \u03a3, pa\u2212 \u2192M p\u2032, qa\u2212 \u2192M q\u2032, {p\u2032, q\u2032} \u2208Sk\no\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n0 \u223c\n0\n1 \u223c\n1\n2 \u223c\n2\n3 \u223c\n3\n4 \u223c\n4\n\u03b5, 0 \u03b5, 0 \u03b5, 0 \u03b5, 0\na, 1\na, 1\na, 1\na, 1\n\u25b7 for all newly marked pairs {p\u2032, q\u2032} check if they have an unmarked\na-predecessor {p, q} (i.e. pa \u2212 \u2192M p\u2032 and qa \u2212 \u2192M q\u2032) and mark it by a:\ne.g. mark {2, 3} by a as 2a \u2212 \u2192M 2 and 3a \u2212 \u2192M 4 and {2, 4} \u2208S0.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Minimizing DFAs: example 122 (337)\nS0 :=\nn\n{p, q} \u2208\n\u0000Q\n2\n\u0001\n| p \u2208 F, q\u0338\u2208 F\no\nSk+1 := Sk\n\u222a\nn\n{p, q} \u2208\n\u0000Q\n2\n\u0001\n\\ Sk | a \u2208 \u03a3, pa\u2212 \u2192M p\u2032, qa\u2212 \u2192M q\u2032, {p\u2032, q\u2032} \u2208Sk\no\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n0 \u223c\n0\n1 \u223c\n1\n2 \u223c\n2\n3 \u223c\n3\n4 \u223c\n4\n\u03b5, 0 \u03b5, 0 \u03b5, 0 \u03b5, 0\na, 1\na, 1\na, 1\na, 1\n\u223c\n\u223c\n\u25b7 Repeat until no new pair has been marked.\n\u25b7 This is here already the case s.t. we may collapse {0, 2} to 0 and {1, 3} to 1.\n(For later: {0, 2} corresponds to [\u03b5]\u223cL and {1, 3} to [b]\u223cL.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Minimizing DFAs: example 122 (338)\nS0 :=\nn\n{p, q} \u2208\n\u0000Q\n2\n\u0001\n| p \u2208 F, q\u0338\u2208 F\no\nSk+1 := Sk\n\u222a\nn\n{p, q} \u2208\n\u0000Q\n2\n\u0001\n\\ Sk | a \u2208 \u03a3, pa\u2212 \u2192M p\u2032, qa\u2212 \u2192M q\u2032, {p\u2032, q\u2032} \u2208Sk\no\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n0 \u223c\n0\n1 \u223c\n1\n2 \u223c\n2\n3 \u223c\n3\n4 \u223c\n4\n\u03b5, 0 \u03b5, 0 \u03b5, 0 \u03b5, 0\na, 1\na, 1\na, 1\na, 1\n\u223c\n\u223c\n02start 13 4\na\nb\nb\na\na, b\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode relation 123 (339)\nD For a language L \u2286 \u03a3\u2217 let \u223cL denote the binary relation\n\u223cL:= {(w, w\u2032) \u2208 \u03a3\u2217 \u00d7 \u03a3\u2217 | for all z \u2208 \u03a3\u2217 : wz \u2208 L iff w\u2032z \u2208 L}\n! If \u201cw \u223cL w\u2032\u201d, any FA accepting L has to treat wz and w\u2032z the same.\nF \u223cL is an equivalence relation again by the properties of the biconditional\n(\u201ciff\u201d, \u201c\u2194\u201d).\nD As \u223cL is an equivalence relation:\n\u25b7 [w]\u223cL := {w\u2032 \u2208 \u03a3\u2217 | w \u223cL w\u2032} denotes the (equivalence) class of w \u2208 \u03a3\u2217.\n\u25b7 \u03a3\u2217/ \u223cL:= {[w]\u223cL | w \u2208 \u03a3\u2217} is the quotient/partition.\n\u25b7 z \u2208 \u03a3\u2217 separates w, w\u2032 \u2208 \u03a3\u2217 wrt. \u223cL if\n(i) wz \u2208 L and w\u2032z \u0338\u2208 L or (ii) wz \u0338\u2208 L and w\u2032z \u2208 L\n(i.e. z yields a counter-example for w \u223cL w\u2032 resp. z is a witness of w \u0338\u223cL w\u2032.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode relation 123 (340)\nD For a language L \u2286 \u03a3\u2217 let \u223cL denote the binary relation\n\u223cL:= {(w, w\u2032) \u2208 \u03a3\u2217 \u00d7 \u03a3\u2217 | for all z \u2208 \u03a3\u2217 : wz \u2208 L iff w\u2032z \u2208 L}\n\u25b7 E.g. for L = L(a\u2217b\u2217) with \u03a3 = {a, b}:\n[\u03b5]\u223cL = L(a\u2217) [ b]\u223cL = L(a\u2217bb\u2217) [ ba]\u223cL = L(a\u2217b\u2217ba(a + b)\u2217)\nThis is a finite partition of \u03a3\u2217 and e.g.\n\u25b7 z = a separates \u03b5 and b: \u03b5 a\u2208 L but b a\u0338\u2208 L.\n\u25b7 z = \u03b5 separates \u03b5 and ba: \u03b5 \u03b5\u2208 L but ba \u03b5\u0338\u2208 L.\n\u25b7 z = \u03b5 separates b and ba: b \u03b5\u2208 L but ba \u03b5\u0338\u2208 L.\nwhile for all z \u2208 \u03a3\u2217:\n\u25b7 ak \u223cL \u03b5 as: ak z \u2208 L iff z = albm iff \u03b5z \u2208 L\n\u25b7 akblb \u223cL b as: akblb z\u2208 L iff z = bm iff bz \u2208 L\n\u25b7 akblbax \u223c ba as: akbax z\u0338\u2208 L\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode relation 123 (341)\nD For a language L \u2286 \u03a3\u2217 let \u223cL denote the binary relation\n\u223cL:= {(w, w\u2032) \u2208 \u03a3\u2217 \u00d7 \u03a3\u2217 | for all z \u2208 \u03a3\u2217 : wz \u2208 L iff w\u2032z \u2208 L}\n\u25b7 E.g. for L = {akbk | k \u2208 N 0} with \u03a3 = {a, b}:\n\u0002\nak\u0003\n\u223cL\n= {ak} (k \u2208 N 0)\u0002\nakab\n\u0003\n\u223cL\n= {ak+l+1bl+1 | l \u2208 N 0} (k \u2208 N 0)\n[b]\u223cL\n= {akbkx | k \u2208 N 0, x\u2208 \u03a3+}\nThis is an infinite partition of \u03a3\u2217:\n\u25b7 z = bk separates akab from b and any aiab with i \u0338= k.\n\u25b7 z = abk+1 separates ak from all other words\nwhile for all z \u2208 \u03a3\u2217\n\u25b7 ak+l+1bl+1 \u223cL ak+1b as: ak+l+1bl+1 z \u2208 L iff z = bk iff ak+1b z\u2208 L.\n\u25b7 akbkx \u223cL b (with x \u0338= \u03b5) as: akbkx z\u0338\u2208 L\n(Visualize akbl as going k times \u201cup and right\u201d and l times \u201cdown and right\u201d.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode relation 123 (342)\nD For a language L \u2286 \u03a3\u2217 let \u223cL denote the binary relation\n\u223cL:= {(w, w\u2032) \u2208 \u03a3\u2217 \u00d7 \u03a3\u2217 | for all z \u2208 \u03a3\u2217 : wz \u2208 L iff w\u2032z \u2208 L}\nF By definition of \u223cL:\nIf w \u223cL w\u2032, then also wa \u223cL w\u2032a for every a \u2208 \u03a3.\n\u25b7 In other words: if [w]\u223cL = [w\u2032]\u223cL, then also [wa]\u223cL = [w\u2032a]\u223cL for any a \u2208 \u03a3.\n! \u03a3\u2217/ \u223cL can be visualized as edge-labeled directed graph\n(\u03a3\u2217/ \u223cL, {([w]\u223cL, a,[wa]\u223cL) | a \u2208 \u03a3})\ni.e. we take the equivalence classes as nodes, and draw an edge labeled by a\nfrom [w]\u223cL to [wa]\u223cL.\n(This is the quotient of the infinite tree (\u03a3\u2217, R\u03a3) wrt. \u223cL: the equivalence classes wrt. \u223cL\n\u201ccolor\u201d the nodes of \u03a3\u2217, and when taking the quotient the tree is folded into an\nautomaton.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode relation 123 (343)\nD For a language L \u2286 \u03a3\u2217 let \u223cL denote the binary relation\n\u223cL:= {(w, w\u2032) \u2208 \u03a3\u2217 \u00d7 \u03a3\u2217 | for all z \u2208 \u03a3\u2217 : wz \u2208 L iff w\u2032z \u2208 L}\n\u25b7 E.g. for L = L(a\u2217b\u2217):\n\u03b5\na b\naa ab ba bb\naaa aab aba abb baa bab bba bbb\n\u03b5start b ba\na\nb\nb\na\na, b\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode relation 123 (344)\nD For a language L \u2286 \u03a3\u2217 let \u223cL denote the binary relation\n\u223cL:= {(w, w\u2032) \u2208 \u03a3\u2217 \u00d7 \u03a3\u2217 | for all z \u2208 \u03a3\u2217 : wz \u2208 L iff w\u2032z \u2208 L}\n\u25b7 E.g. for L = {akbk | k \u2208 N 0}: (red \u201csink\u201d state not shown below)\n\u03b5\na b\naa ab ba bb\naaa aab aba abb baa bab bba bbb\n1\n2\n3 1\n0\n1\n0start 1\n1\n2\n1\n3\n2\n. . .\n. . .\na a\nb b\na\nb\nb\na\nb\nb\nb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode vs. DFA 124 (345)\nL If |\u03a3\u2217/ \u223cL | < \u221e, the quotient automaton wrt. \u223cL\nML = (\u03a3\u2217/ \u223cL, \u03a3, {(([w]\u223cL, a), [wa]\u223cL) | a \u2208 \u03a3}, [\u03b5]\u223cL, {[w]\u223cL | w \u2208 L})\nis a complete DFA with L = L(ML).\n\u25b7 By definition/construction:\nevery equivalence class [w]\u223cL has exactly one a-successor [wa]\u223cL, and\nthe maximal run on input w \u2208 \u03a3\u2217 ends in [w]\u223cL.\nThus: ML accepts w iff w \u2208 L.\n\u25b7 If L is regular, then the equivalence class [u]\u223cL is the regular language of\nwords for which ML ends up in [u]\u223cL, i.e. the language accepted by\n(\u03a3\u2217/ \u223cL, \u03a3, {(([w]\u223cL, a), [wa]\u223cL) | a \u2208 \u03a3}, [\u03b5]\u223cL, {[u]\u223cL})\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode vs. DFA 124 (346)\nL If |\u03a3\u2217/ \u223cL | < \u221e, the quotient automaton wrt. \u223cL\nML = (\u03a3\u2217/ \u223cL, \u03a3, {(([w]\u223cL, a), [wa]\u223cL) | a \u2208 \u03a3}, [\u03b5]\u223cL, {[w]\u223cL | w \u2208 L})\nis a complete DFA with L = L(ML).\nC Let M = (Q, \u03a3, \u03b4, q0, F) be a complete DFA M with L = L(M). Then\n(Q/ \u223cM , \u03a3, {(([p]\u223cM , a), [q]\u223cM ) | pa \u2212 \u2192\u2217\nM q}, [q0]\u223cM , F/\u223cM )\nis isomorphic with ML. In particular |\u03a3\u2217/ \u223cL | \u2264 |Q| < \u221e.\n\u25b7 Fix for any state q a word wq so that q0wq \u2212 \u2192\u2217\nM q.\nThis defines an isomorphism Q/ \u223cM \u2192 \u03a3\u2217/ \u223cL, [q]\u223cM 7\u2192 [wq]\u223cL.\ninjective: wp \u223cL wq iff p \u223cM q.\nsurjective: every w \u2208 \u03a3\u2217 defines qw \u2208 Q by q0w \u2212 \u2192\u2217\nM qw where w \u223cL wqw .\nhomomorphism: if pa \u2212 \u2192M q, then wpa \u223cL wq.\nC L := {akbk | k \u2208 N 0} is not regular as |\u03a3\u2217/ \u223cL | = \u221e.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode vs. DFA: Examples 125 (347)\n\u03b5\na b\naa ab ba bb\naaa aab aba abb baa bab bba bbb\n0\n2\n2\n2\n1\n1\n1\n3\n3 3\n4\n4 4 44\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n\u03b5start b ba\na\nb a\nb\na, b\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode vs. NFA 126 (348)\n\u25b7 NFAs can be exponentially more succinct than the minimal DFA:\n\u25b7 Fix any n \u2208 N and consider L = L((a + b)\u2217a(a + b)n).\n\u25b7 An NFA with n + 1 states can simply \u201cguess\u201d when the last n + 1 letters will\nbe read. For simplicity consider only n = 1:\n0start 1 2\na, b\na a, b\n\u25b7 The power-set construction yields the DFA M (n = 1):\nq100start q110\nq101\nq111\nb\na a\nbb a b\na\nwhich is already minimal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode vs. NFA 126 (349)\n\u25b7 NFAs can be exponentially more succinct than the minimal DFA:\n\u25b7 Fix any n \u2208 N and consider L = L((a + b)\u2217a(a + b)n).\n\u25b7 The power-set construction yields the DFA M (n = 1):\nq100start q110\nq101\nq111\nb\na a\nbb a b\na\nwhich is already minimal.\nThe states thus represent the equivalence classes of \u223cL\nq100 \u02c6 =[bb]\u223cL = L(1 + b + (a + b)\u2217bb) q100 \u02c6 =[ba]\u223cL = L(a + (a + b)\u2217ba)\nq101 \u02c6 =[ab]\u223cL = L((a + b)\u2217ab) q111 \u02c6 =[aa]\u223cL = L((a + b)\u2217aa)\ni.e. the states correspond to a \u201csliding window\u201d of length 2:\n[x1x2]\u223cLa \u2212 \u2192M [x2a]\u223cL\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode vs. NFA 126 (350)\n\u25b7 NFAs can be exponentially more succinct than the minimal DFA:\n\u25b7 Fix any n \u2208 N and consider L = L((a + b)\u2217a(a + b)n).\n\u25b7 The power-set construction yields the DFA M (n = 1):\nbbstart ba\nab\naa\nb\na a\nbb a b\na\nwhich is already minimal.\nThe states thus represent the equivalence classes of \u223cL\nq100 \u02c6 =[bb]\u223cL = L(1 + b + (a + b)\u2217bb) q100 \u02c6 =[ba]\u223cL = L(a + (a + b)\u2217ba)\nq101 \u02c6 =[ab]\u223cL = L((a + b)\u2217ab) q111 \u02c6 =[aa]\u223cL = L((a + b)\u2217aa)\ni.e. the states correspond to a \u201csliding window\u201d of length 2:\n[x1x2]\u223cLa \u2212 \u2192M [x2a]\u223cL\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode vs. NFA 126 (351)\n\u25b7 NFAs can be exponentially more succinct than the minimal DFA:\n\u25b7 Fix any n \u2208 N and consider L = L((a + b)\u2217a(a + b)n).\n\u25b7 The power-set construction yields the DFA M (n = 1):\nbbstart ba\nab\naa\nb\na a\nbb a b\na\nwhich is already minimal.\n\u25b7 In general |\u03a3\u2217/ \u223cL | = 2n+1:\n\u25b7 Fix any two words x = x1 . . . xn+1 and y = y1 . . . yn+1 of length n + 1.\nAssume x1 . . . xk = y1 . . . yk but wlog xk+1 = a and yk+1 = b.\nAppending bk pushes out the k first letters s.t. xbk \u2208 L and ybk \u0338\u2208 L.\nAs a sliding window of size n + 1 suffices, also |\u03a3\u2217/ \u223cL | = 2n+1.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Using Myhill-Nerode 127 (352)\n\u25b7 In order to show that L \u2286 \u03a3\u2217 is not regular:\nfind infinitely many pairwise inequivalent words (w(k))k\u2208N 0\n(i.e. w(i) \u0338\u223cL w(j) if i \u0338= j).\n\u25b7 E.g. whenever we need to store some unbounded counter (or use in general\nunbounded memory) a natural first attempt is to pick for each counter value k\nsome corresponding word w(k), and then try to show their inequivalence.\n\u25b7 Typical example: L = {akbk | k \u2208 N 0} with w(k) = akbk.\n\u25b7 In order to compute \u03a3\u2217/ \u223cL for some given regular L \u2286 \u03a3\u2217:\nDetermine some FA for L, if necessary determinize it, then minimize it.\n\u25b7 The states of a minimal DFA correspond to the equivalence classes:\nSimply determine the regular languages that lead from the initial state to a\nspecific state of the given minimal DFA.\n\u25b7 See the example for L(a\u2217b\u2217).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode, minimization: remarks 128 (353)\n\u25b7 Hopcroft\u2019s algorithm and variants:\nInstead of building the |Q|2 table, we can start with the initial partition\nP0 = {F, Q\\ F} where all accepting resp. rejecting states are assumed to be\nequivalent, and then iteratively split every class C of Pk whose members can\nbe separated by a letter (i.e. the a-successors of at least two members of C are\ncontained in different classes of Pk); can be implemented to run in\nO(|\u03a3| \u00b7 |Q|log |Q|).\n\u25b7 Brzozowski\u2019s algorithm:\nLet MR\nD denote the DFA obtained by first reversing the transitions of M and\napplying the power-set construction; then (MR\nD)R\nD is a minimal DFA for\nL(M).\n\u25b7 L\u2217 algorithm/minimal adequate teacher by Dana Angluin:\nConsiders the setting where a DFA for an unkown L should be learned by\nmeans of examples (w \u2208 L) and counterexamples (w \u0338\u2208 L): it approximates \u223cL\nby learning both representatives and separating words from counterexamples.",
      "L(M).\n\u25b7 L\u2217 algorithm/minimal adequate teacher by Dana Angluin:\nConsiders the setting where a DFA for an unkown L should be learned by\nmeans of examples (w \u2208 L) and counterexamples (w \u0338\u2208 L): it approximates \u223cL\nby learning both representatives and separating words from counterexamples.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Learning DFAs (*) 129 (354)\n\u25b7 The \u201clearner\u201d is allowed to two kind of queries: \u201c w\n?\n\u2208 L\u201d and \u201c L(M)\n?\n= L\u201d.\nThe \u201cteacher\u201d answers correctly; if L(M) \u0338= L, returns a counterexample.\n\u25b7 In every iteration, we (the learner) have a (incomplete) system of representatives\n(states) Q wrt. \u223cL and a set T of separating (test) words. We\nunderapproximate \u223cL by means of (using the teacher to decide uz\n?\n\u2208 L)\nu \u223cT v if for all z \u2208 T : uz \u2208 L iff vz\n\u25b7 The (unknown) language is L = {w \u2208 {a, b}\u2217 : |w|b \u22614 3} (cf. Worrell).\n\u25b7 Initially: Q = {ew} and T = {\u03b5}. Using the teacher we learn \u03b5 \u0338\u2208 L (i.e.\nrejecting) and \u03b5 a\u223cT \u03b5 \u223cT \u03b5 b. This yields the initial guess:\n\u03b5start\na, b\n\u25b7 Assume the \u201cteacher\u201d returns the counterexample bbb.\nWe use the run on bbb to find a mistake: as L \u220b \u03b5 bbb\u2212 \u2192M \u03b5 bb\u0338\u2208 L we learn\nthat bb separates \u03b5 from b and accordingly update Q := {\u03b5, b} and",
      "rejecting) and \u03b5 a\u223cT \u03b5 \u223cT \u03b5 b. This yields the initial guess:\n\u03b5start\na, b\n\u25b7 Assume the \u201cteacher\u201d returns the counterexample bbb.\nWe use the run on bbb to find a mistake: as L \u220b \u03b5 bbb\u2212 \u2192M \u03b5 bb\u0338\u2208 L we learn\nthat bb separates \u03b5 from b and accordingly update Q := {\u03b5, b} and\nT := {\u03b5, bb}. As b \u0338\u2208 L, it has to be rejecting, too.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Learning DFAs (*) 129 (355)\n\u25b7 The \u201clearner\u201d is allowed to two kind of queries: \u201c w\n?\n\u2208 L\u201d and \u201c L(M)\n?\n= L\u201d.\nThe \u201cteacher\u201d answers correctly; if L(M) \u0338= L, returns a counterexample.\n\u25b7 In every iteration, we (the learner) have a (incomplete) system of representatives\n(states) Q wrt. \u223cL and a set T of separating (test) words. We\nunderapproximate \u223cL by means of (using the teacher to decide uz\n?\n\u2208 L)\nu \u223cT v if for all z \u2208 T : uz \u2208 L iff vz\n\u25b7 Wrt. Q = {\u03b5, b} and T = {\u03b5, bb} we check, if the possible transitions are\nclosed: As \u03b5 a\u223cT \u03b5 \u223cT b band b a\u223cT b \u223cT \u03b5 b, we obtain:\n\u03b5start b\na a\nb\nb\n\u25b7 Assume the \u201cteacher\u201d returns again the counterexample bbb.\nWe again use the run L \u220b \u03b5 bbb\u2212 \u2192M b bb\u2212 \u2192M \u03b5 b\u0338\u2208 L to learn that b separates\n\u03b5 from bb and hence update Q := {\u03b5, b, bb} and T = {\u03b5, b, bb}. Again, bb has\nto be rejecting as bb \u0338\u2208 L.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Learning DFAs (*) 129 (356)\n\u25b7 The \u201clearner\u201d is allowed to two kind of queries: \u201c w\n?\n\u2208 L\u201d and \u201c L(M)\n?\n= L\u201d.\nThe \u201cteacher\u201d answers correctly; if L(M) \u0338= L, returns a counterexample.\n\u25b7 In every iteration, we (the learner) have a (incomplete) system of representatives\n(states) Q wrt. \u223cL and a set T of separating (test) words. We\nunderapproximate \u223cL by means of (using the teacher to decide uz\n?\n\u2208 L)\nu \u223cT v if for all z \u2208 T : uz \u2208 L iff vz\n\u25b7 We again check, if the transition are closed wrt. Q = {\u03b5, b, bb} and\nT = {\u03b5, b, bb}.\nThe new separating word b yields that bb b\u0338\u223cT \u03b5, b, bb, i.e. the b-transition\nleaving bb requires an additional state bbb (accepting as bbb \u2208 L).\nThis leads to the final guess:\n\u03b5start b bb bbb\na a a a\nb b b\nb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Learning DFAs (*) 129 (357)\n\u25b7 The \u201clearner\u201d is allowed to two kind of queries: \u201c w\n?\n\u2208 L\u201d and \u201c L(M)\n?\n= L\u201d.\nThe \u201cteacher\u201d answers correctly; if L(M) \u0338= L, returns a counterexample.\n\u25b7 In every iteration, we (the learner) have a (incomplete) system of representatives\n(states) Q wrt. \u223cL and a set T of separating (test) words. We\nunderapproximate \u223cL by means of (using the teacher to decide uz\n?\n\u2208 L)\nu \u223cT v if for all z \u2208 T : uz \u2208 L iff vz\n\u25b7 Hence, we can also learn in this way an NDD representing a linear constraint\n\u27e8a, x\u27e92 \u2264 c (a, x\u2208 Z n, c\u2208 Z ) or more generally a constraint in Presburger\narithmetic.\n\u25b7 Angluin discusses in the original article how this approach can be extended to\na teacher that gives the correct answer to L(M)\n?\n= L only with sufficiently\nhigh probability (cf. probably approximately correct learning) . This can be used\ne.g. to learn a model of an unknown system or environment.\n\u25b7 E.g. Weiss et al. use the L\u2217 algorithm to approximate/compress recurrent",
      "a teacher that gives the correct answer to L(M)\n?\n= L only with sufficiently\nhigh probability (cf. probably approximately correct learning) . This can be used\ne.g. to learn a model of an unknown system or environment.\n\u25b7 E.g. Weiss et al. use the L\u2217 algorithm to approximate/compress recurrent\nneural networks (RNNs). (In principle, an RNN uses R n as state space and input\nalphabet, but in practice only finite precision is used, see e.g. here. Still RNNs have a\nhuge state space with a hard to understand transition relation.)No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "5 Regular languages\nRegular grammars and finite automata\nMoving between regular grammars and finite automata\nClosure properties\nRegular expressions\nMinimizing DFAs and Myhill-Nerode\nBasics of probabilistic finite automata (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Probabilistic finite automata 131 (359)\nD Bra-ket notation: With R n = R n\u00d71, let |i\u27e9 := ei \u2208 R n denote the i-th\ncanonical unit column vector, and \u27e8i| := e\u22a4\ni its transposed row vector.\nD Probabilistic finite automaton (PFA): M = (\u03a3, {Pa \u2208 R n\u00d7n | a \u2208 \u03a3}, F)\n\u25b7 wlog [n] = {1, 2, . . . , n} is the set of states with 1 the initial state.\n\u25b7 \u03a3: the finite input alphabet, e.g. \u03a3 = {0, 1}\n\u25b7 F: the final states with F \u2286 [n]\n\u25b7 Pa: a row/right stochastic n \u00d7 n-matrix (usually even rational), i.e.:\n\u27e8q|Pa|q\u2032\u27e9 \u22650\nX\nq\u2032\u2208[n]\n\u27e8q|Pa|q\u2032\u27e9 = 1\n\u25b7 For a word w = uv \u2208 \u03a3\u2217 set (s.t. w 7\u2192 Pw is a monoid homomorphism) :\nP\u03b5 := Idn Puv := PuPv\nThen its acceptance probability is P\nqf \u2208F \u27e81|Pw|qf \u27e9.\n\u25b7 For \u03b7 \u2208 [0, 1) the \u03b7-language accepted by M is\nL(M, \u03b7) := {w \u2208 \u03a3\u2217 |\nX\nqf \u2208F\n\u27e81|Pw|qf \u27e9 > \u03b7}\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Probabilistic finite automata: basic properties 132 (360)\nP0 :=\n\u00121 0\n1\n2\n1\n2\n\u0013\nP1 :=\n\u00121\n2\n1\n2\n0 1\n\u0013\n1start 2\n0: 1, 1: 12 1: 1, 0: 12\n1: 12\n0: 12\n\u25b7 PFAs were introduced by Rabin (cf. here).\n! PFA: letter determines prob.; Markov model: prob. determines letter.\n\u25b7 For a given \u03b7 \u2208 [0, 1) above PFA accepts\nL(M, \u03b7) = {a1 . . . al \u2208 {0, 1}\u2217 | al2\u22121 + al\u221212\u22122 + . . .+ a12\u2212l > \u03b7}\nFix any (computable) enumeration w(1), w(2), . . .of {0, 1}\u2217 and set\n\u03b7 = 0.w(1)w(2) . . .; then L = L(M, \u03b7) will be non-regular (Myhill-Nerode).\n\u03b7 itself is also not regular/rational; if the enumeration is not computable,\nthen L(M, \u03b7) is not enumerable/semidecidable/recognizable.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Probabilistic finite automata: basic properties 132 (361)\nP0 :=\n\u00121 0\n1\n2\n1\n2\n\u0013\nP1 :=\n\u00121\n2\n1\n2\n0 1\n\u0013\n1start 2\n0: 1, 1: 12 1: 1, 0: 12\n1: 12\n0: 12\n\u25b7 Wrt. \u03b7 = 0 every PFA M is just an NFA:\nIn above example L(M, 0) = L(0\u22171(0 + 1)\u2217)\n\u25b7 Every DFA M is also an PFA s.t. L(M, \u03b7) = L for all \u03b7 \u2208 [0, 1):\nFor every w \u2208 L(M) (w \u0338\u2208 L(M)) its acceptance probability is 1 (0).\n\u25b7 PFAs cannot accept certain (linear) context-free languages (see e.g. here).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Probabilistic finite automata: basic properties 132 (362)\nP0 :=\n\u00121 0\n1\n2\n1\n2\n\u0013\nP1 :=\n\u00121\n2\n1\n2\n0 1\n\u0013\n1start 2\n0: 1, 1: 12 1: 1, 0: 12\n1: 12\n0: 12\n\u25b7 For practical use, we actually want some error bound \u03b3 >0 s.t.\n\u25b7 every word w \u0338\u2208 L has an acceptance prob. of at most \u2264 1/2 \u2212 \u03b3,\n\u25b7 every word w \u2208 L has an acceptance prob. of at least \u2265 1/2 + \u03b3.\n\u03b7 is called an isolated cut-point of a given PFA M if there exists a \u03b3 >0 s.t.\nfor all w \u2208 \u03a3\u2217 :\n\f\f\f\f\f\f\nX\nqf \u2208F\n\u27e81|Pw|qf \u27e9 \u2212\u03b7\n\f\f\f\f\f\f\n\u2265 \u03b3\nIf \u03b7 is an isolated cut-point of M, then L(M, \u03b7) is regular (Rabin).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Probabilistic finite automata: basic properties 132 (363)\nP0 :=\n\u00121 0\n1\n2\n1\n2\n\u0013\nP1 :=\n\u00121\n2\n1\n2\n0 1\n\u0013\n1start 2\n0: 1, 1: 12 1: 1, 0: 12\n1: 12\n0: 12\n\u25b7 Similar to NFAs, also PFAs with isolated cut-points can be more succinct\nthen the minimal DFA which was also shown by Rabin by adapting above\nPFA to the ternary representation of the Cantor set:\n\u25b7 Then \u27e81|Pa1...al|2\u27e9 = al3\u22121 + . . .+ a13\u2212l for a1 . . . al \u2208 {0, 2}\u2217.\n\u25b7 For \u03b7 = 2 \u00b7 3\u22121 + . . .+ 2 \u00b7 3\u2212n + 1 \u00b7 3\u2212(n+1) + 1 \u00b7 3\u2212(n+2):\nL(M, \u03b7) = L((0 + 2)\u22172n+1)\nwhich can only be accepted by a DFA with at least n + 2 states.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Probabilistic finite automata: basic properties 132 (364)\nP0 :=\n\u00121 0\n2\n3\n1\n3\n\u0013\nP2 :=\n\u00121\n3\n2\n3\n0 1\n\u0013\n1start 2\n0: 1, 2: 13 2: 1, 0: 13\n2: 23\n0: 23\n\u25b7 Similar to NFAs, also PFAs with isolated cut-points can be more succinct\nthen the minimal DFA which was also shown by Rabin by adapting above\nPFA to the ternary representation of the Cantor set:\n\u25b7 Then \u27e81|Pa1...al|2\u27e9 = al3\u22121 + . . .+ a13\u2212l for a1 . . . al \u2208 {0, 2}\u2217.\n\u25b7 For \u03b7 = 2 \u00b7 3\u22121 + . . .+ 2 \u00b7 3\u2212n + 1 \u00b7 3\u2212(n+1) + 1 \u00b7 3\u2212(n+2):\nL(M, \u03b7) = L((0 + 2)\u22172n+1)\nwhich can only be accepted by a DFA with at least n + 2 states.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Probabilistic finite automata: basic properties 132 (365)\nP0 :=\n\u00121 0\n2\n3\n1\n3\n\u0013\nP2 :=\n\u00121\n3\n2\n3\n0 1\n\u0013\n1start 2\n0: 1, 2: 13 2: 1, 0: 13\n2: 23\n0: 23\n\u25b7 In contrast to FAs the (strict) emptyness problem is undecidable for PFAs:\nGiven an PFA M and a threshold \u03b7 \u2208 (0, 1) decide L(M, \u03b7) = \u2205.\nas a PCP instance can be encoded into a PFA (see e.g. here).\n\u25b7 For more information see e.g. \u201cintroduction to probabilisitc automata\u201d by Paz\n\u25b7 Conceptually related are (measure once) quantum finite automaton.\nPhysics require that transition matrices are now unitary.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "6 Context-free languages\nContext-free grammars: basic properties\nPush-down automata\nDerivation trees\nPumping lemma and closure properties\nAlgebraic equations and EBNF (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-free languages and grammars (reminder) 135 (368)\nD A grammar G = (V, \u03a3, P, S) is context-free if P \u2286 V \u00d7 (\u03a3 \u222a V )\u2217.\n\u25b7 Recall: the Chomsky hierarchy actually requires P \u2286 V \u00d7 (\u03a3 \u222a V )+, but\n\u03b5-rules only add \u03b5 to L(G) and can be removed.\nL \u2286 \u03a3\u2217 is context-free if L = L(G) for some context-free grammar G.\n\u25b7 G is in Chomsky normal form if P \u2286 V \u00d7 (\u03a3 \u222a V V).\n\u25b7 G is in Greibach normal form if P \u2286 V \u00d7 \u03a3V \u2217.\n! For both normal forms, the usual convention is that S \u2212 \u2192G \u03b5 is allowed if S\ndoes not occur on the right-hand side of any rule.\nA derivation\nS \u2212 \u2192G \u03b11 \u2212 \u2192G \u03b12 \u2212 \u2192G . . .\nis leftmost if in every step the leftmost variable X1 of the current sentential\nform \u03b1i = w0X1w1 . . . Xlwl (wi \u2208 \u03a3\u2217, Xi \u2208 V ) is rewritten:\n\u25b7 E.g. consider S \u2212 \u2192G \u03b5 | aSb | SS\nS \u2212 \u2192G SS \u2212 \u2192G SaSb \u2212 \u2192G aSbaSb \u2212 \u2192G aSbaaSbb \u2212 \u2192G aSbaabb \u2212 \u2192G abaabb\nS \u2212 \u2192G SS \u2212 \u2192G aSbS \u2212 \u2192G abS \u2212 \u2192G abaSb \u2212 \u2192G abaaSbb \u2212 \u2192G abaabb",
      "form \u03b1i = w0X1w1 . . . Xlwl (wi \u2208 \u03a3\u2217, Xi \u2208 V ) is rewritten:\n\u25b7 E.g. consider S \u2212 \u2192G \u03b5 | aSb | SS\nS \u2212 \u2192G SS \u2212 \u2192G SaSb \u2212 \u2192G aSbaSb \u2212 \u2192G aSbaaSbb \u2212 \u2192G aSbaabb \u2212 \u2192G abaabb\nS \u2212 \u2192G SS \u2212 \u2192G aSbS \u2212 \u2192G abS \u2212 \u2192G abaSb \u2212 \u2192G abaaSbb \u2212 \u2192G abaabb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A linear context-free grammar for {anbn | n \u2208 N } 136(369)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G aSb | ab\nS \u2212 \u2192G aSb\n\u2212 \u2192G aaSbb\n\u2212 \u2192G aaaSbbb\n\u2212 \u2192G aaaabbbb\nD A context-free grammar is linear if on the right-hand side of every its rules at\nmost one nonterminal occurs.\n\u25b7 Regular grammars \u201c=\u201d right-linear grammar.\n\u25b7 Every right-linear grammar is equivalent to a left-linear grammar and v.v.\n\u25b7 Recall that {anbn | n \u2208 N 0} is not regular.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A linear context-free grammar for {anbn | n \u2208 N } 136(370)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G aSb | ab\nS \u2212 \u2192G aSb\n\u2212 \u2192G aaSbb\n\u2212 \u2192G aaaSbbb\n\u2212 \u2192G aaaabbbb\nD A context-free grammar is linear if on the right-hand side of every its rules at\nmost one nonterminal occurs.\n\u25b7 Regular grammars \u201c=\u201d right-linear grammar.\n\u25b7 Every right-linear grammar is equivalent to a left-linear grammar and v.v.\n\u25b7 Recall that {anbn | n \u2208 N 0} is not regular.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A linear context-free grammar for {anbn | n \u2208 N } 136(371)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G aSb | ab\nS \u2212 \u2192G aSb\n\u2212 \u2192G aaSbb\n\u2212 \u2192G aaaSbbb\n\u2212 \u2192G aaaabbbb\nD A context-free grammar is linear if on the right-hand side of every its rules at\nmost one nonterminal occurs.\n\u25b7 Regular grammars \u201c=\u201d right-linear grammar.\n\u25b7 Every right-linear grammar is equivalent to a left-linear grammar and v.v.\n\u25b7 Recall that {anbn | n \u2208 N 0} is not regular.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A linear context-free grammar for {anbn | n \u2208 N } 136(372)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G aSb | ab\nS \u2212 \u2192G aSb\n\u2212 \u2192G aaSbb\n\u2212 \u2192G aaaSbbb\n\u2212 \u2192G aaaabbbb\nD A context-free grammar is linear if on the right-hand side of every its rules at\nmost one nonterminal occurs.\n\u25b7 Regular grammars \u201c=\u201d right-linear grammar.\n\u25b7 Every right-linear grammar is equivalent to a left-linear grammar and v.v.\n\u25b7 Recall that {anbn | n \u2208 N 0} is not regular.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A linear context-free grammar for palindromes 137 (373)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G aSa | bSb | \u03b5 | a | b\nS \u2212 \u2192G aSa\n\u2212 \u2192G abSba\n\u2212 \u2192G abbSbba\n\u2212 \u2192G abbaabba\nD wR := al . . . a1 for w = a1 . . . al \u2208 \u03a3\u2217.\nD A word is a palindrome if w = wR.\n\u25b7 {w \u2208 {a, b}\u2217 | w = wR} is also not regular,\ne.g. [ab1ab2ab3a . . . abka]\u223cL have to be distinct equivalence classes for k \u2208 N .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A linear context-free grammar for palindromes 137 (374)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G aSa | bSb | \u03b5 | a | b\nS \u2212 \u2192G aSa\n\u2212 \u2192G abSba\n\u2212 \u2192G abbSbba\n\u2212 \u2192G abbaabba\nD wR := al . . . a1 for w = a1 . . . al \u2208 \u03a3\u2217.\nD A word is a palindrome if w = wR.\n\u25b7 {w \u2208 {a, b}\u2217 | w = wR} is also not regular,\ne.g. [ab1ab2ab3a . . . abka]\u223cL have to be distinct equivalence classes for k \u2208 N .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A linear context-free grammar for palindromes 137 (375)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G aSa | bSb | \u03b5 | a | b\nS \u2212 \u2192G aSa\n\u2212 \u2192G abSba\n\u2212 \u2192G abbSbba\n\u2212 \u2192G abbaabba\nD wR := al . . . a1 for w = a1 . . . al \u2208 \u03a3\u2217.\nD A word is a palindrome if w = wR.\n\u25b7 {w \u2208 {a, b}\u2217 | w = wR} is also not regular,\ne.g. [ab1ab2ab3a . . . abka]\u223cL have to be distinct equivalence classes for k \u2208 N .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A linear context-free grammar for palindromes 137 (376)\nV = {S} \u03a3 = {a, b}\nS \u2212 \u2192G aSa | bSb | \u03b5 | a | b\nS \u2212 \u2192G aSa\n\u2212 \u2192G abSba\n\u2212 \u2192G abbSbba\n\u2212 \u2192G abbaabba\nD wR := al . . . a1 for w = a1 . . . al \u2208 \u03a3\u2217.\nD A word is a palindrome if w = wR.\n\u25b7 {w \u2208 {a, b}\u2217 | w = wR} is also not regular,\ne.g. [ab1ab2ab3a . . . abka]\u223cL have to be distinct equivalence classes for k \u2208 N .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Unary-binary trees and Dyck languages 138 (377)\nV = {S} \u03a3 = {[, ]}\nS \u2212 \u2192G [] | [S] | [SS]\n\"\u0014h\u0002\n[ ]\n\u0003ih\n[ ]\ni\u0015\u0014 hi \u0015#\n\u25b7 Main use of context-free grammars is to define well-formed/structured\nwords/texts like propositional formulas, arithmetic expressions, regular\nexpressions, programs: nested parentheses impose a tree structure on the\nword.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (378)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (379)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (380)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (381)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (382)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (383)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\n\u2212 \u2192G ((\u00acX op X ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (384)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\n\u2212 \u2192G ((\u00acX op X ) op F)\n\u2212 \u2192G ((\u00acX op x) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (385)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\n\u2212 \u2192G ((\u00acX op X ) op F)\n\u2212 \u2192G ((\u00acX op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (386)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\n\u2212 \u2192G ((\u00acX op X ) op F)\n\u2212 \u2192G ((\u00acX op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (387)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\n\u2212 \u2192G ((\u00acX op X ) op F)\n\u2212 \u2192G ((\u00acX op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X\u2032)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (388)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\n\u2212 \u2192G ((\u00acX op X ) op F)\n\u2212 \u2192G ((\u00acX op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X\u2032)\n\u2212 \u2192G ((\u00acx\u2032 op x) op X\u2032)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (389)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\n\u2212 \u2192G ((\u00acX op X ) op F)\n\u2212 \u2192G ((\u00acX op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X\u2032)\n\u2212 \u2192G ((\u00acx\u2032 op x) op X\u2032)\n\u2212 \u2192G ((\u00acx\u2032 op x) op x\u2032)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (390)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\n\u2212 \u2192G ((\u00acX op X ) op F)\n\u2212 \u2192G ((\u00acX op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X\u2032)\n\u2212 \u2192G ((\u00acx\u2032 op x) op X\u2032)\n\u2212 \u2192G ((\u00acx\u2032 op x) op x\u2032)\n\u2212 \u2192G ((\u00acx\u2032 op x) \u2227 x\u2032)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (391)\nV = {F, op, X} S = F \u03a3 = {(, ), \u00ac, \u2227, \u2228, x,\u2032 }\nF \u2212 \u2192G \u00acF | (F op F ) | X\nX \u2212 \u2192G X\u2032 | x\nop \u2212 \u2192G \u2227 | \u2228\nF \u2212 \u2192G (F op F )\n\u2212 \u2192G ((F op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op F ) op F)\n\u2212 \u2192G ((\u00acF op X ) op F)\n\u2212 \u2192G ((\u00acX op X ) op F)\n\u2212 \u2192G ((\u00acX op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op F)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X)\n\u2212 \u2192G ((\u00acX\u2032 op x) op X\u2032)\n\u2212 \u2192G ((\u00acx\u2032 op x) op X\u2032)\n\u2212 \u2192G ((\u00acx\u2032 op x) op x\u2032)\n\u2212 \u2192G ((\u00acx\u2032 op x) \u2227 x\u2032)\n\u2212 \u2192G ((\u00acx\u2032 \u2228 x) \u2227 x\u2032)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CFG, straight-line programs and compression 140 (392)\nX1 \u2212 \u2192G X2X2 . . . Xn\u22121 \u2212 \u2192G XnXn Xn \u2212 \u2192G a\nY1 \u2212 \u2192G baY2Y2 Y2 \u2212 \u2192G na\nD A straight-line program (SLP) for a word w \u2208 \u03a3\u2217 is a context-free grammar\nG = (V, \u03a3, P, X1) with V = {X1, . . . , Xn} so that:\n\u25b7 for every nonterminal Xi there is exactly one rule (Xi, \u03b3i) \u2208 P,\nand if Xj occurs in the right-hand side \u03b3i, then j > i.\ni.e. Xi can only \u201ccall\u201d Xj with j > i.\n(If we allow (Xi, XjXi) \u2208 P with j > i, then L(G) will be regular.)\n\u25b7 SLPs for words are related to LZ compression.\n\u25b7 If we allow multiple rules for each nonterminal, we obtain a circuit (=a\ndirected acyclic graph) that compresses a finite language by means of\nsubterm sharing (=storing isomoprhic subtrees only once).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-free grammars and recursive programs with finite memory\n141(393)\ndef P(x):\n...\nif ...:\ny = Q(x)\nz = R(y)\nelse:\nz = S(x)\nreturn z\nPxz \u2212 \u2192G QxyRyz | Sxz\n\u25b7 Context-free grammars are closely related to recursive programs:\n\u25b7 Nonterminals correspond to procedures.\n\u25b7 Variables with finite ranges can be encoded into the nonterminals.\n\u25b7 Every production rule encodes a potential sequence of recursive calls.\n\u25b7 If we allow the symbols V \u222a \u03a3 to commute, then context-free grammars\n(modulo commutativity) can describe basic parallelism without synchronization:\n\u25b7 A rule X \u2212 \u2192G Y Znow stands for: \u201cprocess X splits into two independent\nparallel processes Y, Z\u201d Also related to communication-free Petri nets.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Recursive descent parser 142 (394)\nstr w = \u2019\u00ac \u22280 \u2227 10\u2019\nint i = 0\ndef eval_F():\nx = w[i]\ni += 1\nif x == \u20190\u2019:\nreturn False\nIf x == \u20191\u2019:\nreturn True\nif x == \u2019\u00ac\u2019:\nreturn not parse_F()\nl = parse_F()\nr = parse_F()\nif x == \u2019\u2228\u2019:\nreturn l or r\nIf x == \u2019\u2227:\nreturn l and r\nassert False\n\u25b7 Recursive descent parsers essentially go the\nother way.\n\u25b7 In particular grammars in Greibach normal\nform where each terminal uniquely identifies\nits rule can be easily parsed.\n\u25b7 A special case is prefix/polish notation.\n\u25b7 Infix notation: \u00ac(0 \u2228 (1 \u2227 0))\nF \u2212 \u2192G \u00acF | (F \u2228 F) | (F \u2227 F) | 0 | 1\nPrefix notation: \u00ac \u22280 \u2227 10\nF \u2212 \u2192G \u00acF | \u2228FF | \u2227FF | 0 | 1\n\u25b7 This leads to the formal computational\nmodel that corresponds to context-free\ngrammars: push-down automata (PDA):\nPDA \u201c=\u201d FA + call stack\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "6 Context-free languages\nContext-free grammars: basic properties\nPush-down automata\nDerivation trees\nPumping lemma and closure properties\nAlgebraic equations and EBNF (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Push-down automata 144 (396)\nD Push-down automaton (PDA) M = (Q, \u03a3, \u0393, \u03b4, q0, \u22a5, F)\n\u25b7 Q: finite set of states (memory content)\n\u25b7 \u03a3: the finite input alphabet (user input), e.g. \u03a3 = {0, 1}\n\u25b7 \u0393: finite stack alphabet (procedure names), e.g. \u0393 = {0, 1, \u22a5}\n\u25b7 \u22a5: the bottom (of stack) symbol (\u201cmain procedure\u201d) with \u22a5 \u2208\u0393\n\u25b7 q0: the initial state with q0 \u2208 Q\n\u25b7 F: the final states with F \u2286 Q\n\u25b7 \u03b4: the transition rules with \u03b4 \u2286 (Q \u00d7 (\u03a3 \u222a {\u03b5}) \u00d7 \u0393) \u00d7 (Q \u00d7 \u0393\u2217)\nA transition rule ((q, a, A), (r, \u03b3)) \u2208 \u03b4 stands for the instruction:\nif state == \u2019q\u2019 and tape[pos] == \u2019a\u2019 and stack[0] == \u2019A\u2019:\nstate = \u2019r\u2019, pos += 1 # as for FA: move head to the left\nstack = \u2019\u03b3\u2019 + stack[1:] # update call stack: pop then push \u03b3\nAn \u03b5-transition rule ((q, \u03b5, A), (r, \u03b3)) \u2208 \u03b4 stands for the instruction:\nif state == \u2019q\u2019 and stack[0] == \u2019A\u2019:\nstate = \u2019r\u2019 # as for FA: head does not move\nstack = \u2019\u03b3\u2019 + stack[1:] # update call stack: pop then push \u03b3\nA PDA is deterministic (DPDA) if |(q, a, A)\u03b4| + |(q, \u03b5, A)\u03b4| \u22641 for every",
      "An \u03b5-transition rule ((q, \u03b5, A), (r, \u03b3)) \u2208 \u03b4 stands for the instruction:\nif state == \u2019q\u2019 and stack[0] == \u2019A\u2019:\nstate = \u2019r\u2019 # as for FA: head does not move\nstack = \u2019\u03b3\u2019 + stack[1:] # update call stack: pop then push \u03b3\nA PDA is deterministic (DPDA) if |(q, a, A)\u03b4| + |(q, \u03b5, A)\u03b4| \u22641 for every\n(q, a, A) \u2208 Q \u00d7 \u03a3 \u00d7 \u0393, otherwise it is nondeterministic.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Remark: Push-down automata and recursive programs 145 (397)\n\u25b7 A PDA can also be read as a (nondeterministic) recursive program, e.g.:\ndef A(state: Q) -> Q:\nx = input() # \u2208 \u03a3 \u222a {\u03b5}\nif state == p1 and x == \"a1\": # ((p1, a1, A), (q1, \u03b5))\nreturn q1\nif state == p2 and x == \"a2\": # ((p2, a2, A), (q2, B))\nreturn B(q2)\nif state == p3 and x == \"a3\": # ((p3, a3, A), (q3, BC))\nreturn C(B(q3))\n...\nraise Exception()\n\u25b7 Programming languages internally handle recursive calls via a call stack:\n\u25b7 The runtime environment thus implements essentially a (D)PDA to handle\nrecursion: In case of a recursive call a call frame is pushed on the (call) stack;\na call frame essentially states where to continue afterwards.\n\u25b7 The states Q represent the potential memory content. The stack alphabet \u0393\nconsists essentially of the potential call frames, in the simplest case just the\nprocedure names/entry addresses. (\u0393 \u00d7 Q can be thought of as call frames.) The\ntransitions are the actual program code.",
      "\u25b7 The states Q represent the potential memory content. The stack alphabet \u0393\nconsists essentially of the potential call frames, in the simplest case just the\nprocedure names/entry addresses. (\u0393 \u00d7 Q can be thought of as call frames.) The\ntransitions are the actual program code.\n\u25b7 See also Stack machine and JVM.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Remark: Push-down automata and recursive programs 145 (398)\n\u25b7 A PDA can also be read as a (nondeterministic) recursive program, e.g.:\ndef A(state: Q) -> Q:\nx = input() # \u2208 \u03a3 \u222a {\u03b5}\nif state == p1 and x == \"a1\": # ((p1, a1, A), (q1, \u03b5))\nreturn q1\nif state == p2 and x == \"a2\": # ((p2, a2, A), (q2, B))\nreturn B(q2)\nif state == p3 and x == \"a3\": # ((p3, a3, A), (q3, BC))\nreturn C(B(q3))\n...\nraise Exception()\n\u25b7 An alternative computational model for context-free languages are thus\nrecursive state machines (RSM) (image source; cf. theorem 1) , i.e. finite\nautomata that can call each other recursively.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "PDA: configuration, run, accepted language 146 (399)\nD Let M = (Q, \u03a3, \u0393, \u03b4, q0, \u22a5, F) be a PDA.\nA PDA is an FA extended with a stack (LIFO), i.e.\n\u25b7 it must not write to the (input) tape,\n\u25b7 it must read/consume its input completely from left to right,\n\u25b7 it may continue from a final state,\n\u25b7 it must pop and read the topmost symbol from the stack in every step,\nand may push an unbounded but finite number of new symboles on the stack.\nA configuration\n\u03b1q\u03b2 := (\u03b1, q, \u03b2) with q \u2208 Q, \u03b2\u2208 \u03a3\u2217, \u03b1\u2208 \u0393\u2217\nof an FA of the current state q, the remaining input \u03b2, and the current (call)\nstack content \u03b3.\n\u25b7 A PDA can also be considered a \u201c 1\n2 TM\u201d: if we extend a PDA with a second\nstack, the two stacks can be used to simulate a 1TM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "PDA: configuration, run, accepted language 146 (400)\nD Let M = (Q, \u03a3, \u0393, \u03b4, q0, \u22a5, F) be a PDA.\nThe rules \u03b4 generate the binary relation \u2212 \u2192M on \u0393\u2217 \u00d7 Q \u00d7 \u03a3\u2217:\n\u2212 \u2192M := {(\u03b1Aqa\u03b2, \u03b1\u03b3Rr\u03b5\u03b2) | ((q, a\n\u2208\u03a3\u222a{\u03b5}\n, A), (r, \u03b3)) \u2208 \u03b4, \u03b2\u2208 \u03a3\u2217, \u03b1\u2208 \u0393\u2217}\nA run of the FA is a path \u03b10q0\u03b20 \u2212 \u2192M \u03b11q1\u03b21 \u2212 \u2192M . . .wrt. \u2212 \u2192M .\n\u25b7 A run on input x1 . . . xl \u2208 \u03a3l is a path wrt. \u2212 \u2192M starting at \u22a5q0x1 . . . xl\n! A PDA always has to read the topmost symbol from the stack; if the stack is\nempty, the PDA \u201ccrashes\u201d; for this reason, the stack is always initialized to \u22a5:\nwhen reading \u22a5, the PDA knows that it is at the bottom.\n! Infinite runs can only arise from \u03b5-transitions as in the case of FAs.\n\u25b7 \u03b4 is deterministic iff every configuration has at most one successor wrt. \u2212 \u2192M\niff for every configuration there is exactly one maximal run.\n\u25b7 Analogously to the transformation into Kuroda normal form, we can modify\nany PDA so that it pushes at most two symbols on the stack.",
      "\u25b7 \u03b4 is deterministic iff every configuration has at most one successor wrt. \u2212 \u2192M\niff for every configuration there is exactly one maximal run.\n\u25b7 Analogously to the transformation into Kuroda normal form, we can modify\nany PDA so that it pushes at most two symbols on the stack.\n\u25b7 A single dummy state suffices as we can store it on the stack instead.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "PDA: configuration, run, accepted language 146 (401)\nD Let M = (Q, \u03a3, \u0393, \u03b4, q0, \u22a5, F) be a PDA.\nThe rules \u03b4 generate the binary relation \u2212 \u2192M on \u0393\u2217 \u00d7 Q \u00d7 \u03a3\u2217:\n\u2212 \u2192M := {(\u03b1Aqa\u03b2, \u03b1\u03b3Rr\u03b5\u03b2) | ((q, a\n\u2208\u03a3\u222a{\u03b5}\n, A), (r, \u03b3)) \u2208 \u03b4, \u03b2\u2208 \u03a3\u2217, \u03b1\u2208 \u0393\u2217}\nA run of the FA is a path \u03b10q0\u03b20 \u2212 \u2192M \u03b11q1\u03b21 \u2212 \u2192M . . .wrt. \u2212 \u2192M .\nFor PDAs two difference \u201csemantics\u201d are commonly used: The language\naccepted wrt. final states (via \u201cexit()\u201d or \u201c raise Exception()\u201d) is:\nLF (M) = {w \u2208 \u03a3\u2217 | \u22a5q0w \u2212 \u2192\u2217\nG \u03b1qf \u03b5 for some qf \u2208 F , \u03b1\u2208 \u0393\u2217}\nwhile the language accepted wrt. empty stack (via \u201creturn\u201d, no pending calls) is:\nL\u03b5(M) = {w \u2208 \u03a3\u2217 | \u22a5q0w \u2212 \u2192\u2217\nG \u03b5q\u03b5 for some q \u2208 Q}\nAs in case of FAs, the input has to be completely processed.\nFor PDAs both can simulate each other, but for DPDAs only acceptance on\nfinal states can simulate acceptance on empty stack.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: PDA for a Dyck language 147 (402)\nq0start qf\na, A: AA; a,\u22a5: A\u22a5\nb, A: \u03b5\n\u03b5,\u22a5: \u03b5\n\u22a5q0abaabb\nq0\n. . .\u25a1 a b a a b b \u25a1 . . .\n\u22a5\n\u25b7 Above PDA accepts\nS \u2212 \u2192G \u03b5 | SS | aSb\nboth wrt. final state and empty\nstack.\n\u25b7 It is not deterministic:\nFrom \u22a5q0a we can transition to\n\u22a5Aq0\u03b5 or to \u03b5qf a.\n\u25b7 PDAs can count in unary using the\nstack.\n\u25b7 A second stack would allow to\nsimulate a 1TM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: PDA for a Dyck language 147 (403)\nq0start qf\na, A: AA; a,\u22a5: A\u22a5\nb, A: \u03b5\n\u03b5,\u22a5: \u03b5\n\u22a5Aq0baabb\nq0\n. . .\u25a1 a b a a b b \u25a1 . . .\nA\n\u22a5\n\u25b7 Above PDA accepts\nS \u2212 \u2192G \u03b5 | SS | aSb\nboth wrt. final state and empty\nstack.\n\u25b7 It is not deterministic:\nFrom \u22a5q0a we can transition to\n\u22a5Aq0\u03b5 or to \u03b5qf a.\n\u25b7 PDAs can count in unary using the\nstack.\n\u25b7 A second stack would allow to\nsimulate a 1TM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: PDA for a Dyck language 147 (404)\nq0start qf\na, A: AA; a,\u22a5: A\u22a5\nb, A: \u03b5\n\u03b5,\u22a5: \u03b5\n\u22a5q0aabb\nq0\n. . .\u25a1 a b a a b b \u25a1 . . .\n\u22a5\n\u25b7 Above PDA accepts\nS \u2212 \u2192G \u03b5 | SS | aSb\nboth wrt. final state and empty\nstack.\n\u25b7 It is not deterministic:\nFrom \u22a5q0a we can transition to\n\u22a5Aq0\u03b5 or to \u03b5qf a.\n\u25b7 PDAs can count in unary using the\nstack.\n\u25b7 A second stack would allow to\nsimulate a 1TM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: PDA for a Dyck language 147 (405)\nq0start qf\na, A: AA; a,\u22a5: A\u22a5\nb, A: \u03b5\n\u03b5,\u22a5: \u03b5\n\u22a5Aq0abb\nq0\n. . .\u25a1 a b a a b b \u25a1 . . .\nA\n\u22a5\n\u25b7 Above PDA accepts\nS \u2212 \u2192G \u03b5 | SS | aSb\nboth wrt. final state and empty\nstack.\n\u25b7 It is not deterministic:\nFrom \u22a5q0a we can transition to\n\u22a5Aq0\u03b5 or to \u03b5qf a.\n\u25b7 PDAs can count in unary using the\nstack.\n\u25b7 A second stack would allow to\nsimulate a 1TM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: PDA for a Dyck language 147 (406)\nq0start qf\na, A: AA; a,\u22a5: A\u22a5\nb, A: \u03b5\n\u03b5,\u22a5: \u03b5\n\u22a5AAq0bb\nq0\n. . .\u25a1 a b a a b b \u25a1 . . .\nA\nA\n\u22a5\n\u25b7 Above PDA accepts\nS \u2212 \u2192G \u03b5 | SS | aSb\nboth wrt. final state and empty\nstack.\n\u25b7 It is not deterministic:\nFrom \u22a5q0a we can transition to\n\u22a5Aq0\u03b5 or to \u03b5qf a.\n\u25b7 PDAs can count in unary using the\nstack.\n\u25b7 A second stack would allow to\nsimulate a 1TM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: PDA for a Dyck language 147 (407)\nq0start qf\na, A: AA; a,\u22a5: A\u22a5\nb, A: \u03b5\n\u03b5,\u22a5: \u03b5\n\u22a5Aq0b\nq0\n. . .\u25a1 a b a a b b \u25a1 . . .\nA\n\u22a5\n\u25b7 Above PDA accepts\nS \u2212 \u2192G \u03b5 | SS | aSb\nboth wrt. final state and empty\nstack.\n\u25b7 It is not deterministic:\nFrom \u22a5q0a we can transition to\n\u22a5Aq0\u03b5 or to \u03b5qf a.\n\u25b7 PDAs can count in unary using the\nstack.\n\u25b7 A second stack would allow to\nsimulate a 1TM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: PDA for a Dyck language 147 (408)\nq0start qf\na, A: AA; a,\u22a5: A\u22a5\nb, A: \u03b5\n\u03b5,\u22a5: \u03b5\n\u22a5q0\nq0\n. . .\u25a1 a b a a b b \u25a1 . . .\n\u22a5\n\u25b7 Above PDA accepts\nS \u2212 \u2192G \u03b5 | SS | aSb\nboth wrt. final state and empty\nstack.\n\u25b7 It is not deterministic:\nFrom \u22a5q0a we can transition to\n\u22a5Aq0\u03b5 or to \u03b5qf a.\n\u25b7 PDAs can count in unary using the\nstack.\n\u25b7 A second stack would allow to\nsimulate a 1TM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: PDA for a Dyck language 147 (409)\nq0start qf\na, A: AA; a,\u22a5: A\u22a5\nb, A: \u03b5\n\u03b5,\u22a5: \u03b5\nqf\nqf\n. . .\u25a1 a b a a b b \u25a1 . . .\n\u03b5\n\u25b7 Above PDA accepts\nS \u2212 \u2192G \u03b5 | SS | aSb\nboth wrt. final state and empty\nstack.\n\u25b7 It is not deterministic:\nFrom \u22a5q0a we can transition to\n\u22a5Aq0\u03b5 or to \u03b5qf a.\n\u25b7 PDAs can count in unary using the\nstack.\n\u25b7 A second stack would allow to\nsimulate a 1TM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for a Dyck language 148 (410)\nq0start q1\na,\u22a5: A\u22a5\na, A: AA; a, A\u22a5: AA\u22a5\nb, A: \u03b5\nb, A\u22a5: \u22a5\n\u22a5q0abaabb\nq0\n. . .\u25a1 a b a a b b \u25a1 . . .\n\u22a5\n\u25b7 Above DPDA accepts\nS \u2212 \u2192G \u03b5 | SS | aSb\nwrt. final states, but the empty\nlanguage wrt. empty stack.\n\u25b7 Define a DPDA that accepts\nS \u2212 \u2192G ab | aSSb | aSb\nwrt. both final states and empty\nstack.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for a Dyck language 148 (411)\nq0start q1\na,\u22a5: A\u22a5\na, A: AA; a, A\u22a5: AA\u22a5\nb, A: \u03b5\nb, A\u22a5: \u22a5\nA\u22a5q1baabb\nq1\n. . .\u25a1 a b a a b b \u25a1 . . .\nA\u22a5\n\u25b7 Above DPDA accepts\nS \u2212 \u2192G \u03b5 | SS | aSb\nwrt. final states, but the empty\nlanguage wrt. empty stack.\n\u25b7 Define a DPDA that accepts\nS \u2212 \u2192G ab | aSSb | aSb\nwrt. both final states and empty\nstack.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for a Dyck language 148 (412)\nq0start q1\na,\u22a5: A\u22a5\na, A: AA; a, A\u22a5: AA\u22a5\nb, A: \u03b5\nb, A\u22a5: \u22a5\n\u22a5q0aabb\nq0\n. . .\u25a1 a b a a b b \u25a1 . . .\n\u22a5\n\u25b7 Above DPDA accepts\nS \u2212 \u2192G \u03b5 | SS | aSb\nwrt. final states, but the empty\nlanguage wrt. empty stack.\n\u25b7 Define a DPDA that accepts\nS \u2212 \u2192G ab | aSSb | aSb\nwrt. both final states and empty\nstack.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for a Dyck language 148 (413)\nq0start q1\na,\u22a5: A\u22a5\na, A: AA; a, A\u22a5: AA\u22a5\nb, A: \u03b5\nb, A\u22a5: \u22a5\nA\u22a5q1abb\nq1\n. . .\u25a1 a b a a b b \u25a1 . . .\nA\u22a5\n\u25b7 Above DPDA accepts\nS \u2212 \u2192G \u03b5 | SS | aSb\nwrt. final states, but the empty\nlanguage wrt. empty stack.\n\u25b7 Define a DPDA that accepts\nS \u2212 \u2192G ab | aSSb | aSb\nwrt. both final states and empty\nstack.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for a Dyck language 148 (414)\nq0start q1\na,\u22a5: A\u22a5\na, A: AA; a, A\u22a5: AA\u22a5\nb, A: \u03b5\nb, A\u22a5: \u22a5\nA\u22a5Aq1bb\nq1\n. . .\u25a1 a b a a b b \u25a1 . . .\nA\nA\u22a5\n\u25b7 Above DPDA accepts\nS \u2212 \u2192G \u03b5 | SS | aSb\nwrt. final states, but the empty\nlanguage wrt. empty stack.\n\u25b7 Define a DPDA that accepts\nS \u2212 \u2192G ab | aSSb | aSb\nwrt. both final states and empty\nstack.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for a Dyck language 148 (415)\nq0start q1\na,\u22a5: A\u22a5\na, A: AA; a, A\u22a5: AA\u22a5\nb, A: \u03b5\nb, A\u22a5: \u22a5\nA\u22a5q1b\nq1\n. . .\u25a1 a b a a b b \u25a1 . . .\nA\u22a5\n\u25b7 Above DPDA accepts\nS \u2212 \u2192G \u03b5 | SS | aSb\nwrt. final states, but the empty\nlanguage wrt. empty stack.\n\u25b7 Define a DPDA that accepts\nS \u2212 \u2192G ab | aSSb | aSb\nwrt. both final states and empty\nstack.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for a Dyck language 148 (416)\nq0start q1\na,\u22a5: A\u22a5\na, A: AA; a, A\u22a5: AA\u22a5\nb, A: \u03b5\nb, A\u22a5: \u22a5\n\u22a5q0\nq0\n. . .\u25a1 a b a a b b \u25a1 . . .\n\u22a5\n\u25b7 Above DPDA accepts\nS \u2212 \u2192G \u03b5 | SS | aSb\nwrt. final states, but the empty\nlanguage wrt. empty stack.\n\u25b7 Define a DPDA that accepts\nS \u2212 \u2192G ab | aSSb | aSb\nwrt. both final states and empty\nstack.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for binary-unary trees in prefix notation 149 (417)\nq0start\na, S: SS\nb, S: S\nc, S: \u03b5\nSq0aabccc\nq0\n. . .\u25a1 a a b c c c \u25a1 . . .\nS\n\u25b7 Above DPDA accepts\nS \u2212 \u2192G aSS | bS | c\nwrt. empty stack.\n\u25b7 The grammar encodes binary-unary\ntrees/terms in polish/prefix\nnotation, e.g. abcc\u02c6 =a(b(c), c).\n\u25b7 Such languages are prefix-free, i.e. if\nu \u2208 L, then uv \u0338\u2208 L for all v \u2208 \u03a3+.\n! Show: wrt. empty stack, an DPDA\ncan only accept prefix-free\nlanguages.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for binary-unary trees in prefix notation 149 (418)\nq0start\na, S: SS\nb, S: S\nc, S: \u03b5\nSSq0abccc\nq0\n. . .\u25a1 a a b c c c \u25a1 . . .\nS\nS\n\u25b7 Above DPDA accepts\nS \u2212 \u2192G aSS | bS | c\nwrt. empty stack.\n\u25b7 The grammar encodes binary-unary\ntrees/terms in polish/prefix\nnotation, e.g. abcc\u02c6 =a(b(c), c).\n\u25b7 Such languages are prefix-free, i.e. if\nu \u2208 L, then uv \u0338\u2208 L for all v \u2208 \u03a3+.\n! Show: wrt. empty stack, an DPDA\ncan only accept prefix-free\nlanguages.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for binary-unary trees in prefix notation 149 (419)\nq0start\na, S: SS\nb, S: S\nc, S: \u03b5\nSSSSq 0bccc\nq0\n. . .\u25a1 a a b c c c \u25a1 . . .\nS\nS\nS\n\u25b7 Above DPDA accepts\nS \u2212 \u2192G aSS | bS | c\nwrt. empty stack.\n\u25b7 The grammar encodes binary-unary\ntrees/terms in polish/prefix\nnotation, e.g. abcc\u02c6 =a(b(c), c).\n\u25b7 Such languages are prefix-free, i.e. if\nu \u2208 L, then uv \u0338\u2208 L for all v \u2208 \u03a3+.\n! Show: wrt. empty stack, an DPDA\ncan only accept prefix-free\nlanguages.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for binary-unary trees in prefix notation 149 (420)\nq0start\na, S: SS\nb, S: S\nc, S: \u03b5\nSSSq 0ccc\nq0\n. . .\u25a1 a a b c c c \u25a1 . . .\nS\nS\nS\n\u25b7 Above DPDA accepts\nS \u2212 \u2192G aSS | bS | c\nwrt. empty stack.\n\u25b7 The grammar encodes binary-unary\ntrees/terms in polish/prefix\nnotation, e.g. abcc\u02c6 =a(b(c), c).\n\u25b7 Such languages are prefix-free, i.e. if\nu \u2208 L, then uv \u0338\u2208 L for all v \u2208 \u03a3+.\n! Show: wrt. empty stack, an DPDA\ncan only accept prefix-free\nlanguages.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for binary-unary trees in prefix notation 149 (421)\nq0start\na, S: SS\nb, S: S\nc, S: \u03b5\nSSq0cc\nq0\n. . .\u25a1 a a b c c c \u25a1 . . .\nS\nS\n\u25b7 Above DPDA accepts\nS \u2212 \u2192G aSS | bS | c\nwrt. empty stack.\n\u25b7 The grammar encodes binary-unary\ntrees/terms in polish/prefix\nnotation, e.g. abcc\u02c6 =a(b(c), c).\n\u25b7 Such languages are prefix-free, i.e. if\nu \u2208 L, then uv \u0338\u2208 L for all v \u2208 \u03a3+.\n! Show: wrt. empty stack, an DPDA\ncan only accept prefix-free\nlanguages.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for binary-unary trees in prefix notation 149 (422)\nq0start\na, S: SS\nb, S: S\nc, S: \u03b5\nSq0c\nq0\n. . .\u25a1 a a b c c c \u25a1 . . .\nS\n\u25b7 Above DPDA accepts\nS \u2212 \u2192G aSS | bS | c\nwrt. empty stack.\n\u25b7 The grammar encodes binary-unary\ntrees/terms in polish/prefix\nnotation, e.g. abcc\u02c6 =a(b(c), c).\n\u25b7 Such languages are prefix-free, i.e. if\nu \u2208 L, then uv \u0338\u2208 L for all v \u2208 \u03a3+.\n! Show: wrt. empty stack, an DPDA\ncan only accept prefix-free\nlanguages.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for binary-unary trees in prefix notation 149 (423)\nq0start\na, S: SS\nb, S: S\nc, S: \u03b5\nq0\nq0\n. . .\u25a1 a a b c c c \u25a1 . . .\n\u03b5\n\u25b7 Above DPDA accepts\nS \u2212 \u2192G aSS | bS | c\nwrt. empty stack.\n\u25b7 The grammar encodes binary-unary\ntrees/terms in polish/prefix\nnotation, e.g. abcc\u02c6 =a(b(c), c).\n\u25b7 Such languages are prefix-free, i.e. if\nu \u2208 L, then uv \u0338\u2208 L for all v \u2208 \u03a3+.\n! Show: wrt. empty stack, an DPDA\ncan only accept prefix-free\nlanguages.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "PDA: final states vs. empty stack 150 (424)\nL From every PDA M = (Q, \u03a3, \u0393, \u03b4, q0, \u22a5, F) we can construct M\u2032 and M\u2032\u2032 s.t.\nL\u03b5(M) = LF (M\u2032) LF (M) = L\u03b5(M\u2032\u2032)\n(In general, LF (M) \u0338= L\u03b5(M) for a specific PDA!)\n\u25b7 L\u03b5(M) = LF (M\u2032): Let \u22a5\u2032 \u0338\u2208 \u0393 be an unused \u201cbelow-bottom symbol\u201d.\nM\u2032 starts with (q\u2032\n0w, \u22a5\u2032) \u2212 \u2192M (q0w, \u22a5\u22a5\u2032) and then calls M on w until only\n\u22a5\u2032 remains (M returns).\nAs M does not know \u22a5\u2032, we can let M\u2032 take over and transition into a unique\nfinal state whenever M has emptied its stack (i.e. \u201cM(w); exit(0);\u201d).\n\u25b7 LF (M) = L\u03b5(M\u2032\u2032):\nAgain, M\u2032\u2032 starts with (q\u2032\n0w, \u22a5\u2032) \u2212 \u2192M (q0w, \u22a5\u22a5\u2032) and then calls M on w.\nNow add \u03b5-transitions (leading to nondeterminism) that allow to empty the stack\nwhenever a final state is reached (potentially terminating the run prematurely) .\n(This is somewhat similar to what the runtime does in case of an exception.)\nAs before M cannot process \u22a5\u2032 itself; so only M\u2032 can erase it while using its\nrules to empty the stack after M has reached a final state.",
      "whenever a final state is reached (potentially terminating the run prematurely) .\n(This is somewhat similar to what the runtime does in case of an exception.)\nAs before M cannot process \u22a5\u2032 itself; so only M\u2032 can erase it while using its\nrules to empty the stack after M has reached a final state.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "PDA: final states vs. empty stack 150 (425)\nL From every PDA M = (Q, \u03a3, \u0393, \u03b4, q0, \u22a5, F) we can construct M\u2032 and M\u2032\u2032 s.t.\nL\u03b5(M) = LF (M\u2032) LF (M) = L\u03b5(M\u2032\u2032)\n(In general, LF (M) \u0338= L\u03b5(M) for a specific PDA!)\n! For DPDAs acceptance wrt. final state resp. on empty stack do not define\nthe same class of languages!\n\u25b7 We still can construct for every DPDA M an DPDA M\u2032 s.t.\nL\u03b5(M) = LF (M\u2032) as M\u2032 can deterministically proceed from \u22a5\u2032.\n\u25b7 But the other direction is not true anymore: a DPDA can accept on empty\nstack only the strict subset of prefix-free languages accepted wrt. final state.\n\u201cprefix-free\u201d means that if w \u2208 L, then ww\u2032 \u0338\u2208 L for any w\u2032 \u0338= \u03b5.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From CFG to PDA (accept on empty stack) 151(426)\nS \u2212 \u2192G aSS S \u2212 \u2192G bS S \u2212 \u2192G c\nq0start\n\u03b5, S: aSS\n\u03b5, S: bS\n\u03b5, S: c\na, a: \u03b5; b, b: \u03b5; c, c: \u03b5\nL For every context-free G there is a PDA M with L\u03b5(M) = L(G):\nThe PDA guesses nondeterministically a leftmost(=topmost) derivation on its stack.\nA rewrite rule A \u2212 \u2192G \u03b3 gives rise to an \u03b5-transition (guess) ((q0, \u03b5, A), (q0, \u03b3)).\nFor every a \u2208 \u03a3 add the transition ((q0, a, a), (q0, \u03b5)) to check the current input.\nq0\n. . .\u25a1 a b c c \u25a1 . . .\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From CFG to PDA (accept on empty stack) 151(427)\nS \u2212 \u2192G aSS S \u2212 \u2192G bS S \u2212 \u2192G c\nq0start\n\u03b5, S: aSS\n\u03b5, S: bS\n\u03b5, S: c\na, a: \u03b5; b, b: \u03b5; c, c: \u03b5\nL For every context-free G there is a PDA M with L\u03b5(M) = L(G):\nThe PDA guesses nondeterministically a leftmost(=topmost) derivation on its stack.\nA rewrite rule A \u2212 \u2192G \u03b3 gives rise to an \u03b5-transition (guess) ((q0, \u03b5, A), (q0, \u03b3)).\nFor every a \u2208 \u03a3 add the transition ((q0, a, a), (q0, \u03b5)) to check the current input.\nq0\n. . .\u25a1 a b c c \u25a1 . . .\na\nS\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From CFG to PDA (accept on empty stack) 151(428)\nS \u2212 \u2192G aSS S \u2212 \u2192G bS S \u2212 \u2192G c\nq0start\n\u03b5, S: aSS\n\u03b5, S: bS\n\u03b5, S: c\na, a: \u03b5; b, b: \u03b5; c, c: \u03b5\nL For every context-free G there is a PDA M with L\u03b5(M) = L(G):\nThe PDA guesses nondeterministically a leftmost(=topmost) derivation on its stack.\nA rewrite rule A \u2212 \u2192G \u03b3 gives rise to an \u03b5-transition (guess) ((q0, \u03b5, A), (q0, \u03b3)).\nFor every a \u2208 \u03a3 add the transition ((q0, a, a), (q0, \u03b5)) to check the current input.\nq0\n. . .\u25a1 a b c c \u25a1 . . .\nS\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From CFG to PDA (accept on empty stack) 151(429)\nS \u2212 \u2192G aSS S \u2212 \u2192G bS S \u2212 \u2192G c\nq0start\n\u03b5, S: aSS\n\u03b5, S: bS\n\u03b5, S: c\na, a: \u03b5; b, b: \u03b5; c, c: \u03b5\nL For every context-free G there is a PDA M with L\u03b5(M) = L(G):\nThe PDA guesses nondeterministically a leftmost(=topmost) derivation on its stack.\nA rewrite rule A \u2212 \u2192G \u03b3 gives rise to an \u03b5-transition (guess) ((q0, \u03b5, A), (q0, \u03b3)).\nFor every a \u2208 \u03a3 add the transition ((q0, a, a), (q0, \u03b5)) to check the current input.\nq0\n. . .\u25a1 a b c c \u25a1 . . .\nb\nS\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From CFG to PDA (accept on empty stack) 151(430)\nS \u2212 \u2192G aSS S \u2212 \u2192G bS S \u2212 \u2192G c\nq0start\n\u03b5, S: aSS\n\u03b5, S: bS\n\u03b5, S: c\na, a: \u03b5; b, b: \u03b5; c, c: \u03b5\nL For every context-free G there is a PDA M with L\u03b5(M) = L(G):\nThe PDA guesses nondeterministically a leftmost(=topmost) derivation on its stack.\nA rewrite rule A \u2212 \u2192G \u03b3 gives rise to an \u03b5-transition (guess) ((q0, \u03b5, A), (q0, \u03b3)).\nFor every a \u2208 \u03a3 add the transition ((q0, a, a), (q0, \u03b5)) to check the current input.\nq0\n. . .\u25a1 a b c c \u25a1 . . .\nS\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From CFG to PDA (accept on empty stack) 151(431)\nS \u2212 \u2192G aSS S \u2212 \u2192G bS S \u2212 \u2192G c\nq0start\n\u03b5, S: aSS\n\u03b5, S: bS\n\u03b5, S: c\na, a: \u03b5; b, b: \u03b5; c, c: \u03b5\nL For every context-free G there is a PDA M with L\u03b5(M) = L(G):\nThe PDA guesses nondeterministically a leftmost(=topmost) derivation on its stack.\nA rewrite rule A \u2212 \u2192G \u03b3 gives rise to an \u03b5-transition (guess) ((q0, \u03b5, A), (q0, \u03b3)).\nFor every a \u2208 \u03a3 add the transition ((q0, a, a), (q0, \u03b5)) to check the current input.\nq0\n. . .\u25a1 a b c c \u25a1 . . .\nc\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From CFG to PDA (accept on empty stack) 151(432)\nS \u2212 \u2192G aSS S \u2212 \u2192G bS S \u2212 \u2192G c\nq0start\n\u03b5, S: aSS\n\u03b5, S: bS\n\u03b5, S: c\na, a: \u03b5; b, b: \u03b5; c, c: \u03b5\nL For every context-free G there is a PDA M with L\u03b5(M) = L(G):\nThe PDA guesses nondeterministically a leftmost(=topmost) derivation on its stack.\nA rewrite rule A \u2212 \u2192G \u03b3 gives rise to an \u03b5-transition (guess) ((q0, \u03b5, A), (q0, \u03b3)).\nFor every a \u2208 \u03a3 add the transition ((q0, a, a), (q0, \u03b5)) to check the current input.\nq0\n. . .\u25a1 a b c c \u25a1 . . .\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From CFG to PDA (accept on empty stack) 151(433)\nS \u2212 \u2192G aSS S \u2212 \u2192G bS S \u2212 \u2192G c\nq0start\n\u03b5, S: aSS\n\u03b5, S: bS\n\u03b5, S: c\na, a: \u03b5; b, b: \u03b5; c, c: \u03b5\nL For every context-free G there is a PDA M with L\u03b5(M) = L(G):\nThe PDA guesses nondeterministically a leftmost(=topmost) derivation on its stack.\nA rewrite rule A \u2212 \u2192G \u03b3 gives rise to an \u03b5-transition (guess) ((q0, \u03b5, A), (q0, \u03b3)).\nFor every a \u2208 \u03a3 add the transition ((q0, a, a), (q0, \u03b5)) to check the current input.\nq0\n. . .\u25a1 a b c c \u25a1 . . .\nc\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From CFG to PDA (accept on empty stack) 151(434)\nS \u2212 \u2192G aSS S \u2212 \u2192G bS S \u2212 \u2192G c\nq0start\n\u03b5, S: aSS\n\u03b5, S: bS\n\u03b5, S: c\na, a: \u03b5; b, b: \u03b5; c, c: \u03b5\nL For every context-free G there is a PDA M with L\u03b5(M) = L(G):\nThe PDA guesses nondeterministically a leftmost(=topmost) derivation on its stack.\nA rewrite rule A \u2212 \u2192G \u03b3 gives rise to an \u03b5-transition (guess) ((q0, \u03b5, A), (q0, \u03b3)).\nFor every a \u2208 \u03a3 add the transition ((q0, a, a), (q0, \u03b5)) to check the current input.\nq0\n. . .\u25a1 a b c c \u25a1 . . .\n\u03b5\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From PDA (accept on empty stack) to CFG 152 (435)\n0start 1 2\na,\u22a5: X\u22a5\na, X: XX\nb, X: \u03b5\nb, X: \u03b5\n\u03b5,\u22a5: \u03b5\nS \u2212 \u2192G \u22a50,r\n\u22a50,r \u2212 \u2192G aX0,r\u2032\u22a5r\u2032,r\n\u22a51,2 \u2212 \u2192G \u03b5\nX0,r \u2212 \u2192G aX0,r\u2032Xr\u2032,r\nX0,1 \u2212 \u2192G b\nX1,1 \u2212 \u2192G b\n(r\u2032, r\u2208 {0, 1, 2})\ndef bot(q): # \u22a5\nx = input()\nif q == 0 and x == \"a\":\nreturn bot(X(0))\nif q == 1 and x == \"\":\nreturn 2\nraise Exception()\ndef X(q):\nx = input()\nif q == 0 and x == \"a\":\nreturn X(X(0))\nif q == 0 and x == \"b\":\nreturn 1\nif q == 1 and x == \"b\":\nreturn 1\nraise Exception()\nL For every PDA M there is a context-free G with L(G) = L\u03b5(M):\nThe states Q describe the memory content/state. A transition ((q, a, A), (q\u2032, \u03b3)) \u2208 \u03b4 is a\ncall of the procedure A that updates the memory from state q to state q\u2032 and leads to the\nsubsequent recursive calls \u03b3.\nThe variable Ap,r produces all inputs that make the procedure A return the memory state\nr when originally called in memory state p.",
      "call of the procedure A that updates the memory from state q to state q\u2032 and leads to the\nsubsequent recursive calls \u03b3.\nThe variable Ap,r produces all inputs that make the procedure A return the memory state\nr when originally called in memory state p.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From PDA (accept on empty stack) to CFG 152 (436)\n0start 1 2\na,\u22a5: X\u22a5\na, X: XX\nb, X: \u03b5\nb, X: \u03b5\n\u03b5,\u22a5: \u03b5\nS \u2212 \u2192G \u22a50,r\n\u22a50,r \u2212 \u2192G aX0,r\u2032\u22a5r\u2032,r\n\u22a51,2 \u2212 \u2192G \u03b5\nX0,r \u2212 \u2192G aX0,r\u2032Xr\u2032,r\nX0,1 \u2212 \u2192G b\nX1,1 \u2212 \u2192G b\n(r\u2032, r\u2208 {0, 1, 2})\ndef bot(q): # \u22a5\nx = input()\nif q == 0 and x == \"a\":\nreturn bot(X(0))\nif q == 1 and x == \"\":\nreturn 2\nraise Exception()\ndef X(q):\nx = input()\nif q == 0 and x == \"a\":\nreturn X(X(0))\nif q == 0 and x == \"b\":\nreturn 1\nif q == 1 and x == \"b\":\nreturn 1\nraise Exception()\n\u25b7 Simplify the grammar by removing non-productive and unreachable variables:\nAs only \u22a51,2, X0,1, X1,1 are directly productive, by backward propagation/resolution\nonly \u22a50,1 \u2212 \u2192G aX0,1\u22a51,2 and X0,1 \u2212 \u2192G aX0,1X1,1 can terminate.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From PDA (accept on empty stack) to CFG 152 (437)\n0start 1 2\na,\u22a5: X\u22a5\na, X: XX\nb, X: \u03b5\nb, X: \u03b5\n\u03b5,\u22a5: \u03b5\nS \u2212 \u2192G \u22a50,2\n\u22a51,2 \u2212 \u2192G \u03b5\nX0,1 \u2212 \u2192G b\nX1,1 \u2212 \u2192G b\n\u22a50,2 \u2212 \u2192G aX0,1\u22a51,2\nX0,1 \u2212 \u2192G aX0,1X1,1\ndef bot(q): # \u22a5\nx = input()\nif q == 0 and x == \"a\":\nreturn bot(X(0))\nif q == 1 and x == \"\":\nreturn 2\nraise Exception()\ndef X(q):\nx = input()\nif q == 0 and x == \"a\":\nreturn X(X(0))\nif q == 0 and x == \"b\":\nreturn 1\nif q == 1 and x == \"b\":\nreturn 1\nraise Exception()\n\u25b7 Simplify the grammar by removing non-productive and unreachable variables:\nAs only \u22a51,2, X0,1, X1,1 are directly productive, by backward propagation/resolution\nonly \u22a50,1 \u2212 \u2192G aX0,1\u22a51,2 and X0,1 \u2212 \u2192G aX0,1X1,1 can terminate.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From PDA (accept on empty stack) to CFG 152 (438)\n0start 1 2\na,\u22a5: X\u22a5\na, X: XX\nb, X: \u03b5\nb, X: \u03b5\n\u03b5,\u22a5: \u03b5\nS \u2212 \u2192G \u22a50,2\n\u22a51,2 \u2212 \u2192G \u03b5\nX0,1 \u2212 \u2192G b\nX1,1 \u2212 \u2192G b\n\u22a50,2 \u2212 \u2192G aX0,1\u22a51,2\nX0,1 \u2212 \u2192G aX0,1X1,1\ndef bot(q): # \u22a5\nx = input()\nif q == 0 and x == \"a\":\nreturn bot(X(0))\nif q == 1 and x == \"\":\nreturn 2\nraise Exception()\ndef X(q):\nx = input()\nif q == 0 and x == \"a\":\nreturn X(X(0))\nif q == 0 and x == \"b\":\nreturn 1\nif q == 1 and x == \"b\":\nreturn 1\nraise Exception()\nS \u2212 \u2192G \u22a50,2 \u2212 \u2192G aX0,1\u22a51,2\n\u2212 \u2192G aaX0,1X1,1\u22a51,2\n\u2212 \u2192G aabX1,1\u22a51,2 \u2212 \u2192G aabb\u22a51,2 \u2212 \u2192G aabb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-free languages and PDAs 153 (439)\nT The languages accepted by (nondeterministic) PDAs (on empty stack) are\nexactly the context-free languages.\n\u25b7 From grammar G = (V, \u03a3, P, S) to PDA M = (Q, \u03a3, \u0393, \u03b4, q0, \u22a5, F):\nLet the PDA guess a leftmost derivation on the stack using \u03b5-transitions, i.e.\n\u25b7 Use S as bottom symbol (\u201c main()\u201d).\n\u25b7 For every production rule (A, \u03b3) \u2208 P add the transition ((q0, \u03b5, A), (q0, \u03b3)).\n(I.e. nondeterministically rewrite the leftmoste/topmost nonterminal.)\n\u25b7 For every terminal a \u2208 \u03a3 add the transition ((q0, a, a), (q0, \u03b5)).\n(I.e. check the current input symbol against the leftmost/topmost terminal.)\nThen L(G) = L\u03b5(M).\n(The formal proof requires to show both L(G) \u2286 L\u03b5(M) and L\u03b5(M) \u2286 L(G) using\ninduction on the length of a derivation.)\n(Note that this requires only a single state: this is analogous to including the\ncomplete memory content into the stack frame.)",
      "Then L(G) = L\u03b5(M).\n(The formal proof requires to show both L(G) \u2286 L\u03b5(M) and L\u03b5(M) \u2286 L(G) using\ninduction on the length of a derivation.)\n(Note that this requires only a single state: this is analogous to including the\ncomplete memory content into the stack frame.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-free languages and PDAs 153 (440)\nT The languages accepted by (nondeterministic) PDAs (on empty stack) are\nexactly the context-free languages.\n\u25b7 From PDA M = (Q, \u03a3, \u0393, \u03b4, q0, \u22a5, F) to grammar G = (V, \u03a3, P, S):\nwlog we assume that the PDA accepts on empty stack.\nWe thus need to define G s.t. L(G) = L\u03b5(M).\nConsider an accepting run\n\u22a5q0w \u2212 \u2192\u2217\nM \u03b3\u2032Apw\u2032 \u2212 \u2192\u2217\nM \u03b5q\u2032\u03b5\nAs the stack eventually has to become empty, there has to be some\nfactorization w\u2032 = uw\u2032\u2032 and a state r \u2208 Q s.t.\nApu \u2212 \u2192\u2217\nM \u03b5r\u03b5 \u22a5q0w \u2212 \u2192\u2217\nM \u03b3\u2032Apw\u2032 \u2212 \u2192\u2217\nM \u03b3\u2032rw\u2032\u2032 \u2212 \u2192\u2217\nM \u03b5q\u2032\u03b5\n(The intuition is to read a rule ((p, a, A), (p\u2032, BC)) as a call of the procedure A with\np the global memory state and a the current input which leads to the updated global\nmemory state p\u2032 followed by recursive calls of first B and then C. The partial run\nApu \u2212 \u2192\u2217\nM \u03b5r\u03b5 thus means that a call of A in state p completely consumes the input u\nand returns the state r finally.)",
      "p the global memory state and a the current input which leads to the updated global\nmemory state p\u2032 followed by recursive calls of first B and then C. The partial run\nApu \u2212 \u2192\u2217\nM \u03b5r\u03b5 thus means that a call of A in state p completely consumes the input u\nand returns the state r finally.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-free languages and PDAs 153 (441)\nT The languages accepted by (nondeterministic) PDAs (on empty stack) are\nexactly the context-free languages.\n\u25b7 From PDA M = (Q, \u03a3, \u0393, \u03b4, q0, \u22a5, F) to grammar G = (V, \u03a3, P, S):\nWe therefore let the nonterminal Ap,r produce exactly the language\n{u \u2208 \u03a3\u2217 | Apu \u2212 \u2192\u2217\nM r}\nTo this end, for each transition ((p, a, A), (p\u2032, \u03b3)) \u2208 \u03b4 with a \u2208 {\u03b5} \u222a\u03a3:\n\u25b7 if \u03b3 = \u03b5, add Ap,p\u2032 \u2212 \u2192G a\n\u25b7 if \u03b3 = B, add Ap,r \u2212 \u2192G aBp\u2032,r for all r \u2208 Q\n\u25b7 if \u03b3 = BC, add Ap,r \u2212 \u2192G aBp\u2032,r\u2032Cr\u2032,r for all r\u2032, r\u2208 Q.\n(Reduce |\u03b3| > 2 to a sequence of calls as for grammars:\n[BCD]p,r \u2212 \u2192G Bp,q[CD]q,r.)\ni.e. the rules guess the intermediate states M enters while emptying its stack\n(as in the composition of binary relations) .\nFinally, add S \u2212 \u2192G \u22a5q0,q for all q \u2208 Q (i.e. guess the terminal state q). Then\nL(G) = L\u03b5(M). (Again, the formal proof requires induction.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-free languages and PDAs 153 (442)\nT The languages accepted by (nondeterministic) PDAs (on empty stack) are\nexactly the context-free languages.\nD A context-free grammar G = (V, \u03a3, P, S) is simple if P \u2286 V \u00d7 \u03a3V \u2217 (Greibach\nnormal form) and for every pair (X, a) \u2208 V \u00d7 \u03a3 there is at most one applicable\nrule (unambiguous).\n\u25b7 E.g. every DFA yields a simple grammar;\n\u25b7 E.g. F \u2212 \u2192G \u00acF | \u2228F F| \u2227F F| p | q is simple.\n\u25b7 A simple grammar can obviously be accepted by a DPDA:\nThe production (A, a\u03b3) becomes the transition ((q0, a, A), (q0, \u03b3)).\n\u25b7 As we will see, not all context-free languages can be accepted by an DPDA.\n(In contrast to FAs and 1TMs; recall, for LBAs we do not know so far.)\nThe subset of languages accepted by DPDAs is called deterministic\ncontext-free languages and it is a strict subset of the context-free languages.\n\u25b7 In general, recursive descent therefore needs to use backtracking in order to do\na search/enumeration over all possible derivation trees, while deterministic",
      "context-free languages and it is a strict subset of the context-free languages.\n\u25b7 In general, recursive descent therefore needs to use backtracking in order to do\na search/enumeration over all possible derivation trees, while deterministic\ncontext-free languages can be parsed in linear time (cf. CYK, LR parser).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From simple context-free grammar to DPDA 154 (443)\nS \u2212 \u2192G aSS S \u2212 \u2192G bS S \u2212 \u2192G c\nq0start\na, S: SS\nb, S: S\nc, S: \u03b5\n\u25b7 In case of a simple grammar, every terminal already determines at most one applicable rule,\nso there is no need to guess.\nq0\n. . .\u25a1 a b c c \u25a1 . . .\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From simple context-free grammar to DPDA 154 (444)\nS \u2212 \u2192G aSS S \u2212 \u2192G bS S \u2212 \u2192G c\nq0start\na, S: SS\nb, S: S\nc, S: \u03b5\n\u25b7 In case of a simple grammar, every terminal already determines at most one applicable rule,\nso there is no need to guess.\nq0\n. . .\u25a1 a b c c \u25a1 . . .\nS\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From simple context-free grammar to DPDA 154 (445)\nS \u2212 \u2192G aSS S \u2212 \u2192G bS S \u2212 \u2192G c\nq0start\na, S: SS\nb, S: S\nc, S: \u03b5\n\u25b7 In case of a simple grammar, every terminal already determines at most one applicable rule,\nso there is no need to guess.\nq0\n. . .\u25a1 a b c c \u25a1 . . .\nS\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From simple context-free grammar to DPDA 154 (446)\nS \u2212 \u2192G aSS S \u2212 \u2192G bS S \u2212 \u2192G c\nq0start\na, S: SS\nb, S: S\nc, S: \u03b5\n\u25b7 In case of a simple grammar, every terminal already determines at most one applicable rule,\nso there is no need to guess.\nq0\n. . .\u25a1 a b c c \u25a1 . . .\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From simple context-free grammar to DPDA 154 (447)\nS \u2212 \u2192G aSS S \u2212 \u2192G bS S \u2212 \u2192G c\nq0start\na, S: SS\nb, S: S\nc, S: \u03b5\n\u25b7 In case of a simple grammar, every terminal already determines at most one applicable rule,\nso there is no need to guess.\nq0\n. . .\u25a1 a b c c \u25a1 . . .\n\u03b5\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "6 Context-free languages\nContext-free grammars: basic properties\nPush-down automata\nDerivation trees\nPumping lemma and closure properties\nAlgebraic equations and EBNF (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (449)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, \u03b5) (S, aSb)\n(S, \u03b5)\n1.\n2.\n3.\n4.\n5.\n6.\nS \u2212 \u2192G SS\n\u2212 \u2192G SaSb \u2212 \u2192G aSbaSb \u2212 \u2192G aSbaaSbb \u2212 \u2192G aSbaabb \u2212 \u2192G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, \u03a3, P, S) is an\nordered tree whose nodes are labeled by rules (X, \u03b3) \u2208 P s.t.\nif \u03b3 = w0X1w1 . . . Xrwr with w0, w1, . . . , wr \u2208 \u03a3\u2217 and X1, . . . , Xr \u2208 V ,\nthen the i-th child from the left is labeled by a rule (Xi, \u03b3i) \u2208 P.\nF Every derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, \u03b3) defines a unique\nleftmost derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (450)\n(S, SS)\n(S, aSb)\n(S, aSb)\n(S, \u03b5) (S, aSb)\n(S, \u03b5)\n1.\n2.\n3.\n4.\n5.\n6.\nS \u2212 \u2192G SS \u2212 \u2192G SaSb\n\u2212 \u2192G aSbaSb \u2212 \u2192G aSbaaSbb \u2212 \u2192G aSbaabb \u2212 \u2192G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, \u03a3, P, S) is an\nordered tree whose nodes are labeled by rules (X, \u03b3) \u2208 P s.t.\nif \u03b3 = w0X1w1 . . . Xrwr with w0, w1, . . . , wr \u2208 \u03a3\u2217 and X1, . . . , Xr \u2208 V ,\nthen the i-th child from the left is labeled by a rule (Xi, \u03b3i) \u2208 P.\nF Every derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, \u03b3) defines a unique\nleftmost derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (451)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, \u03b5) (S, aSb)\n(S, \u03b5)\n1.\n2.\n3.\n4.\n5.\n6.\nS \u2212 \u2192G SS \u2212 \u2192G SaSb \u2212 \u2192G aSbaSb\n\u2212 \u2192G aSbaaSbb \u2212 \u2192G aSbaabb \u2212 \u2192G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, \u03a3, P, S) is an\nordered tree whose nodes are labeled by rules (X, \u03b3) \u2208 P s.t.\nif \u03b3 = w0X1w1 . . . Xrwr with w0, w1, . . . , wr \u2208 \u03a3\u2217 and X1, . . . , Xr \u2208 V ,\nthen the i-th child from the left is labeled by a rule (Xi, \u03b3i) \u2208 P.\nF Every derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, \u03b3) defines a unique\nleftmost derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (452)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, \u03b5)\n(S, aSb)\n(S, \u03b5)\n1.\n2.\n3.\n4.\n5.\n6.\nS \u2212 \u2192G SS \u2212 \u2192G SaSb \u2212 \u2192G aSbaSb \u2212 \u2192G aSbaaSbb\n\u2212 \u2192G aSbaabb \u2212 \u2192G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, \u03a3, P, S) is an\nordered tree whose nodes are labeled by rules (X, \u03b3) \u2208 P s.t.\nif \u03b3 = w0X1w1 . . . Xrwr with w0, w1, . . . , wr \u2208 \u03a3\u2217 and X1, . . . , Xr \u2208 V ,\nthen the i-th child from the left is labeled by a rule (Xi, \u03b3i) \u2208 P.\nF Every derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, \u03b3) defines a unique\nleftmost derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (453)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, \u03b5)\n(S, aSb)\n(S, \u03b5)\n1.\n2.\n3.\n4.\n5.\n6.\nS \u2212 \u2192G SS \u2212 \u2192G SaSb \u2212 \u2192G aSbaSb \u2212 \u2192G aSbaaSbb \u2212 \u2192G aSbaabb\n\u2212 \u2192G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, \u03a3, P, S) is an\nordered tree whose nodes are labeled by rules (X, \u03b3) \u2208 P s.t.\nif \u03b3 = w0X1w1 . . . Xrwr with w0, w1, . . . , wr \u2208 \u03a3\u2217 and X1, . . . , Xr \u2208 V ,\nthen the i-th child from the left is labeled by a rule (Xi, \u03b3i) \u2208 P.\nF Every derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, \u03b3) defines a unique\nleftmost derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (454)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, \u03b5) (S, aSb)\n(S, \u03b5)\n1.\n2.\n3.\n4.\n5.\n6.\nS \u2212 \u2192G SS \u2212 \u2192G SaSb \u2212 \u2192G aSbaSb \u2212 \u2192G aSbaaSbb \u2212 \u2192G aSbaabb \u2212 \u2192G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, \u03a3, P, S) is an\nordered tree whose nodes are labeled by rules (X, \u03b3) \u2208 P s.t.\nif \u03b3 = w0X1w1 . . . Xrwr with w0, w1, . . . , wr \u2208 \u03a3\u2217 and X1, . . . , Xr \u2208 V ,\nthen the i-th child from the left is labeled by a rule (Xi, \u03b3i) \u2208 P.\nF Every derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, \u03b3) defines a unique\nleftmost derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (455)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, \u03b5) (S, aSb)\n(S, \u03b5)\n1.\n2.\n3.\n4.\n5.\n6.\nS \u2212 \u2192G SS\n\u2212 \u2192G aSbS \u2212 \u2192G abS \u2212 \u2192G abaSb \u2212 \u2192G abaaSbb \u2212 \u2192G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, \u03a3, P, S) is an\nordered tree whose nodes are labeled by rules (X, \u03b3) \u2208 P s.t.\nif \u03b3 = w0X1w1 . . . Xrwr with w0, w1, . . . , wr \u2208 \u03a3\u2217 and X1, . . . , Xr \u2208 V ,\nthen the i-th child from the left is labeled by a rule (Xi, \u03b3i) \u2208 P.\nF Every derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, \u03b3) defines a unique\nleftmost derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (456)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, \u03b5) (S, aSb)\n(S, \u03b5)\n1.\n2.\n3.\n4.\n5.\n6.\nS \u2212 \u2192G SS \u2212 \u2192G aSbS\n\u2212 \u2192G abS \u2212 \u2192G abaSb \u2212 \u2192G abaaSbb \u2212 \u2192G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, \u03a3, P, S) is an\nordered tree whose nodes are labeled by rules (X, \u03b3) \u2208 P s.t.\nif \u03b3 = w0X1w1 . . . Xrwr with w0, w1, . . . , wr \u2208 \u03a3\u2217 and X1, . . . , Xr \u2208 V ,\nthen the i-th child from the left is labeled by a rule (Xi, \u03b3i) \u2208 P.\nF Every derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, \u03b3) defines a unique\nleftmost derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (457)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, \u03b5) (S, aSb)\n(S, \u03b5)\n1.\n2.\n3.\n4.\n5.\n6.\nS \u2212 \u2192G SS \u2212 \u2192G aSbS \u2212 \u2192G abS\n\u2212 \u2192G abaSb \u2212 \u2192G abaaSbb \u2212 \u2192G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, \u03a3, P, S) is an\nordered tree whose nodes are labeled by rules (X, \u03b3) \u2208 P s.t.\nif \u03b3 = w0X1w1 . . . Xrwr with w0, w1, . . . , wr \u2208 \u03a3\u2217 and X1, . . . , Xr \u2208 V ,\nthen the i-th child from the left is labeled by a rule (Xi, \u03b3i) \u2208 P.\nF Every derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, \u03b3) defines a unique\nleftmost derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (458)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, \u03b5) (S, aSb)\n(S, \u03b5)\n1.\n2.\n3.\n4.\n5.\n6.\nS \u2212 \u2192G SS \u2212 \u2192G aSbS \u2212 \u2192G abS \u2212 \u2192G abaSb\n\u2212 \u2192G abaaSbb \u2212 \u2192G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, \u03a3, P, S) is an\nordered tree whose nodes are labeled by rules (X, \u03b3) \u2208 P s.t.\nif \u03b3 = w0X1w1 . . . Xrwr with w0, w1, . . . , wr \u2208 \u03a3\u2217 and X1, . . . , Xr \u2208 V ,\nthen the i-th child from the left is labeled by a rule (Xi, \u03b3i) \u2208 P.\nF Every derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, \u03b3) defines a unique\nleftmost derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (459)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, \u03b5) (S, aSb)\n(S, \u03b5)\n1.\n2.\n3.\n4.\n5.\n6.\nS \u2212 \u2192G SS \u2212 \u2192G aSbS \u2212 \u2192G abS \u2212 \u2192G abaSb \u2212 \u2192G abaaSbb\n\u2212 \u2192G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, \u03a3, P, S) is an\nordered tree whose nodes are labeled by rules (X, \u03b3) \u2208 P s.t.\nif \u03b3 = w0X1w1 . . . Xrwr with w0, w1, . . . , wr \u2208 \u03a3\u2217 and X1, . . . , Xr \u2208 V ,\nthen the i-th child from the left is labeled by a rule (Xi, \u03b3i) \u2208 P.\nF Every derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, \u03b3) defines a unique\nleftmost derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (460)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, \u03b5) (S, aSb)\n(S, \u03b5)\n1.\n2.\n3.\n4.\n5.\n6.\nS \u2212 \u2192G SS \u2212 \u2192G aSbS \u2212 \u2192G abS \u2212 \u2192G abaSb \u2212 \u2192G abaaSbb \u2212 \u2192G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, \u03a3, P, S) is an\nordered tree whose nodes are labeled by rules (X, \u03b3) \u2208 P s.t.\nif \u03b3 = w0X1w1 . . . Xrwr with w0, w1, . . . , wr \u2208 \u03a3\u2217 and X1, . . . , Xr \u2208 V ,\nthen the i-th child from the left is labeled by a rule (Xi, \u03b3i) \u2208 P.\nF Every derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, \u03b3) defines a unique\nleftmost derivation X \u2212 \u2192\u2217\nG w \u2208 \u03a3\u2217 via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "(Simple) derivation/parse trees 157 (461)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, \u03b5) (S, aSb)\n(S, \u03b5)\nS\nS S\nSa b\n\u03b5\nSa b\nSa b\n\u03b5\nD A (simple) derivation/parse/syntax tree is an ordered labeled tree s.t.:\n\u25b7 Every leaf is labeled by a nonterminal or the empty word.\n\u25b7 Every inner node is labeled by a variable X and\nif its children are labeled by \u03b30, \u03b31, . . . , \u03b3r \u2208 {\u03b5} \u222a\u03a3 \u222a V from left to right,\nthen (X, \u03b30\u03b31, . . . , \u03b3r) \u2208 P; and if \u03b3i = \u03b5, then r = 0 and (X, \u03b5) \u2208 P.\nF Extended and simple derivation trees are in bijection.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: propositional formulas 158 (462)\nS \u2212 \u2192G [S \u2227 S] | [S \u2228 S] | \u00acS | p | q\n(S, [S \u2227 S])\n(S, \u00acS)\n(S, p)\n(S, [S \u2228 S])\n(S, q) (S, p)\n\u2227\n\u00ac\np\n\u2228\nq p\nS\n[ \u2227 ]S\n\u00ac S\np\nS\n[ \u2228 ]S\nq\nS\np\n\u25b7 Most textbooks use simple derivation trees and also call them syntax trees.\n\u25b7 In case of formulas/expressions/terms/programs extended derivation trees are\ncloser to classic syntax trees than simple derivation trees.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Chomsky normal form and derivation trees 159 (463)\nS \u2212 \u2192G SS | AB | AT T \u2212 \u2192G SB A \u2212 \u2192G a B \u2212 \u2192G b\n(S, SS)\n(S, AB)\n(A, a) (B, b)\n(S, AT)\n(A, a) (T, SB)\n(S, AB) (B, b)\n(A, a) (B, b)\n\u25b7 Assume G is in (strict) Chomsky normal form, i.e. P \u2286 V \u00d7 (\u03a3 \u222a V V).\nThen an extended derivation T wrt. G will be a binary tree.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Chomsky normal form and derivation trees 159 (464)\nS \u2212 \u2192G SS | AB | AT T \u2212 \u2192G SB A \u2212 \u2192G a B \u2212 \u2192G b\n(S, SS)\n(S, AB)\n(A, a) (B, b)\n(S, AT)\n(A, a) (T, SB)\n(S, AB) (B, b)\n(A, a) (B, b)\n\u25b7 If w \u2208 L(G), then every derivation S \u2212 \u2192\u2217\nG w gives rise to a binary derivation\ntree with |w| leaves and thus |w| \u22121 inner nodes.\nS \u2212 \u2192G SS \u2212 \u2192G ABS \u2212 \u2192G aBS \u2212 \u2192G abS \u2212 \u2192G . . .\u2212 \u2192G abaabb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Chomsky normal form and derivation trees 159 (465)\nS \u2212 \u2192G SS | AB | AT T \u2212 \u2192G SB A \u2212 \u2192G a B \u2212 \u2192G b\n(S, SS)\n(S, AB)\n(A, a) (B, b)\n(S, AT)\n(A, a) (T, SB)\n(S, AB) (B, b)\n(A, a) (B, b)\n\u25b7 If we remove the root (S, UV), the derivation tree will be split into two\nderivation trees each representing a derivation U \u2212 \u2192\u2217\nG u and V \u2212 \u2192\u2217\nG v,\nrespectively, with w = uv.\nS \u2212 \u2192G SS S \u2212 \u2192\u2217\nG ab S \u2212 \u2192\u2217\nG aabbb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Chomsky normal form and derivation trees 159 (466)\nS \u2212 \u2192G SS | AB | AT T \u2212 \u2192G SB A \u2212 \u2192G a B \u2212 \u2192G b\n(S, SS)\n(S, AB)\n(A, a) (B, b)\n(S, AT)\n(A, a) (T, SB)\n(S, AB) (B, b)\n(A, a) (B, b)\n\u25b7 On the other hand, given a word w we can (i) choose some rule (S, UV), (ii)\nsplit (factorize) w = uv and (iii) recursively try compute to compute\nderivation trees for U \u2212 \u2192\u2217\nG u and V \u2212 \u2192\u2217\nG v.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Chomsky normal form and derivation trees 159 (467)\nS \u2212 \u2192G SS | AB | AT T \u2212 \u2192G SB A \u2212 \u2192G a B \u2212 \u2192G b\n(S, SS)\n(S, AB)\n(A, a) (B, b)\n(S, AT)\n(A, a) (T, SB)\n(S, AB) (B, b)\n(A, a) (B, b)\n\u25b7 Replacing the nondeterministic choice of (S, UV) and w = uv by an iteration\nover all rules and the length of w (u = w[:i], v = w[i:]), respectively,\ncombined with dynamic programming yields the CYK algorithm.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cocke-Younger-Kasami (CYK) algorithm 160 (468)\ndef CYK(w, V , P, \u03a3, S): # w = a1 . . . al\nfor i = 1, 2, . . . , l:\n# determine all X \u2208 V with X \u2212 \u2192G ai\nVi,i = {X | (X, ai) \u2208 P}\nfor d = 1, 2, . . . , l\u2212 1: # d = k \u2212 i\nfor i = 1, 2, . . . , l\u2212 d:\nk = i + d\nVi,k = \u2205\nfor j = i, i+ 1, . . . , k\u2212 1:\n# determine all X \u2208 P with X \u2212 \u2192G Y Z and Y \u2212 \u2192\u2217\nG ai . . . aj\n# and Z \u2212 \u2192\u2217\nG aj+1 . . . ak\nVi,k = Vi,k \u222a {X | (X, Y Z) \u2208 P, Y\u2208 Vi,j, Z\u2208 Vj+1,k}\nL Let G = (V, \u03a3, P, S) be in (strict) Chomsky normal form P \u2286 V \u00d7 (\u03a3 \u222a V V).\nFor w = a1 . . . al \u2208 \u03a3\u2217 \\ {\u03b5} we have Vi,k = {X \u2208 V | X \u2212 \u2192\u2217\nG ai . . . ak}.\nThus: w \u2208 L(G) iff S \u2208 V1,|w|\n\u25b7 Simply induction on l = |w|.\n! If we remember in Vi,k not only X but (X, Y Z, j), then the Vi,j will be a\ncompressed representation (actually a context-free grammar itself) of all\nderivation trees of w wrt. G: if (X, Y Z, j) \u2208 Vi,k, then Xi,k \u2212 \u2192Yi,jZj+1,k.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: example (might contain errors, check it for yourself) 161 (469)\nS \u2212 \u2192G AB | BC A \u2212 \u2192G BA | a B \u2212 \u2192G CC | b C \u2212 \u2192G AB | a\n15\n14 25\n13 24 35\n12 23 34 45\n11 22 33 44 55\nb a a b a\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: example (might contain errors, check it for yourself) 161 (470)\nS \u2212 \u2192G AB | BC A \u2212 \u2192G BA | a B \u2212 \u2192G CC | b C \u2212 \u2192G AB | a\n15\n14 25\n13 24 35\n12 23 34 45\n11 22 33 44 55\nb a a b a\nB A, C A, C B A, C\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: example (might contain errors, check it for yourself) 161 (471)\nS \u2212 \u2192G AB | BC A \u2212 \u2192G BA | a B \u2212 \u2192G CC | b C \u2212 \u2192G AB | a\n15\n14 25\n13 24 35\n12 23 34 45\n11 22 33 44 55\nb a a b a\nB A, C A, C B A, C\nS1, A1 B2 S3, C3 S4, A4\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: example (might contain errors, check it for yourself) 161 (472)\nS \u2212 \u2192G AB | BC A \u2212 \u2192G BA | a B \u2212 \u2192G CC | b C \u2212 \u2192G AB | a\n15\n14 25\n13 24 35\n12 23 34 45\n11 22 33 44 55\nb a a b a\nB A, C A, C B A, C\nS1, A1 B2 S3, C3 S4, A4\nB2 B4\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: example (might contain errors, check it for yourself) 161 (473)\nS \u2212 \u2192G AB | BC A \u2212 \u2192G BA | a B \u2212 \u2192G CC | b C \u2212 \u2192G AB | a\n15\n14 25\n13 24 35\n12 23 34 45\n11 22 33 44 55\nb a a b a\nB A, C A, C B A, C\nS1, A1 B2 S3, C3 S4, A4\nB2 B4\nS2, C2\nA3\nS4, A4\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: example (might contain errors, check it for yourself) 161 (474)\nS \u2212 \u2192G AB | BC A \u2212 \u2192G BA | a B \u2212 \u2192G CC | b C \u2212 \u2192G AB | a\n15\n14 25\n13 24 35\n12 23 34 45\n11 22 33 44 55\nb a a b a\nB A, C A, C B A, C\nS1, A1 B2 S3, C3 S4, A4\nB2 B4\nS2, C2\nA3\nS4, A4\nS1, A1\nS2, C2\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: example (might contain errors, check it for yourself) 161 (475)\nS \u2212 \u2192G AB | BC A \u2212 \u2192G BA | a B \u2212 \u2192G CC | b C \u2212 \u2192G AB | a\n15\n14 25\n13 24 35\n12 23 34 45\n11 22 33 44 55\nb a a b a\nB A A B C\nA1 C3\nB4\nS2\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: applications 162 (476)\nD A context-free grammar G is unambiguous if every word w \u2208 L(G) has\nexactly one derivation trees; otherwise G is ambiguous.\nL If a context-free language L is accepted by an DPDA, then there is also an\nunambiguous context-free grammar G with L = L(G).\n\u25b7 Simply translate the DPDA into a grammar as discussed:\nAs there is at most once accepting run, the grammar will have at most one\nderivation tree by construction.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: applications 162 (477)\nD A context-free grammar G is unambiguous if every word w \u2208 L(G) has\nexactly one derivation trees; otherwise G is ambiguous.\nL If a context-free language L is accepted by an DPDA, then there is also an\nunambiguous context-free grammar G with L = L(G).\n\u25b7 Remarks:\n\u25b7 For programming languages (including formulas/expressions/terms), we want\ndeterministic context-free languages to make parsing simpler:\n! CYK uses O(|w|3) time in general, whereas deterministic context-free\nlanguages can be parsed in time O(|w|) (cf. LR parser).\n\u25b7 But we also want unambiguous grammars as the derivation tree actually tells\nus how to interpret the input, e.g.:\na \u2228 (b \u2227 c) vs a \u2228 b \u2227 c vs (a \u2228 b) \u2227 c\nWhile operator precedence allows to avoid parentheses, it complicates\nrecovering the derivation tree.\nThe parentheses actually encode the derivation tree, and thus how the term\nshould be evaluated (cf. Dyck language, binary trees, Catalan numbers).",
      "a \u2228 (b \u2227 c) vs a \u2228 b \u2227 c vs (a \u2228 b) \u2227 c\nWhile operator precedence allows to avoid parentheses, it complicates\nrecovering the derivation tree.\nThe parentheses actually encode the derivation tree, and thus how the term\nshould be evaluated (cf. Dyck language, binary trees, Catalan numbers).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: applications 162 (478)\nD A context-free grammar G is unambiguous if every word w \u2208 L(G) has\nexactly one derivation trees; otherwise G is ambiguous.\nL If a context-free language L is accepted by an DPDA, then there is also an\nunambiguous context-free grammar G with L = L(G).\n\u25b7 Remarks:\n\u25b7 Context-free grammars have been used to model at least subsets of natural\nlanguages like English \u2013 Chomsky\u2019s original interest was linguistics.\n\u25b7 And while not all aspects of a natural language are context-free (see e.g.\ncross-serial dependencies), context-free grammars can capture many aspects.\n\u25b7 Also in case of natural language, the actually meaning (semantics) of a\nsentence can depend on its derivation tree:\n\u201cI saw the man on the hill with a telescope.\u201d\nUsing parentheses to visualize the derivation tree/dependencies, e.g.:\n[I [saw [the man] [with [a telescope]]]]\nvs [I [saw [[the man] [with [a telescope]]]]]",
      "sentence can depend on its derivation tree:\n\u201cI saw the man on the hill with a telescope.\u201d\nUsing parentheses to visualize the derivation tree/dependencies, e.g.:\n[I [saw [the man] [with [a telescope]]]]\nvs [I [saw [[the man] [with [a telescope]]]]]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: applications 162 (479)\nD A context-free grammar G is unambiguous if every word w \u2208 L(G) has\nexactly one derivation trees; otherwise G is ambiguous.\nL If a context-free language L is accepted by an DPDA, then there is also an\nunambiguous context-free grammar G with L = L(G).\n\u25b7 Remarks:\n\u25b7 For natural language processing, extending CYK so that it recovers all possible\nderivation trees is thus actually very useful.\n\u25b7 If we further extend each rule of a context-free grammar with a weight, e.g. a\nprobability, then CYK can e.g. also be extended to compute of a derivation tree\nminimal/maximal total weight (sum/product over all rules used in the tree).\n(As the derivation trees then represent multiplicative terms over a semiring, like the\nViterbi semiring ([0, 1], max, \u00b7, 0, 1), and multiplication distributes over addition in\nevery semiring.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "6 Context-free languages\nContext-free grammars: basic properties\nPush-down automata\nDerivation trees\nPumping lemma and closure properties\nAlgebraic equations and EBNF (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma 164 (481)\n(S, AX)\n(A, a)\n(X, Y B)\n(Y, AZ)\n(A, a)\n(Z, Y B)\n(Y, AZ)\n(A, a)\n(Z, AB)\n(A, a) (B, b) (B, b) (B, b)\nS \u2212 \u2192G AX\nX \u2212 \u2192G Y B\nY \u2212 \u2192G AZ\nZ \u2212 \u2192G Y B| AB\nA \u2212 \u2192G a\nB \u2212 \u2192G b\nS \u2212 \u2192\u2217G aY b\n\u2212 \u2192\u2217G a aY b b\n\u2212 \u2192\u2217G a a aab bb\n\u25b7 Recall: For every binary relation R \u2286 V \u00d7 V we have R\u2217 = R<|V | as (by the\npigeon-hole principle) every path of length \u2265 |V | has to contain a cycle.\n\u25b7 Analogously, if a derivation tree has a path of length \u2265 |V | from its root to\none of its leaves (i.e. the tree has height \u2265 |V |), then at least one variable\nhas to occur multiple times along this path.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma 164 (482)\n(S, AX)\n(A, a)\n(X, Y B)\n(Y, AZ)\n(A, a)\n(Z, Y B)\n(Y, AZ)\n(A, a)\n(Z, AB)\n(A, a) (B, b) (B, b) (B, b)\nS \u2212 \u2192G AX\nX \u2212 \u2192G Y B\nY \u2212 \u2192G AZ\nZ \u2212 \u2192G Y B| AB\nA \u2212 \u2192G a\nB \u2212 \u2192G b\nS \u2212 \u2192\u2217G aY b\n\u2212 \u2192\u2217G a aY b b\n\u2212 \u2192\u2217G a a aab bb\n\u25b7 This corresponds to a derivation of the form\nS \u2212 \u2192\u2217\nG xlY xr \u2212 \u2192\u2217\nG xlylY yrxr \u2212 \u2192\u2217\nG xlylzyrxr = w\nIn particular, we can repeat or skip the derivation Y \u2212 \u2192\u2217\nG ylY yr s.t.\nxlylkzyrkxr \u2208 L for all k \u2208 N 0\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma 165 (483)\n(S, AX)\n(A, a)\n(X, Y B)\n(Y, AZ)\n(A, a)\n(Z, Y B)\n(Y, AZ)\n(A, a)\n(Z, AB)\n(A, a) (B, b) (B, b) (B, b)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma 165 (484)\n(S, AX)\n(A, a)\n(X, Y B)\n(Y, AZ)\n(A, a)\n(Z, AB)\n(A, a) (B, b)\n(B, b)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma 165 (485)\n(S, AX)\n(A, a)\n(X, Y B)\n(Y, AZ)\n(A, a)\n(Z, Y B)\n(Y, AZ)\n(A, a)\n(Z, Y B)\n(B, b)\n(Y, AZ)\n(A, a)\n(Z, AB)\n(A, a) (B, b)\n(B, b)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma 165 (486)\n(S, AX)\n(A, a)\n(X, Y B)\n(Y, AZ)\n(A, a)\n(Z, Y B)\n(Y, AZ)\n(A, a)\n(Z, Y B)\n(B, b)\n(Y, AZ)\n(A, a)\n(Z, Y B)\n(B, b)\n(Y, AZ)\n(A, a)\n(Z, AB)\n(A, a) (B, b)\n(B, b)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma for context-free languages 166 (487)\nL If L \u2286 \u03a3\u2217 is context-free, then there exists a constant nL \u2208 \u03a3\u2217 s.t.:\nFor every word w \u2208 L with |w| \u2265nL there exists a factorization\nw = xlylzyrxr\ns.t.\n1 ylyr \u0338= \u03b5 2 |ylzyr| \u2264nL 3 xlyk\nl zyk\nr xr \u2208 L for all k \u2208 N 0\n\u25b7 If L is context-free, then there exists a context-free grammar G = (V, \u03a3, P, S)\nin Chomsky NF with L = L(G).\nEvery (extended) derivation tree of a word w \u2208 L \\ {\u03b5} is a binary tree with\n|w| leaves and |w| \u22121 inner nodes.\nThe height of a minimal binary tree with |w| leaves is \u2308log2 |w|\u2309\nSo, if |w| \u2265nL := 2|V |, then there has to be at least one path of length |V |\nfrom the root to a leaf of the derivation tree.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma for context-free languages 166 (488)\nL If L \u2286 \u03a3\u2217 is context-free, then there exists a constant nL \u2208 \u03a3\u2217 s.t.:\nFor every word w \u2208 L with |w| \u2265nL there exists a factorization\nw = xlylzyrxr\ns.t.\n1 ylyr \u0338= \u03b5 2 |ylzyr| \u2264nL 3 xlyk\nl zyk\nr xr \u2208 L for all k \u2208 N 0\n\u25b7 Pick a leaf that has maximal distance to the root, and let Y be the first\nnonterminal that appears a second time when ascending to the root. This\ngives rise to a derivation\nS \u2212 \u2192\u2217\nG xlY xr Y \u2212 \u2192\u2217\nG ylY yr Y \u2212 \u2192\u2217\nG z\nAs G is in Chomsky NF, we have ylyr \u0338= \u03b5.\nEvery path to a leaf from this second (from the bottom) occurrence of Y has\nlength at most |V |, so that |ylzyr| \u22642|V | = nL.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma for context-free languages 166 (489)\nL If L \u2286 \u03a3\u2217 is context-free, then there exists a constant nL \u2208 \u03a3\u2217 s.t.:\nFor every word w \u2208 L with |w| \u2265nL there exists a factorization\nw = xlylzyrxr\ns.t.\n1 ylyr \u0338= \u03b5 2 |ylzyr| \u2264nL 3 xlyk\nl zyk\nr xr \u2208 L for all k \u2208 N 0\n! The statement is trivially true if |L| < \u221e: simply choose\nnL := 1 + max{|w|: w \u2208 L}.\n! nL depends on the language resp. the underlying context-free grammar in\nChomsky NF. The factorization depends on w.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma for context-free languages 166 (490)\nL If L \u2286 \u03a3\u2217 is context-free, then there exists a constant nL \u2208 \u03a3\u2217 s.t.:\nFor every word w \u2208 L with |w| \u2265nL there exists a factorization\nw = xlylzyrxr\ns.t.\n1 ylyr \u0338= \u03b5 2 |ylzyr| \u2264nL 3 xlyk\nl zyk\nr xr \u2208 L for all k \u2208 N 0\n! Also a non-context-free language can satisfy the pumping lemma for\ncontext-free languages. But a language that does not satisfy the pumping\nlemma, can never be context-free:\n\u25b7 Assume L is context-free.\n\u25b7 Then fix some nL \u2208 N with above properties, nothing else is known about nL.\n\u25b7 Depending on nL fix a concrete word w \u2208 L.\n\u25b7 Finally show that every possible factorization of w violates at least one of the\nproperties 1 , 2 , 3 .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma for context-free languages 166 (491)\nL If L \u2286 \u03a3\u2217 is context-free, then there exists a constant nL \u2208 \u03a3\u2217 s.t.:\nFor every word w \u2208 L with |w| \u2265nL there exists a factorization\nw = xlylzyrxr\ns.t.\n1 ylyr \u0338= \u03b5 2 |ylzyr| \u2264nL 3 xlyk\nl zyk\nr xr \u2208 L for all k \u2208 N 0\nC If L \u2286 \u03a3\u2217 is regular, then there exists a constant nL \u2208 \u03a3\u2217 s.t.:\nFor every word w \u2208 L with |w| \u2265nL there exists a factorization\nw = xlylz\ns.t.\n1 yl \u0338= \u03b5 2 |ylz| \u2264nL 3 xlyk\nl z \u2208 L for all k \u2208 N 0\n\u25b7 As then L = L(G) for some regular (=right-linear context-free) grammar G,\ni.e. all rules are of the form X \u2192 aY or X \u2192 a, so that xryr = \u03b5.\nEvery (extended) derivation tree is just a path, hence we can choose\nnL = |V | + 1. (Alternatively consider a DFA that accepts L. Then every accepting\nrun of length \u2265 |Q| has to contain a cycle that can be repeated/\u201cpumped\u201d.)No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma: {akbkck | k \u2208 N 0} 167(492)\n\u25b7 The pumping lemma is used to argue that a language is not context-free.\n\u25b7 Consider L := {akbkck} and assume that L is context-free:\nThen there exists nL \u2208 N 0 s.t. for every word w \u2208 L with |w| \u2265nL there\nexists a factorization w = xlylzyrxr s.t.:\n1 ylyr \u0338= \u03b5 2 |ylzyr| \u2264nL 3 xlyk\nl zyk\nr xr \u2208 L for all k \u2208 N 0\nThe goal is to show a contradiction.\n\u25b7 2 means that the \u201cpumpable\u201d part has to be within a window of nL letters.\n\u25b7 As akbkck has a specific block structure, we should choose k so large, that\nylzyr can range over at most two blocks/kinds of terminals.\nE.g. choose w = anLbnLcnL.\n! We do not need to know the precise value of nL, it suffices that we know\nthat there is some nL \u2208 N under the assumption that L is context-free.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma: {akbkck | k \u2208 N 0} 167(493)\n\u25b7 The pumping lemma is used to argue that a language is not context-free.\n\u25b7 Consider L := {akbkck} and assume that L is context-free:\nThen there exists nL \u2208 N 0 s.t. for every word w \u2208 L with |w| \u2265nL there\nexists a factorization w = xlylzyrxr s.t.:\n1 ylyr \u0338= \u03b5 2 |ylzyr| \u2264nL 3 xlyk\nl zyk\nr xr \u2208 L for all k \u2208 N 0\nThe goal is to show a contradiction.\n\u25b7 We have to argue that there is no factorization of w = anLbnLcnL that\nsatisfies all properties 1 , 2 , 3 at the same time.\n\u25b7 Because of 2 we have to have\nylzyr = aibj or ylzyr = bicj\nfor some i, j\u2208 N 0 s.t. i + j \u2264 nL.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma: {akbkck | k \u2208 N 0} 167(494)\n\u25b7 The pumping lemma is used to argue that a language is not context-free.\n\u25b7 Consider L := {akbkck} and assume that L is context-free:\nThen there exists nL \u2208 N 0 s.t. for every word w \u2208 L with |w| \u2265nL there\nexists a factorization w = xlylzyrxr s.t.:\n1 ylyr \u0338= \u03b5 2 |ylzyr| \u2264nL 3 xlyk\nl zyk\nr xr \u2208 L for all k \u2208 N 0\nThe goal is to show a contradiction.\n\u25b7 Assume ylzyr = aibj.\n\u25b7 Because of 1 yl or yr have to contain at least one a or one b, but neither\ncan contain a c.\n\u25b7 Thus, if we introduce additional copies of yl and yr, we increase the number\nof as or bs, while the number cs stays unchanged.\n\u25b7 So xlyk\nl zyk\nr xr \u0338\u2208 L for any k >1.\n\u25b7 Thus {akbkck | k \u2208 N 0} is not context-free, but only context-sensitive.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma: {uu | u \u2208 {a, b}\u2217} 168(495)\n\u25b7 For another example, we show that L := {uu | w \u2208 {a, b}\u2217} is not\ncontext-free.\n\u25b7 Again, assume that L is context-free in order to obtain a contradiction:\nThen there exists nL \u2208 N 0 s.t. for every word w \u2208 L with |w| \u2265nL there\nexists a factorization w = xlylzyrxr s.t.:\n1 ylyr \u0338= \u03b5 2 |ylzyr| \u2264nL 3 xlyk\nl zyk\nr xr \u2208 L for all k \u2208 N 0\nThe goal is again to pick a word w \u2208 L for which we can show that\n\u201cpumping\u201d will destroy the required structures.\n\u25b7 Let w = uu = anLbnLanLbnL with u = anLbnL\n\u25b7 Because of 2 , only three cases can arise: ylzyr is contained in the\n\u25b7 left anLbnL: then xly0\nl zy0\nrxr = akblanLbnL.\n\u25b7 right anLbnL: then xly0\nl zy0\nrxr = anLbnLakbl.\n\u25b7 middle bnLanL: then xly0\nl zy0\nrxr = anLbkalbnL.\nwith k < nL or l < nL as ylyr \u0338= \u03b5. Complete the proof.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Closure properties 169 (496)\nL Context-free languages are closed wrt. union, concatenation and Kleene star.\n\u25b7 Union: S \u2212 \u2192G S1 | S2 (as for any grammar type).\n\u25b7 Concatenation: S \u2212 \u2192G S1S2.\n\u25b7 Kleene star: S \u2212 \u2192G \u03b5 | S1S.\nC Context-free languages are not closed wrt. intersection or complement.\n\u25b7 as {akbkcl | k, l\u2208 N 0} \u2229 {akblcl | k, l\u2208 N 0} = {akbkck | k \u2208 N 0}.\n! Both languages are already deterministic context-free.\n\u25b7 And because of de Morgan: L1 \u2229 L2 = L1 \u222a L2.\n\u25b7 (Recall: intersection and complement are still context-sensitive.)\n! Show: The intersection of a context-free language with a regular language is\nstill context-free.\nL (w/o proof): Deterministic cf. languages are closed wrt. complement;\nbut not closed wrt. union, intersection, concatenation, Kleene star.\nC The det. cf. languages are a strict subset of the cf. languages.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Decidable properties 170 (497)\nL |L(G)| = 0 and |L(G)| = \u221e are decidable for context-free grammars:\n\u25b7 L(G) = \u2205 iff there is a derivation tree of height at most |V | \u22121.\n\u25b7 |L(G)| = \u221e iff there is a derivation tree of height at least |V |.\nL w \u2208 L(G) is decidable for context-free grammars (CYK).\nT Given two DPDA M1, M2 it is decidable if L(M1) = L(M2).\n(S\u00b4 enizergues, G\u00a8odel Prize 2002).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Undecidable properties 171 (498)\nT For two context-free grammars G1, G2\nL(G1)\u2229L(G2) = \u2205 |L(G1)\u2229L(G2)| = \u221e L(G1) \u2286 L(G2) L(G1) = L(G2)\nare all undecidable.\n\u25b7 We have shown the first two already by encoding a PCP instance\n(u(1), v(1)), . . . ,(u(n), v(n)) as two linear context-free grammars\nSu \u2212 \u2192Gu u(i)Suai | u(i)#ai resp. Sv \u2212 \u2192Gv v(i)Svai | v(i)#ai\nLet GR\nu and GR\nv be the grammars with the right-hand sides reversed, i.e.\n(X, \u03b3) is replaced by (X, \u03b3R).\nThen L1 := L(GR\nu ) and L2 = L(GR\nv ) are also deterministic.\nThus also L1 and L2 are deterministic context-free. Then:\nL1 \u2229 L2 = \u2205 iff L1 \u2286 L2 iff L1 \u222a L2 = L2\nwith L3 := L1 \u222a L2 still context-free.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Undecidable properties 171 (499)\nT For two context-free grammars G1, G2\nL(G1)\u2229L(G2) = \u2205 |L(G1)\u2229L(G2)| = \u221e L(G1) \u2286 L(G2) L(G1) = L(G2)\nare all undecidable.\nC For two deterministic context-free languages L1, L2\nL1 \u2229 L2 = \u2205 |L1 \u2229 L2| = \u221e L1 \u2286 L2\nare all undecidable.\nC For a context-free grammar G it is in general undecidable if L(G) = \u03a3\u2217.\n\u25b7 If L1, L2 are det. context-free, then L1 \u222a L2 is still context-free.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Undecidable properties 171 (500)\nT For two context-free grammars G1, G2\nL(G1)\u2229L(G2) = \u2205 |L(G1)\u2229L(G2)| = \u221e L(G1) \u2286 L(G2) L(G1) = L(G2)\nare all undecidable.\nT W/o proofs (see e.g. Sch \u00a8oning):\nFor two (det.) cf. languages it is undecidable if L1 \u2229 L2 is again context-free.\nFor a cf. grammar G it is undecidable if (i) G is ambiguous, (ii) L(G) is\ncontext-free, (iii) L(G) is regular, (iv) L(G) is deterministic context-free.\n\u25b7 Thus it is also undecidable for a context-sensitive grammar G if L(G) is\ncontext-free as L1 \u2229 L2 is context-sensitive.\n\u25b7 Recall: for a unrestricted grammar G it is undecidable if L(G) is\ncontext-sensitive.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Chomsky hierarchy: Comparison 172 (501)\n\u25b7 Closure properties (with 2d for deterministic context-free):\nType L1 \u222a L2 L1 \u2229 L2 L L1L2 L\u2217\n3 3 3 3 3 3\n2d 2 1 2d 2 2\n2 2 1 1 2 2\n1 1 1 1 1 1\n0 0 0 \u2212 0 0\n\u25b7 (Un)decidable problems:\nType w \u2208 L L = \u2205 L = \u03a3\u2217 L1 = L2 L1 \u2286 L2 L1 \u2229 L2 = \u2205\n3 d, O(|w|) d d d d d\n2d d, O(|w|) d d d u u\n2 d, O(|w|3) d u u u u\n1 d, O(2|w|) u u u u u\n0 u u u u u u\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "6 Context-free languages\nContext-free grammars: basic properties\nPush-down automata\nDerivation trees\nPumping lemma and closure properties\nAlgebraic equations and EBNF (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-free grammars as equations 174 (503)\n\u25b7 While regular grammars correspond to systems of linear fixed-point\nequations, context-free grammars correspond to systems of\npolynomial/algebraic fixed-point equations.\n\u25b7 E.g.\nS \u2212 \u2192G aSS | bS | c X = f(X) := aXX + bX + c\nAgain, L(G) is the least solution of X = f(X) which is the limit of the\nfixed-point iteration fk(0).\nIn the example\nf(0) = c f 2(0) = acc + bc + c . . .\n\u25b7 Induction shows that fk+1(0) is exactly the language of words that posses an\nextended derivation of height k.\nIn the example, the grammar is unambiguous and every word is the prefix\nencoding of its derivation tree, e.g. abcc\u02c6 =a(b(c), c).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended Backus-Naur form (EBNF) 175 (504)\n\u25b7 As context-free languages are closed wrt. regular/rational operations, i.e.\nunion, concatenation and Kleene star, we could (but won\u2019t!) allow regular\nexpression over both \u03a3 and V on the right-hand side, e.g.:\nS \u2212 \u2192G XY S| Y Z| V S \u2212 \u2192G (XY + Z)\u2217V\nThis requires that \u2217, (, ) and + are treated as meta symbols just like |.\n\u25b7 Alternatively, we could define context-free grammars using algebraic notation\nfor semirings extended by the Kleene star, but that makes the use of the then\nreserved meta symbols 0, 1, +, \u2217, (, ) cumbersome.\n\u25b7 In practice, extended Backus-Naur form (EBNF) is thus used e.g. for defining\nthe context-free grammars of programming languages.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended Backus-Naur form (EBNF) 176 (505)\n\u25b7 EBNF uses the following meta symbols with the intended semantics:\n\u25b7 Quotation marks \u201d or \u2032 are used to indicate terminals, cf. strings in Python.\n\u25b7 Unquoted text (can include spaces) is treated as nonterminals.\n\u25b7 \u03b3, \u03b3\u2032 (comma!) is used for concatenation, i.e. \u03b3\u03b3\u2032.\n\u25b7 \u03b3|\u03b3\u2032 is still used for choice, i.e. \u03b3 + \u03b3\u2032.\n\u25b7 X = \u03b3; is used for a production rule, i.e. X \u2212 \u2192G \u03b3.\n\u25b7 {\u03b3} stands for the Kleene star, i.e. \u03b3\u2217.\n\u25b7 [\u03b3] stands for \u201cat most once/optional\u201d, i.e. (\u03b3 + \u03b5)\n\u25b7 Parentheses (, ) are used for grouping as in regular expressions.\n\u25b7 k\u2217 \u03b3 with k \u2208 N stands for \u201c k copies\u201d, i.e. \u03b3k.\n! These rules still only allow to define context-free languages.\nNonZeroDigit = \u201d1\u201d | \u201d2\u201d | \u201d3\u201d | \u201d4\u201d | \u201d5\u201d | \u201d6\u201d | \u201d7\u201d | \u201d8\u201d | \u201d9\u201d ;\nDigit = \u201d0\u201d | NonZeroDigit ;\nNaturalNumber = NonZeroDigit, {Digit};\nInteger = \u201d0\u201d | [\u201d-\u201d], NaturalNumber ;\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended Backus-Naur form (EBNF) 176 (506)\n\u25b7 EBNF uses the following meta symbols with the intended semantics:\n\u25b7 Quotation marks \u201d or \u2032 are used to indicate terminals, cf. strings in Python.\n\u25b7 Unquoted text (can include spaces) is treated as nonterminals.\n\u25b7 \u03b3, \u03b3\u2032 (comma!) is used for concatenation, i.e. \u03b3\u03b3\u2032.\n\u25b7 \u03b3|\u03b3\u2032 is still used for choice, i.e. \u03b3 + \u03b3\u2032.\n\u25b7 X = \u03b3; is used for a production rule, i.e. X \u2212 \u2192G \u03b3.\n\u25b7 {\u03b3} stands for the Kleene star, i.e. \u03b3\u2217.\n\u25b7 [\u03b3] stands for \u201cat most once/optional\u201d, i.e. (\u03b3 + \u03b5)\n\u25b7 Parentheses (, ) are used for grouping as in regular expressions.\n\u25b7 k\u2217 \u03b3 with k \u2208 N stands for \u201c k copies\u201d, i.e. \u03b3k.\n! EBNF also knows \u2212 (minus) for set difference, but recall that in general\n\u25b7 L1 \\ L2 is not context-free anymore; but if L2 is regular, L1 \\ L2 = L1 \u2229 L2 is\nguaranteed to remain context-free.\n\u25b7 It is undecidable, if a variable X of a cf. grammar produces a regular language;",
      "! EBNF also knows \u2212 (minus) for set difference, but recall that in general\n\u25b7 L1 \\ L2 is not context-free anymore; but if L2 is regular, L1 \\ L2 = L1 \u2229 L2 is\nguaranteed to remain context-free.\n\u25b7 It is undecidable, if a variable X of a cf. grammar produces a regular language;\nbut we can check, if the grammar X refers to is regular.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "7 Complexity theory: deterministic vs. nondeterministic polynomial time\nResource-bounded computation\nPolynomially bounded computation\nP vs NP\nNP-complete problems\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Measuring time and space 179 (509)\n\u25b7 So far, we have mostly considered the question if some problem is\n(semi)decidable/computable or not. Recall:\n\u25b7 Computability of a (partial) function f : {0, 1}\u2217 ,\u2192 {0, 1}\u2217$ can be reduced to\nthe (semi)decidability of its \u201cgraph\u201d {x#y | f(x) = yz}.\n\u25b7 If L is given by a DPDA, we can decide w \u2208 L in time and space O(|w|).\n\u25b7 If L is given by cf. grammar in Chomsky NF, we can decide w \u2208 L using CYK\nin time and space O(|w|3).\n\u25b7 If L is given by a context-sensitive grammar (type 1), i.e. we impose a linear\nspace bound (LBA), w \u2208 L can be decided in time and space 2O(|w|) using\nBFS (and nondeterministically in space O(|w|)).\n\u25b7 If L is given by a unrestricted grammar (type 0), then w \u2208 L is only\nsemidecidable. In particular, there cannot be a computable time or space\nbound.\n\u25b7 Motivated by the word problem, the natural way of measuring time and space\nused is wrt. the length |w| of the input w.",
      "\u25b7 If L is given by a unrestricted grammar (type 0), then w \u2208 L is only\nsemidecidable. In particular, there cannot be a computable time or space\nbound.\n\u25b7 Motivated by the word problem, the natural way of measuring time and space\nused is wrt. the length |w| of the input w.\n! For an integer n \u2208 N its input length is usually the length of its binary\nrepresentation, i.e. |lsbf(n)| = |msbf(n)| \u2248log2 n.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: Landau symbols 180 (510)\n\u25b7 Reminder: If f, gare continuous and lim supx\u2192\u221e\nf(x)\ng(x) < \u221e, then f \u2208 O(g).\n\u25b7 Reminder: If f, gare continuous and limx\u2192\u221e\nf(x)\ng(x) = 0, then f \u2208 o(g), e.g.:\nlog(log(x)) <o log(x) <o\n\u221ax <o x <o x log(x) <o x2 <o x3 <o e\n\u221ax <o ex\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Multi-tape Turing machines 181 (511)\n\u25b7 In short, a k-tape TM (Q, \u03a3, \u0393, \u03b4, q0, \u25a1, F) is a 1TM with a vector alphabet\n\u0393k but k independent heads, i.e.:\n\u03b4 \u2286 (Q \u00d7 \u0393k) \u00d7 (Q \u00d7 (\u0393 \u00d7 {\u22121, 0, +1})k)\nA k-tape TM can be thought of a k-core CPU (choose\nQ = Q0 \u00d7 Q1 \u00d7 . . .\u00d7 Qk).\n\u25b7 \u201cTM\u201d short for \u201cmulti-tape TM\u201d, if the concrete k does not matter.\nCan be nondeterministic; also NTM for emphasis.\n\u25b7 \u201cDTM\u201d short for \u201cdeterministic multi-tape TM\u201d\n\u25b7 Often multi-tape TMs have an exclusive read-only input tape.\n(Sometimes also an exclusive write-only output tape.)\n\u25b7 Multi-tape TMs are just a very simple and convenient formalism to abstract\nfrom a concrete CPU model (number of cores, clock speed, architecture, . . . )\nwhen considering resource-bounded computations.\nIn the end, a 1TM can simulate a single step of k-tape TM by means of a\nvector alphabet and a sweep over the complete tape content.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Resource-bounded computation 182 (512)\nD Let f : N 0 \u2192 N 0 be some function, and M some (multi-tape) TM.\nM is f-time-bounded if there is some f\u2032 \u2208 O(f) s.t. M terminates for all\ninputs w \u2208 \u03a3\u2217 after at most f\u2032(|w|) steps.\nM is f-space-bounded if there is some f\u2032 \u2208 O(f) s.t. M for all inputs\nw \u2208 \u03a3\u2217 moves every head at most f\u2032(|w|) positions from the start.\n\u25b7 In case of space-bounded computation, we sometimes need to use a separate\nread-only input tape, so that we can bound the space used for the actual\ncomputation.\n\u25b7 The convention is to treat f : R \u2192 R as \u2308|f(|w|)|\u2309, so that we can also use\nf(n) = n log(n) or f(n) = 2n or f(n) = 2\n\u221an as bounds.\n\u25b7 For time-bounded computation, if f(n) < n(sublinear), then the TM cannot\nread the complete input w, so the output can only depend on a sublinear\nprefix of w.\n\u25b7 For space-bounded computation, sublinear bounds like log(n) (excluding the\nexclusive input tape) still allow an NTM to decide graph reachability.",
      "read the complete input w, so the output can only depend on a sublinear\nprefix of w.\n\u25b7 For space-bounded computation, sublinear bounds like log(n) (excluding the\nexclusive input tape) still allow an NTM to decide graph reachability.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Resource-bounded computation 182 (513)\nD Let f : N 0 \u2192 N 0 be some function, and M some (multi-tape) TM.\nM is f-time-bounded if there is some f\u2032 \u2208 O(f) s.t. M terminates for all\ninputs w \u2208 \u03a3\u2217 after at most f\u2032(|w|) steps.\nM is f-space-bounded if there is some f\u2032 \u2208 O(f) s.t. M for all inputs\nw \u2208 \u03a3\u2217 moves every head at most f\u2032(|w|) positions from the start.\n\u25b7 The use of O(f) allows to abstract from the concrete TM (CPU):\n\u25b7 The (non-unary) input alphabet \u03a3 only matters up to a factor log2 |\u03a3| (\u201c32bit\nvs. 64bit\u201d).\n\u25b7 By the Linear speedup theorem a k-tape TM with a t-time bound and an\ns-space bound can be simulated by t\nc -time, s-space bounded TM for any\nchoice of c >1 (\u201c2GHz vs. 4GHz\u201d).\n\u25b7 A t-time, s-space bounded k-tape TM can be simulated by a t2-time, s-space\nbounded 1TM using a vector tape alphabet (\u201c1 core vs. k cores\u201d).\n\u25b7 In all cases, if the original TM was deterministic, then also the simulator will\nbe deterministic; in particular, for space-bounded computation, there is no",
      "bounded 1TM using a vector tape alphabet (\u201c1 core vs. k cores\u201d).\n\u25b7 In all cases, if the original TM was deterministic, then also the simulator will\nbe deterministic; in particular, for space-bounded computation, there is no\ndifference (cf. LBA).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Resource-bounded computation 182 (514)\nD Let f : N 0 \u2192 N 0 be some function, and M some (multi-tape) TM.\nM is f-time-bounded if there is some f\u2032 \u2208 O(f) s.t. M terminates for all\ninputs w \u2208 \u03a3\u2217 after at most f\u2032(|w|) steps.\nM is f-space-bounded if there is some f\u2032 \u2208 O(f) s.t. M for all inputs\nw \u2208 \u03a3\u2217 moves every head at most f\u2032(|w|) positions from the start.\n! In particular for an NTM M where we may have multiple runs:\n\u25b7 If M is f-time-bounded, then every run of M on input |w| has length at most\nf(|w|).\n\u25b7 If M is f-space-bounded, then every run of M on input |w| has to terminate\nor become cyclic after at most (Q \u00d7 \u0393)f(|w|) steps.\nF If M is f-time-bounded, it is also f-space-bounded.\n\u25b7 As M can move its tape heads by at most one position in every step.\nF If M is f-space-bounded, it can be made 2f -time-bounded (BFS),\nat least if f is constructible.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Resource-bounded computation 182 (515)\nD Let f : N 0 \u2192 N 0 be some function, and M some (multi-tape) TM.\nM is f-time-bounded if there is some f\u2032 \u2208 O(f) s.t. M terminates for all\ninputs w \u2208 \u03a3\u2217 after at most f\u2032(|w|) steps.\nM is f-space-bounded if there is some f\u2032 \u2208 O(f) s.t. M for all inputs\nw \u2208 \u03a3\u2217 moves every head at most f\u2032(|w|) positions from the start.\n\u25b7 If M is f-time/space-bounded, then M can be turned into a\nf-time/space-bounded TM that on input 1n outputs 1m with m \u2264 f(n).\nD f : N 0 \u2192 N 0 is time/space-constructible if there exists a\nf-time/space-bounded (multi-tape) DTM that on input 1n outputs 1f(n).\n\u25b7 For space-bounds f(n) < nan exclusive read-only input tape is used and the\nadditional space is bounded by f.\nF If f is time/space-constructible, then we can impose a f-time/space-bound\non every other TM.\nF Every polynomial p \u2208 Z [X] is time/space constructible.\nAlso n log(n) and n\u221an and 2n are space/time-constructible.",
      "additional space is bounded by f.\nF If f is time/space-constructible, then we can impose a f-time/space-bound\non every other TM.\nF Every polynomial p \u2208 Z [X] is time/space constructible.\nAlso n log(n) and n\u221an and 2n are space/time-constructible.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Resource-bounded computation 182 (516)\nD Let f : N 0 \u2192 N 0 be some function, and M some (multi-tape) TM.\nM is f-time-bounded if there is some f\u2032 \u2208 O(f) s.t. M terminates for all\ninputs w \u2208 \u03a3\u2217 after at most f\u2032(|w|) steps.\nM is f-space-bounded if there is some f\u2032 \u2208 O(f) s.t. M for all inputs\nw \u2208 \u03a3\u2217 moves every head at most f\u2032(|w|) positions from the start.\nT As mentioned already for LBAs (w/o proofs):\nSavitch\u2019s theorem:\nFor every f-space-bounded NTM M, there exists a f2-space-bounded DTM\nM\u2032 s.t. L(M) = L(M\u2032).\nImmerman\u2013Szelepcs\u00b4 enyi theorem:\nFor every f-space-bounded NTM M with f(|w|) \u2265 log(|w|), there exists a\nf-space-bounded NTM M\u2032 s.t. L(M\u2032) = L(M).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Resource-bounded computation 182 (517)\nD Let f : N 0 \u2192 N 0 be some function, and M some (multi-tape) TM.\nM is f-time-bounded if there is some f\u2032 \u2208 O(f) s.t. M terminates for all\ninputs w \u2208 \u03a3\u2217 after at most f\u2032(|w|) steps.\nM is f-space-bounded if there is some f\u2032 \u2208 O(f) s.t. M for all inputs\nw \u2208 \u03a3\u2217 moves every head at most f\u2032(|w|) positions from the start.\n! In case of time-bounded computation, we do not know in general\n\u25b7 if nondeterminism can be simulated by determinism with less than an\nexponential blow up, and\n\u25b7 if nondeterminism can be complemented more efficiently then via determinism.\n\u25b7 The class of \u201ctractable decision problem\u201d is thus defined as the set of all\nlanguages L that can be decided by some polynomial-time-bounded DTM.\n\u25b7 Polynomial bounds have the advantage that for p, q\u2208 Z [X] also\np + q, pq, p\u25e6 q \u2208 Z [X]:\nE.g. if a p-time bounded DTM computes w\u2032 from w and then calls a q-time\nbounded DTM on w\u2032 to obtain w\u2032\u2032, we have |w\u2032\u2032| \u2264q(|w\u2032|) \u2264 q(p(|w|)).",
      "languages L that can be decided by some polynomial-time-bounded DTM.\n\u25b7 Polynomial bounds have the advantage that for p, q\u2208 Z [X] also\np + q, pq, p\u25e6 q \u2208 Z [X]:\nE.g. if a p-time bounded DTM computes w\u2032 from w and then calls a q-time\nbounded DTM on w\u2032 to obtain w\u2032\u2032, we have |w\u2032\u2032| \u2264q(|w\u2032|) \u2264 q(p(|w|)).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "7 Complexity theory: deterministic vs. nondeterministic polynomial time\nResource-bounded computation\nPolynomially bounded computation\nP vs NP\nNP-complete problems\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Complexity classes for decision problems 184 (519)\nD For simplicity, let L \u2286 {0, 1}\u2217 denote a binary language:\n\u25b7 DTIME(f) is the set of all L decidable by a f-time bounded DTM.\n\u25b7 NTIME(f) is the set of all L decidable by a f-time bounded NTM.\n\u25b7 DSPACE(f) is the set of all L decidable by a f-space bounded DTM.\n\u25b7 NSPACE(f) is the set of all L decidable by a f-space bounded NTM.\n\u25b7 P := S\nk\u2208N 0 DTIME(nk) (polynomial time)\n\u25b7 NP := S\nk\u2208N 0 NTIME(nk) (nondeterministic polynomial time)\n\u25b7 PSPACE := S\nk\u2208N 0 DSPACE(nk) (polynomial space)\nF P \u2286 NP \u2286 PSPACE\nL By Savitch\u2019s theorem: PSPACE = S\nk\u2208N 0 NSPACE(nk)\n\u25b7 Let \u230aG, w\u2309 denote some fixed binary encoding of a grammar G = (V, \u03a3, P, S)\nand a word w \u2208 \u03a3\u2217. Then the word problem for languages of type \u03c4 is the\nlanguage L\u03c4 = {\u230aG, w\u2309 |G = (V, \u03a3, P, S) is of type \u03c4; w \u2208 L(G)}.\nAs seen: L2 \u2208 P and L1 \u2208 PSPACE and L0 \u0338\u2208 PSPACE.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Complexity classes for decision problems 184 (520)\nD For simplicity, let L \u2286 {0, 1}\u2217 denote a binary language:\n\u25b7 DTIME(f) is the set of all L decidable by a f-time bounded DTM.\n\u25b7 NTIME(f) is the set of all L decidable by a f-time bounded NTM.\n\u25b7 DSPACE(f) is the set of all L decidable by a f-space bounded DTM.\n\u25b7 NSPACE(f) is the set of all L decidable by a f-space bounded NTM.\n\u25b7 P := S\nk\u2208N 0 DTIME(nk) (polynomial time)\n\u25b7 NP := S\nk\u2208N 0 NTIME(nk) (nondeterministic polynomial time)\n\u25b7 PSPACE := S\nk\u2208N 0 DSPACE(nk) (polynomial space)\n! It is still open whether P = NP or even P = PSPACE.\nF If L \u2208 P, then also L \u2208 P; if L \u2208 PSPACE, then also L \u2208 PSPACE.\n! It is open whether NP = coNP := {L | L \u2208 NP}.\nF If NP \u0338= coNP, then P \u0338= NP and P \u0338= PSPACE.\n! But also P \u0338= NP = coNP is still possible.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Complexity classes for decision problems 184 (521)\nD For simplicity, let L \u2286 {0, 1}\u2217 denote a binary language:\n\u25b7 DTIME(f) is the set of all L decidable by a f-time bounded DTM.\n\u25b7 NTIME(f) is the set of all L decidable by a f-time bounded NTM.\n\u25b7 DSPACE(f) is the set of all L decidable by a f-space bounded DTM.\n\u25b7 NSPACE(f) is the set of all L decidable by a f-space bounded NTM.\n\u25b7 P := S\nk\u2208N 0 DTIME(nk) (polynomial time)\n\u25b7 NP := S\nk\u2208N 0 NTIME(nk) (nondeterministic polynomial time)\n\u25b7 PSPACE := S\nk\u2208N 0 DSPACE(nk) (polynomial space)\n\u25b7 P is the usual formal definition of \u201ctractable decision problem\u201d as it is\nindependent of the number of tapes/cores, alphabet/architecture, . . . .\n\u25b7 In case of quantum computers, BQP is commonly used as \u201cclass of\nquantum-tractable decision problems\u201d.\n\u25b7 We know that P \u2286 BQP \u2286 PSPACE (via counting),\nbut as P = PSPACE is still possible, so is P = BQP.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Overview 185 (522)\nP\n(type2)\n(type3)\nNPcoNP\nBQP\nPSPACE(type1, LBAs)\nEXPTIME\n...\nRE(type0) coRE\n(tikz code based on [here])\n\u25b7 P := S\nk\u2208N 0 DTIME(nk) (polynomial time)\n\u25b7 NP := S\nk\u2208N 0 NTIME(nk) (nondeterministic polynomial time)\n\u25b7 PSPACE := S\nk\u2208N 0 DSPACE(nk) (polynomial space)\n\u25b7 EXPTIME := S\nk\u2208N 0 DTIME(2nk\n) (exponential time)\n\u25b7 RE: recursively/computably enumerable, semi-decidable, (Turing) recognizable, type 0\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Overview 185 (523)\nP\n(type2)\n(type3)\nNPcoNP\nBQP\nPSPACE(type1, LBAs)\nEXPTIME\n...\nRE(type0) coRE\n(tikz code based on [here])\n\u25b7 Recall, if L = L(G) is of type \u03c4, then w\n?\n\u2208 L can be decided in\n\u03c4 = 1: (constext-sensitive, LBAs) NSPACE(|w|) \u2286 DSPACE(|w|2).\nBut e.g. {anbncn | n \u2208 N 0} is already decidable in DTIME(|w|).\n\u03c4 = 2: (context-free, PDAs) DTIME(|w|3) using e.g. CYK.\n\u03c4 = 3: (regular, FAs) in DTIME(|w|). In fact, \u03c4 = 3 corresponds to DSPACE(1) (i.e. 2-way\nFAs) as for space-bounded computation the read-only input is not counted, cf. here.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Overview 185 (524)\nP\n(type2)\n(type3)\nNPcoNP\nBQP\nPSPACE(type1, LBAs)\nEXPTIME\n...\nRE(type0) coRE\n(tikz code based on [here])\n\u25b7 DTIME(f) \u2286 NTIME(f) \u2286 DSPACE(f) \u2286 DTIME(2O(f)) for f constructible.\n! A constructible time bound implies decidability (recall the Busy Beaver functions).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Overview 185 (525)\nP\n(type2)\n(type3)\nNPcoNP\nBQP\nPSPACE(type1, LBAs)\nEXPTIME\n...\nRE(type0) coRE\n(tikz code based on [here])\n\u25b7 Hierachy theorems (w/o proofs):\nDTIME(f) \u228a DTIME(f(log f)2) and DSPACE(f) \u228a DSPACE(f log f)\nThus: P \u228a EXPTIME\n\u25b7 Savitch: NSPACE(f) \u2286 DSPACE(f2) if f(n) \u2208 \u2126(log(n)) constructible.\nThus PSPACE = NPSPACE.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Overview 185 (526)\nP\n(type2)\n(type3)\nNPcoNP\nBQP\nPSPACE(type1, LBAs)\nEXPTIME\n...\nRE(type0) coRE\n(tikz code based on [here])\n\u25b7 coX(f) := {L | L \u2208 X(f)}\n! The complement L is usually taken wrt. some specific universe.\nOften L = \u03a3\u2217 \\ L if L \u2286 \u03a3\u2217.\nBut e.g. if SAT denotes the set of all satisfiable propositional formulas, then\nSAT = UNSAT is the set of all unsatisfiable boolean formulas (as we can decide in\npolynomial time, if some x is a syntactically valid propositional formula).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Overview 185 (527)\nP\n(type2)\n(type3)\nNPcoNP\nBQP\nPSPACE(type1, LBAs)\nEXPTIME\n...\nRE(type0) coRE\n(tikz code based on [here])\n\u25b7 coX(f) := {L | L \u2208 X(f)}\n\u25b7 RE \u0338= coRE (recall Halting problem and friends)\n\u25b7 If X is deterministic, then X(f) = coX(f), so: P = coP, PSPACE = coPSPACE\n\u25b7 Immerman\u2013Szelepcs\u00b4 enyi:NSPACE(f) = coNSPACE(f) if f(n) \u2208 \u2126(log(n)) constructible.\n! Wrt. nondeterministic time complementation seems to require an exponential blow-up.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Overview 185 (528)\nP\n(type2)\n(type3)\nNPcoNP\nBQP\nPSPACE(type1, LBAs)\nEXPTIME\n...\nRE(type0) coRE\n(tikz code based on [here])\n\u25b7 BQP (\u201cbounded-error quantum polynomial time\u201d): BQP \u2286 PSPACE\n! Open: NP ?= coNP (i.e. SAT\n?\n\u2208 coNP, see NP-completeness)\n! Open: P ?= NP (i.e. SAT \u2208 P; then also NP = coNP).\n! Open: P ?= PSPACE (i.e. type 1 decidable in P; then also NP = coNP = BQP).\n\u25b7 Only very rough picture, see e.g. complexity zoo for more information.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Decision vs. general computation in polynomial time 186 (529)\n\u25b7 Recall: f : {0, 1}\u2217 ,\u2192 {0, 1}\u2217{$} is computable\niff Lf = {w#u | f(w) = uv} is semidecidable.\nD FP denotes the set of all functions f : {0, 1}\u2217 \u2192 {0, 1}\u2217 computable by some\npolynomial-time bounded DTM.\n! If f \u2208 FP, then there exists some polynomial p \u2208 N 0[X] s.t. |f(w)| \u2264p(|w|)\nfor all w \u2208 {0, 1}\u2217. Hence, also Lf := {w#u | f(w) = uv, |u| \u2264p(|w|)} \u2208P.\nAnd if Lf \u2208 P, then f \u2208 FP via search using at most p(|w|) decision calls:\n\u25b7 Binary search: iteratively decide whether the i-th bit of f(w) is 0 or 1.\n\u25b7 We currently do not know whether \u201cnondeterministic polynomial-time function\ncomputation\u201d can be reduced to \u201cnondeterministic polynomial-time decision\u201d\nby means of search (see e.g. Bellare, Goldwasser and complexity zoo).\n(See Shafi Goldwasser and Mihir Bellare on wikipedia; both researchers are known for\nthe important contributions to both complexity theory and cryptography, as modern",
      "by means of search (see e.g. Bellare, Goldwasser and complexity zoo).\n(See Shafi Goldwasser and Mihir Bellare on wikipedia; both researchers are known for\nthe important contributions to both complexity theory and cryptography, as modern\ncryptography is building up on complexity theor; e.g. cryptography requires P \u0338= NP.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "7 Complexity theory: deterministic vs. nondeterministic polynomial time\nResource-bounded computation\nPolynomially bounded computation\nP vs NP\nNP-complete problems\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "P: Big-Oh and worst-case complexity 188 (531)\n\u25b7 Some \u201cdrawbacks\u201d of P:\n\u25b7 Worst-case computation: not always that important.\nOften \u201cefficient for most problems\u201d is good enough (cf. simplex algorithm,\nsmoothed analysis); and Cryptography actually wants \u201calmost always\nworst-case\u201d.\n\u25b7 Big-Oh notation: hidden constant factor can be forbiddingly large.\n\u25b7 For internet applications, already quadratic time can be too high; e.g. consider\nencryption of a video stream.\n\u25b7 Cubic time is often already problematic e.g. numerical simulations lead to large\nsystems of linear equations where Gaussian elimination (see iterative methods).\n\u25b7 Still, the first step is to show L \u2208 P resp. f \u2208 FP.\n\u25b7 E.g. only in 2002 Agrawal, Kayal, and Saxena could give a\nO((log n)12(log logn)) deterministic algorithm for deciding if a given n \u2208 N is\na prime; but within the next few years, the complexity was reduced to\nO((log n)6(log logn)).\n! Recall: the input length of n is |lsbf(n)| = log2 n.",
      "\u25b7 E.g. only in 2002 Agrawal, Kayal, and Saxena could give a\nO((log n)12(log logn)) deterministic algorithm for deciding if a given n \u2208 N is\na prime; but within the next few years, the complexity was reduced to\nO((log n)6(log logn)).\n! Recall: the input length of n is |lsbf(n)| = log2 n.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Important polynomial-time decision problems 189 (532)\n\u25b7 Important problems included in P = S\nk\u2208N 0 DTIME(nk):\n\u25b7 Word problem for a cf. grammar G: given w and G decide w \u2208 L(G).\n\u25b7 Circuit value problem (CVP): given a circuit C (\u201cformula with subterm\nsharing\u201d, boolean SLP) and an input x compute the output of a specific gate.\n\u25b7 2SAT: given a clause set F where every clause contains at most two literals\ndecide if F is satisfiable.\n\u25b7 Horn-SAT: given a clause set F where every clause contains at most one\npositive literal decide if F is satisfiable.\n\u25b7 2COL: given a finite graph G = (V, E) decide if there exists a node coloring\n\u03c7: V \u2192 [2] using at most 2 colors.\n\u25b7 Linear programming (LP): given A \u2208 Z m\u00d7n, b\u2208 Z n, c\u2208 Z m, \u03b3\u2208 Z decide if\nthere is some x \u2208 Q n with x \u2265 0, Ax \u2264 b and c\u22a4x \u2265 \u03b3.\n\u25b7 Graph reachability: decide sE\u2217t for a graph G = (V, E) and nodes s, t\u2208 V .\n\u25b7 Reachability games: decide if player 1 can guarantee a visit to node t starting",
      "\u25b7 Linear programming (LP): given A \u2208 Z m\u00d7n, b\u2208 Z n, c\u2208 Z m, \u03b3\u2208 Z decide if\nthere is some x \u2208 Q n with x \u2265 0, Ax \u2264 b and c\u22a4x \u2265 \u03b3.\n\u25b7 Graph reachability: decide sE\u2217t for a graph G = (V, E) and nodes s, t\u2208 V .\n\u25b7 Reachability games: decide if player 1 can guarantee a visit to node t starting\nin s no matter how player 2 chooses to move in G = (V1 \u228eV2, E) and s, t\u2208 V .\n\u25b7 Primality: given n \u2208 N in binary decide if n is prime.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Important nondeterministic polynomial-time decision problems 190 (533)\n\u25b7 Important problems included in NP = S\nk\u2208N 0 NTIME(nk), but so far not\nknown to be included in P:\n\u25b7 SAT: given a propositional formula F decide if F is satisfiable.\n\u25b7 3SAT: given a clause set F where every clause contains at most three literals\ndecide if F is satisfiable.\n\u25b7 3COL: given a finite graph G = (V, E) decide if there exists a node coloring\n\u03c7: V \u2192 [3] using at most 3 colors.\n\u25b7 Hamiltonian cycle problem: given a finite graph G = (V, E) decide if G is\nhamiltonian, i.e. if there is a simple cycle that visits every node of G exactly\nonce.\n\u25b7 Integer linear programming (ILP): given A \u2208 Z m\u00d7n, b\u2208 Z n, c\u2208 Z m, \u03b3\u2208 Z\ndecide if there is some x \u2208 Z n with x \u2265 0, Ax \u2264 b and c\u22a4x \u2265 \u03b3.\n\u25b7 Graph isomorphism: given G1 = (V1, E1) and G2 = (V2, E2) decide if\nG1 \u223c= G2.\n\u25b7 Subgraph isomorphism: given G1 = (V1, E1) and G2 = (V2, E2) decide if there\nis some H1 \u2264 G1 with H1 \u223c= G2.",
      "decide if there is some x \u2208 Z n with x \u2265 0, Ax \u2264 b and c\u22a4x \u2265 \u03b3.\n\u25b7 Graph isomorphism: given G1 = (V1, E1) and G2 = (V2, E2) decide if\nG1 \u223c= G2.\n\u25b7 Subgraph isomorphism: given G1 = (V1, E1) and G2 = (V2, E2) decide if there\nis some H1 \u2264 G1 with H1 \u223c= G2.\n\u25b7 Factoring: given n, k\u2208 N in binary decide if n has a nontrivial factor in [k, n).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "NP: problems with polynomial-time verifiable solutions 191 (534)\n\u25b7 The characterizing property of decision problems contained in NP is that in\npolynomial-time a potential solution to the related computational problem\ncan be guessed and verified.\n\u25b7 SAT: guess an assignment, then evaluate the formula (cf. CVP).\n\u25b7 3SAT: guess an assignment, then evaluate the formula (cf. CVP).\n\u25b7 3COL: guess an coloring, then check for each node its neighbors\n\u25b7 Hamiltonian cycle problem: guess a permutation of the nodes, then check if it\nis a cycle.\n\u25b7 Integer linear programming (ILP): guess some polynomial-size z \u2208 Z n (!)\n\u25b7 Graph isomorphism: guess a bijection of the nodes, then check that the edges\nare respected.\n\u25b7 Subgraph isomorphism: additionally, guess a subgraph first; seems to make\nthings more difficult (cf. here).\n\u25b7 Factoring: guess a number in [k, n) and check that it divides n.\n! In general, the nondeterministic choices that led to an accepting computation\nof a NTM.",
      "are respected.\n\u25b7 Subgraph isomorphism: additionally, guess a subgraph first; seems to make\nthings more difficult (cf. here).\n\u25b7 Factoring: guess a number in [k, n) and check that it divides n.\n! In general, the nondeterministic choices that led to an accepting computation\nof a NTM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Nondeterminism as deterministic verifier 192 (535)\nu0 u1 u2 u3 u4 u5 u6 u7\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\nM(w, ui)\n\u25b7 Sometimes it is easier to view an NTM M as a DTM which takes as\nadditional input a \u201ctie-breaker string\u201d u that tells M which applicable\ntransition rule to apply ( wlog always at most two rules are applicable).\nIf M runs in time O(p(|w|), then we can assume that u \u2208 {0, 1}p(n) for all\nw \u2208 {0, 1}n and that M reads in the i-th step the i-th bit of u. This yields:\nL L \u2208 NP iff there exists a polynomial p and a p-time bounded DTM M s.t.\nL = {w \u2208 {0, 1}\u2217 | M accepts w#u for some u \u2208 {0, 1}p(|w|)}\nu is a certificate for w if M accepts (\u201cverifies\u201d) w#u.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Nondeterminism as deterministic verifier 192 (536)\nu0 u1 u2 u3 u4 u5 u6 u7\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\nM(w, ui)\n! For L \u2208 NP:\n\u25b7 If x \u2208 L, then at least one ui is a certificate.\n\u25b7 If x \u0338\u2208 L, then no ui is a certificate.\n! For L \u2208 coNP (i.e. L \u2208 NP):\n\u25b7 If x \u2208 L, then all ui are certificates.\n\u25b7 If x \u0338\u2208 L, then at least one ui is not a certificate.\n\u25b7 E.g. SAT \u2208 NP and TAUT\u201c=\u201dUNSAT\u201c=\u201dSAT \u2208 coNP.\nexistential (exists) vs. universal (for all) acceptance\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Polynomial-time reductions: motivation 193 (537)\n\u25b7 The Cook-Levin theorem states that for any decision problem L \u2208 NP there\nis a polynomial-time computable function \u03c1L \u2208 FP so that:\nw \u2208 L iff \u03c1L(w) \u2208 SAT.\n\u25b7 I.e. the function \u03c1L translates a problem instance w for L in polynomial time\ninto a propositional formula \u03c1L(w) so that \u03c1L(w) hast at least one satisfying\nassignment iff w \u2208 L.\n\u25b7 E.g. if L is the Hamiltonian cycle problem, then w = \u230aG\u2309 will be the binary\nencoding of a directed graph, and a satisfying assignment of \u03c1L(\u230aG\u2309) will in\nfact already describe a Hamiltonian cycle in G.\n\u25b7 I.e. any polynomial-time algorithm for deciding the satisfiability of a\npropositional formula (SAT) could be used as a subroutine to obtain a\npolynomial-time algorithm for any problem in NP,\nThis function \u03c1L is called a polynomial-time (Karp) reduction from L to SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Polynomial-time reductions 194 (538)\nD For A \u2286 \u03a3\u2217\nA, B\u2286 \u03a3\u2217\nB, we say that A is polynomial-time reducible to B\n(short: A \u2264p B) if there exists a polynomial-time computable function\nf : \u03a3\u2217\nA \u2192 \u03a3\u2217\nB so that for all w \u2208 \u03a3\u2217\nA we have\nw \u2208 A iff f(w) \u2208 B\n\u25b7 As polynomials are closed wrt. addition, multiplication and composition:\nL If B \u2208 P and A \u2264p B, then also A \u2208 P.\nIf B \u2208 NP and A \u2264p B, then also A \u2208 NP.\nIf A \u2264p B and B \u2264p C, then A \u2264p C.\nT Every L \u2208 PSPACE is polynomial-time reducible to the word problem for\ncontext-sensitive grammars.\n\u25b7 If L \u2208 PSPACE, then there is polynomial p and some p-space bounded DTM\nM that decides L.\n\u25b7 We can give M enough space by simply inflating the input w to\nw\u2032 = \u25a1p(|w|)w\u25a1p(|w|), and adapting M so that it skips the initial blanks.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cook-Levin theorem 195 (539)\nT For L \u2208 NP: L \u2264p SAT := {\u230a\u03d5\u2309 |\u03d5 is a satisfiable propositional formula }.\n(\u230a\u03d5\u2309 is some fixed binary encoding, e.g. UTF8 or ASCII.)\n\u25b7 Contributed to both Stephen Cook and Leonid Levin, but sometimes also only\nshort Cook\u2019s theorem.\n\u25b7 Main idea: As M is p-time-bounded, all of its heads can move at most p(|w|)\nsteps in total. Hence, for a given fixed w we can restrict all tapes to the cells\nat positions between \u2212p(|w|) and p(|w|).\nI.e. every tape can be treated as a word of length 2p(|w|).\nWe can describe the successor relation \u2212 \u2192M using a propositional formula of\nlength O(p(|w|)):\n\u25b7 In principle just as in circuit design via binary encoding as truth tables.\n\u25b7 The main point is that the change to a cell only depends on the current state,\nhead position/read symbol and the applied rule, i.e. on a constant number of\nvalues, so we need to encode 2p(|w|) independent relations of constant size.\n(This is also what was used in the encoding of a TM as PCP instance.)",
      "head position/read symbol and the applied rule, i.e. on a constant number of\nvalues, so we need to encode 2p(|w|) independent relations of constant size.\n(This is also what was used in the encoding of a TM as PCP instance.)\nAll possible runs on w can thus be described as the conjunction of p(|w|) steps\nyielding a formula of length O(p(|w|)2).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cook-Levin theorem 195 (540)\nT For L \u2208 NP: L \u2264p SAT := {\u230a\u03d5\u2309 |\u03d5 is a satisfiable propositional formula }.\n(\u230a\u03d5\u2309 is some fixed binary encoding, e.g. UTF8 or ASCII.)\n\u25b7 Let p be a polynomial and M a p-time-bounded NTM s.t. L = L(M).\nwlog we may assume that M is a 1TM.\nWe use the following propositional variables\n\u25b7 Qt,q: \u201cAt time t, M is in state q\u201d\n\u25b7 Ht,i: \u201cAt time t, the head is at position i\u201d\n\u25b7 Tt,i,a: \u201cAt time t, the content of the i-the tape cell is the symbol a\u201d\n\u25b7 Rt,r: \u201cAt time t, the rule r \u2208 \u03b4 is applied\u201d\nfor t \u2208 {0, 1, . . . , p(|w|)}, i \u2208 {\u2212p(|w|), . . . , p(|w|)}, q \u2208 Q, a \u2208 \u0393, r \u2208 \u03b4.\n\u25b7 As auxiliary means we need the formula of the lenght O(m2):\nEO(x1, . . . , xm) =\n m_\ni=1\nxi\n!\n\u2227\n^\n1\u2264i<j\u2264m\n(\u00acxi \u2228 \u00acxj)\nEO(x1, . . . , xm) is true iff exactly one of the variables x1, . . . , xk is true.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cook-Levin theorem 195 (541)\nT For L \u2208 NP: L \u2264p SAT := {\u230a\u03d5\u2309 |\u03d5 is a satisfiable propositional formula }.\n(\u230a\u03d5\u2309 is some fixed binary encoding, e.g. UTF8 or ASCII.)\n\u25b7 Using EO we can specify that at time t the TM should be in exactly one state,\nits head at exactly one position, in every cell exactly one symbol, exactly one\nrule must be applied:\nFEO,t := EO(Qt,q0 , . . . , Qt,qk )\n\u2227 EO(Ht,\u2212p(|w|), . . . , Ht,p(|w|))\n\u2227 EO(Rt,r1 , . . . , Rt,rm)\n\u2227\n^\ni\nEO(Tt,i,a1 , . . . , Tt,i,al)\nfor fixed enumerations Q = {q0, . . . , qk}, \u0393 = {a1, . . . , al}, \u03b4 = {r1, . . . , rm}.\nNote that |FEO,t| \u2208 O(p(|w|)).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cook-Levin theorem 195 (542)\nT For L \u2208 NP: L \u2264p SAT := {\u230a\u03d5\u2309 |\u03d5 is a satisfiable propositional formula }.\n(\u230a\u03d5\u2309 is some fixed binary encoding, e.g. UTF8 or ASCII.)\n\u25b7 We then specify that, if Rt,r is true, then r \u2208 \u03b4 has to be applicable, and how\nits application changes the state and tape content:\nF\u03b4,t :=\n^\ni,a\n\u0010\n(\u00acHt,i \u2227 Tt,i,a) \u2192 Tt+1,i,a\n\u0011\n\u2227\n^\ni,r=(p,a,q,r,d)\u2208\u03b4\n\u0010\n(Ht,i \u2227 Rt,r) \u2192 (Qt,p \u2227 Tt,i,a \u2227 Qt+1,q \u2227 Tt+1,i,b \u2227 Ht,i+d)\n\u0011\nNote that |F\u03b4,t| \u2208 O(p(|w|)).\nIn conjunction with FEO,t exactly one Rt,r will be true and thus enforce the\ncorrection application of the rule r = (p, a, q, r, d) (if it is applicable).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cook-Levin theorem 195 (543)\nT For L \u2208 NP: L \u2264p SAT := {\u230a\u03d5\u2309 |\u03d5 is a satisfiable propositional formula }.\n(\u230a\u03d5\u2309 is some fixed binary encoding, e.g. UTF8 or ASCII.)\n\u25b7 Next we specify the initial configuration for the given w = w1 . . . wn:\nFinit := Q0,q0 \u2227 H0,0 \u2227\nn^\ni=1\nT0,i,wi \u2227\n^\n|i|\u2264p(|w|),i\u0338\u2208[n]\nT0,i,\u25a1\nNote that |Finit| \u2208 O(p(|w|)).\n\u25b7 And we specify that we want the computation to accept w:\nFacc :=\n_\nqf \u2208F\nQp(|w|),qf\nwlog we can assume that M always does exactly p(|w|) steps e.g. by forcing\nM to count till p(|w|) (in unary or binary).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cook-Levin theorem 195 (544)\nT For L \u2208 NP: L \u2264p SAT := {\u230a\u03d5\u2309 |\u03d5 is a satisfiable propositional formula }.\n(\u230a\u03d5\u2309 is some fixed binary encoding, e.g. UTF8 or ASCII.)\n\u25b7 All possible accepting runs of M on w are then described by the formula\n\u03c1L(w) := Finit \u2227 Facc \u2227\np(|w|)\u22121^\nt=0\nF\u03b4,t \u2227\np(|w|)^\nt=0\nFEO,t\nwhich is of length O(p(|w|2).\n! By construction, the accepting runs of M on w are then in bijection with the\nsatisfying assignments to \u03c1L(w) by means of the truth values of the Rt,r.\nIn particular, given a satisfying assignment, we can recover from Rt,r the\naccepting run of M on w in linear time; i.e. SAT is used to \u201cguess\u201d via Rt,r\nthe rules used in an accepting run.\nIf M is deterministic, we can simply hard-code the uniquely determined values\nfor the Rt,r; then above construction essentially says that for every L \u2208 P and\nevery fixed input length n we can compute a polynomial-sized circuit C with n\ninputs so that: C(w1, . . . , wn) = 1 iff w \u2208 L.",
      "If M is deterministic, we can simply hard-code the uniquely determined values\nfor the Rt,r; then above construction essentially says that for every L \u2208 P and\nevery fixed input length n we can compute a polynomial-sized circuit C with n\ninputs so that: C(w1, . . . , wn) = 1 iff w \u2208 L.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Oracles (*) 196 (545)\nD Let O be any concrete Turing machine called the oracle. Then MO denotes a\npolynomial-time deterministic (multi-tape) TM M with additional oracle\naccess to O.\n\u25b7 M has an additional query tape for calling O in order to obtain O(x):\nDuring any time of a computation, M may also write some \u201cquery\u201d x to its\nquery tape (e.g. \u201cdoes the Turing machine Mw encoded by w halt on w?\u201d).\nM then may call O on x (by changing into a specific state) and within a single\ntime step, the content of the query tape will be overwritten by the output\nO(x) of O. (writing the argument x and reading the actual ouput O(x) still take\ntime |x| resp. |O(x)|)\n\u25b7 Otherwise, MO behaves like a normal deterministic polynomial-time TM.\nIn particular, there is a polynomial p so that MO halts after at most p(|w|)\nsteps for every input w.\n\u25b7 For example, we can combine any polynomial time reduction from some\nL \u2208 NP to SAT with a SAT solver.",
      "time |x| resp. |O(x)|)\n\u25b7 Otherwise, MO behaves like a normal deterministic polynomial-time TM.\nIn particular, there is a polynomial p so that MO halts after at most p(|w|)\nsteps for every input w.\n\u25b7 For example, we can combine any polynomial time reduction from some\nL \u2208 NP to SAT with a SAT solver.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Oracles (*) 196 (546)\n\u25b7 As we do not know if a concrete oracle O is reliable, we usually expect O to\nnot only return \u201cyes\u201d or \u201cno\u201d, but also some sort of certificate (proof,\ncomputation) that can be verified by M:\nD A language L is an element of POracle if there exists a deterministic\npolynomial-time TM M s.t. both:\n\u25b7 there exists at least one oracle O s.t. L(MO) = L, and\n\u25b7 for every possible oracle O\u2032 we have L(MO\u2032\n) \u2286 L.\nI.e. M is a \u201ccorrect proof checker\u201d that cannot be tricked by any O\u2032 into\naccepting a word w \u0338\u2208 L as M will detect an erroneous certificate.\nL NP = POracle\n\u25b7 NP = PSAT by virtue of Cook-Levin.\n\u25b7 If L \u2208 POracle, then turn the oracle TM M into a NTM M\u2032 that guesses the\n(necessarily polynomial) output of the oracle.\n! I.e. as M has to be deterministic and run in polynomial time, any oracle more\npowerful than a SAT-solver is useless to it.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Oracles (*) 196 (547)\n\u25b7 In an interactive proof system M must still be \u201cdeterministic\u201d (up to\nrandomization) and polynomial time, but it may further throw a fair coin.\nD A language L is an element of IP if there exists a randomized\npolynomial-time TM M s.t. both:\n\u25b7 there is one oracle O s.t. MO accepts every w \u2208 L with prob. \u2265 2/3, and\n\u25b7 for every oracle O\u2032 the prob. that MO\u2032\naccepts a word w \u0338\u2208 L is \u2264 1/3.\nT PSPACE = IP (see e.g. Papadimitriou) .\n\u25b7 Extensions/applications:\n\u25b7 Zero-knowledge proofs (ZKP).\n\u25b7 multi prover interactive proofs (MIP): M has access to two (or more)\nindependent, strictly separated orcales; M can now e.g. use one oracle to\ncheck the answers of the other oracle; MIP = NEXPTIME.\n\u25b7 probabilistically checkable proofs (PCP): How much randomization, how much\ninteraction is required to decide certain problems?\nPCP theorem (G\u00a8odelpreis 2001): O(log n) bits of randomness and O(1) bits of\nthe proof from the oracle suffice for NP.",
      "\u25b7 probabilistically checkable proofs (PCP): How much randomization, how much\ninteraction is required to decide certain problems?\nPCP theorem (G\u00a8odelpreis 2001): O(log n) bits of randomness and O(1) bits of\nthe proof from the oracle suffice for NP.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "7 Complexity theory: deterministic vs. nondeterministic polynomial time\nResource-bounded computation\nPolynomially bounded computation\nP vs NP\nNP-complete problems\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Hardness and completeness 198 (549)\nD A language L is X-hard if L\u2032 \u2264p L for all L\u2032 \u2208 X.\nA language L is X-complete if L \u2208 X and L is X-hard.\nT Cook-Levin: SAT is NP-complete.\nT The word problem for context-sensitive grammars is NP-hard and\nPSPACE-complete.\n\u25b7 As discussed before by means of inflating the input in polynomial time.\n\u25b7 As P\n?\n= PSPACE is still open, the world problem for context-sensitive\ngrammars might be even NP-complete.\n\u25b7 Should someone be able to show the word problem for context-sensitive\ngrammars can be decided in polynomial time, then P = NP = PSPACE.\n\u25b7 Should someone be able to show that SAT \u2208 P, then P = NP.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Hardness and completeness 198 (550)\nD A language L is X-hard if L\u2032 \u2264p L for all L\u2032 \u2208 X.\nA language L is X-complete if L \u2208 X and L is X-hard.\nT Cook-Levin: SAT is NP-complete.\n! The X-complete problems can be consider the \u201chardest\u201d problems within X:\n\u25b7 E.g. every L \u2208 NP with SAT \u2264p L is itself NP-complete.\nA polynomial-time algorithm for L would also yield a polynomial-time\nalgorithm for SAT and thus any other decision problem in NP.\n\u25b7 If we can show that L is NP-complete, it is deemed very unlikely that we will\nfind a polynomial-time algorithm for L; instead we should consider a restricted\nsubset of L.\nC If SAT \u2208 P, then P = NP.\nC SAT =p UNSAT =p TAUT is coNP-complete. (A =p B short for A \u2264p B \u2264p A.)\nC If SAT \u2208 coNP, then NP = coNP.\nT Ladner\u2019s theorem (w/o proof): If P \u0338= NP, then the NP-complete problems\nare a strict subset of NP.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "NP-complete problems 199 (551)\n\u25b7 E.g. the following problems are known to be NP-complete:\n\u25b7 SAT: given a propositional formula F decide if F is satisfiable.\n\u25b7 3SAT: given a clause set F where every clause contains at most three literals\ndecide if F is satisfiable.\n\u25b7 3COL: given a finite graph G = (V, E) decide if there exists a node coloring\n\u03c7: V \u2192 [3] using at most 3 colors.\n\u25b7 Hamiltonian cycle problem: given a finite graph G = (V, E) decide if G is\nhamiltonian, i.e. if there is a simple cycle that visits every node of G exactly\nonce.\n\u25b7 Integer linear programming (ILP): given A \u2208 Z m\u00d7n, b\u2208 Z n, c\u2208 Z m, \u03b3\u2208 Z\ndecide if there is some x \u2208 Z n with x \u2265 0, Ax \u2264 b and c\u22a4x \u2265 \u03b3.\n\u25b7 Subgraph isomorphism: given G1 = (V1, E1) and G2 = (V2, E2) decide if there\nis some H1 \u2264 G1 with H1 \u223c= G2.\nbut e.g. factoring and graph isomorphism are not known to be NP-hard.\nFor both we know sub-exponential time algorithms in contrast to SAT.",
      "\u25b7 Subgraph isomorphism: given G1 = (V1, E1) and G2 = (V2, E2) decide if there\nis some H1 \u2264 G1 with H1 \u223c= G2.\nbut e.g. factoring and graph isomorphism are not known to be NP-hard.\nFor both we know sub-exponential time algorithms in contrast to SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: construction of an equisatisfiable 3CNF 200 (552)\n\u2228q\u03b5 :\n\u2227q0 : \u2227q1 :\np1 p2 p3 p4\n\u25b7 Intermediate step: Auxiliary variables \u201cstore\u201d intermediate results.\nq\u03b5 \u2227 (q\u03b5 \u2194 (q0 \u2228 q1))| {z }\n=:G\u03b5\n\u2227(q0 \u2194 (p1 \u2227 p2))| {z }\n=:G0\n\u2227(q1 \u2194 (p3 \u2227 p4))| {z }\n=:G1\nD Clause: conjunction of literals W\ni Li.\n! Every formula with 3 variables has a 3CNF with at most \u2264 4 clauses:\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: construction of an equisatisfiable 3CNF 200 (553)\n\u2228q\u03b5 :\n\u2227q0 : \u2227q1 :\np1 p2 p3 p4\n\u25b7 Replace G\u03b5, G0, G1 by semantically equivalent 3CNF:\nEF := q\u03b5 \u2227 (\u00acq\u03b5 \u2228 q0 \u2228 q1) \u2227 (q\u03b5 \u2228 \u00acq0) \u2227 (q\u03b5 \u2228 \u00acq1)\n\u2227 (q0 \u2228 \u00acp1 \u2228 \u00acp2) \u2227 (\u00acq0 \u2228 p1) \u2227 (\u00acq0 \u2228 p2)\n\u2227 (q1 \u2228 \u00acp3 \u2228 \u00acp4) \u2227 (\u00acq1 \u2228 p3) \u2227 (\u00acq1 \u2228 p4)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: construction of an equisatisfiable 3CNF 200 (554)\n\u2228q\u03b5 :\n\u2227q0 : \u2227q1 :\np1 p2 p3 p4\n\u25b7 Replace G\u03b5, G0, G1 by semantically equivalent 3CNF:\nEF := q\u03b5 \u2227 (\u00acq\u03b5 \u2228 q0 \u2228 q1) \u2227 (q\u03b5 \u2228 \u00acq0) \u2227 (q\u03b5 \u2228 \u00acq1)\n\u2227 (q0 \u2228 \u00acp1 \u2228 \u00acp2) \u2227 (\u00acq0 \u2228 p1) \u2227 (\u00acq0 \u2228 p2)\n\u2227 (q1 \u2228 \u00acp3 \u2228 \u00acp4) \u2227 (\u00acq1 \u2228 p3) \u2227 (\u00acq1 \u2228 p4)\n\u25b7 Every satisfying EF -assignment is also satisfying for F.\neg \u03b2(p1) = \u03b2(p2) = \u03b2(p3) = 1, \u03b2(p4) = 0, \u03b2(q0) = 1, \u03b2(q1) = 0, \u03b2(qew) = 1.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: construction of an equisatisfiable 3CNF 200 (555)\n\u2228q\u03b5 :\n\u2227q0 : \u2227q1 :\np1 p2 p3 p4\n\u25b7 Replace G\u03b5, G0, G1 by semantically equivalent 3CNF:\nEF := q\u03b5 \u2227 (\u00acq\u03b5 \u2228 q0 \u2228 q1) \u2227 (q\u03b5 \u2228 \u00acq0) \u2227 (q\u03b5 \u2228 \u00acq1)\n\u2227 (q0 \u2228 \u00acp1 \u2228 \u00acp2) \u2227 (\u00acq0 \u2228 p1) \u2227 (\u00acq0 \u2228 p2)\n\u2227 (q1 \u2228 \u00acp3 \u2228 \u00acp4) \u2227 (\u00acq1 \u2228 p3) \u2227 (\u00acq1 \u2228 p4)\n\u25b7 Every satisfying F-assignment can be adapted to a sat. EF -assignment\neg \u03b2(p1) = \u03b2(p2) = \u03b2(p3) = 1, \u03b2(p4) = 0 and \u03b2(q0) = 1, \u03b2(q1) = 0, \u03b2(q\u03b5) = 1.\nbut also to a unsatisfying EF -assignment:\neg \u03b2(p1) = \u03b2(p2) = \u03b2(p3) = 1, \u03b2(p4) = 0 and \u03b2(q0) = 1, \u03b2(q1) = 0, \u03b2(q\u03b5) = 0.\nThus F \u2261e EF but F \u0338\u2261 EF !\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: construction of an equisatisfiable 3CNF 200 (556)\n\u2228q\u03b5 :\n\u2227q0 : \u2227q1 :\np1 p2 p3 p4\n\u25b7 Replace G\u03b5, G0, G1 by semantically equivalent 3CNF:\nEF := q\u03b5 \u2227 (\u00acq\u03b5 \u2228 q0 \u2228 q1) \u2227 (q\u03b5 \u2228 \u00acq0) \u2227 (q\u03b5 \u2228 \u00acq1)\n\u2227 (q0 \u2228 \u00acp1 \u2228 \u00acp2) \u2227 (\u00acq0 \u2228 p1) \u2227 (\u00acq0 \u2228 p2)\n\u2227 (q1 \u2228 \u00acp3 \u2228 \u00acp4) \u2227 (\u00acq1 \u2228 p3) \u2227 (\u00acq1 \u2228 p4)\n! The computation of the equisatisfiable 3CNF is polynomial in the length of\nthe original formula: the syntax tree can be computed using CYK (or even in\nlinear time using a deterministic grammar), after that a constant amount of\nwork is done for each inner node.\nC 3SAT := {\u230a\u03d5\u2309 |\u03d5 is a propositional formula in 3CNF } is NP-complete.\nL 2SAT := {\u230a\u03d5\u2309 |\u03d5 is a propositional formula in 2CNF } \u2208P (see e.g. here).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "More NP-complete problems 201 (557)\n\u25b7 The restricted form of 3CNF makes it easier to encode a formula F in 3CNF\nin time polynomial in |F| into a graph GF so that\nF is satisfiable iff the graph GF has e.g.\n\u25b7 a node coloring with 3 colors, and a 3-coloring\n\u25b7 a Hamiltonian cycle, and a Hamiltonian cycle\n\u25b7 a subgraph isomorphic with Kl (a l-clique), and a l-clique\nencodes a satisfying assignment, i.e. all of these problems are also NP-hard.\n\u25b7 In particular, the last means that not only subgraph isomorphism, but the\nrestricted question whether there is a clique is already NP-hard.\n\u25b7 All these problems can also easily described in propositional logic in time\npolynomial in the size of the graph, i.e. all problems are NP-complete.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "More NP-complete problems 201 (558)\n\u25b7 The restricted form of 3CNF makes it easier to encode a formula F in 3CNF\nin time polynomial in |F| into a graph GF so that\nF is satisfiable iff the graph GF has e.g.\n\u25b7 a node coloring with 3 colors, and a 3-coloring\n\u25b7 a Hamiltonian cycle, and a Hamiltonian cycle\n\u25b7 a subgraph isomorphic with Kl (a l-clique), and a l-clique\nencodes a satisfying assignment, i.e. all of these problems are also NP-hard.\n\u25b7 We have a brief look at the basic ideas of these reductions.\n\u25b7 For more examples, see e.g.:\n\u25b7 Karp\u2019s 21 NP-complete problems.\n\u25b7 List of NP-complete problems on wikipedia.\n\u25b7 But \u201canything\u201d that can encode somehow a graph and a path (=run of a TM)\ncan be turned into a NP-complete problem e.g. including Tetris, Sokoban,\nZelda, Super Mario (Kart), . . . (of course all \u201cTM\u201d), see e.g. here.\n\u25b7 \u201ccomputation = reachability in a compressed graph given by rewrite rules\u201d",
      "\u25b7 But \u201canything\u201d that can encode somehow a graph and a path (=run of a TM)\ncan be turned into a NP-complete problem e.g. including Tetris, Sokoban,\nZelda, Super Mario (Kart), . . . (of course all \u201cTM\u201d), see e.g. here.\n\u25b7 \u201ccomputation = reachability in a compressed graph given by rewrite rules\u201d\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "3-Colorability: CHECK ME 202 (559)\na \u00aca b \u00acb c \u00acc d \u00acd\nL1,1\nL1,2 L1,3\nL2,1\nL2,2 L2,3\nL3,1\nL3,2 L3,3\nL4,1\nL4,2 L4,3\n\u25b7 The triangles at the top with the common center node require 3 colors.\n\u25b7 wlog the common node has to be colored blue.\n\u25b7 wlog the single node of degree 1 is colored green (not needed actually).\n\u25b7 wlog the remaining nodes are colored either green (true) or red (false).\n\u25b7 Exactly one of the two nodes representing literals of the same variable can be\ncolored green.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "3-Colorability: CHECK ME 202 (560)\na \u00aca b \u00acb c \u00acc d \u00acd\nL1,1\nL1,2 L1,3\nL2,1\nL2,2 L2,3\nL3,1\nL3,2 L3,3\nL4,1\nL4,2 L4,3\n\u25b7 Each house encodes a clause, e.g. here the first clause is \u00aca \u2228 \u00acb \u2228 c.\n\u25b7 The nodes of the house are connected to the top as shown:\n\u25b7 The center of each house has to be colored red; its neighbors green or blue.\n\u25b7 The node Li,j can only be colored blue or the negated truth value of the\nliteral it is connected to.\n\u25b7 In particular, if all literal nodes a house is connected to are colored red (false),\nthe nodes Li,1, Li,2, Li,3 can only be colored green or blue.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "3-Colorability: CHECK ME 202 (561)\na \u00aca b \u00acb c \u00acc d \u00acd\nL1,1\nL1,2 L1,3\nL2,1\nL2,2 L2,3\nL3,1\nL3,2 L3,3\nL4,1\nL4,2 L4,3\n\u25b7 Observe, that each house without its center is a simple cycle of length 5,\ni.e. we need at least three colors.\n\u25b7 The neighbors of the center can only be colored green or blue.\n\u25b7 Thus: a house is 3-colorable iff\nat least one literal it is connected to is colored green (true).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "3-Colorability: CHECK ME 202 (562)\na \u00aca b \u00acb c \u00acc d \u00acd\nL1,1\nL1,2 L1,3\nL2,1\nL2,2 L2,3\nL3,1\nL3,2 L3,3\nL4,1\nL4,2 L4,3\n\u25b7 Determine the underlying formula and complete the node coloring.\n\u25b7 Remark:\nThe additional green node in the middle is actually not required, but\nsimplifies the argument.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Hamiltonian Cycle Problem 203 (563)\ni1\ni2\ni3\no1\no2\no3\n\u25b7 This graph is has no Hamiltonian cycle as we cannot go from right to left.\n\u25b7 But it has a Hamiltonian path: i1, i2, i3, o3, o2, o1.\n\u25b7 In particular, there are exactly three Hamitonian paths which are defined by\nthe \u201cbridge\u201d they use to go from left to right.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Hamiltonian Cycle Problem 203 (564)\ni1\ni2\ni3\no1\no2\no3\n\u25b7 Each clause will be represented by a copy of the graph on the right:\n\u25b7 E.g. consider \u00aca \u2228 \u00acb \u2228 c.\n\u25b7 Then i1 and o1 represent \u00aca.\n\u25b7 Then i2 and o2 represent \u00acb.\n\u25b7 Then i3 and o3 represent c.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Hamiltonian Cycle Problem 203 (565)\na\nb\nc\nd\na\nb\nc\nd\nia\ni\u00acb\nic\noa\no\u00acb\noc\ni\u00aca\ni\u00acc\nid\no\u00aca\no\u00acc\nod\nia\nib\ni\u00acd\noa\nob\no\u00acd\nib\ni\u00acc\nid\nob\no\u00acc\nod\n\u25b7 The graphs will be connected in series with the nodes labeled ik as \u201cinputs\u201d,\nand the nodes labeled by ok as \u201coutputs\u201d.\n\u25b7 The nodes labeled a, b, c, drepresent the variables, and have to be identified.\n\u25b7 From every variable x node on the left, we start two paths, one for each truth\nvalue. Both paths will end in the \u201cnext\u201d variable so that we cycle through all\nvariables, e.g. in the order a, b, c, d, a.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Hamiltonian Cycle Problem 203 (566)\na\nb\nc\nd\na\nb\nc\nd\nia\ni\u00acb\nic\noa\no\u00acb\noc\ni\u00aca\ni\u00acc\nid\no\u00aca\no\u00acc\nod\nia\nib\ni\u00acd\noa\nob\no\u00acd\nib\ni\u00acc\nid\nob\no\u00acc\nod\n\u25b7 E.g. the graph with the paths for the variable a.\n\u25b7 The important point is: a satisfying assignment gives rise to a \u201cmeta cycle\u201d\nthat visits every \u201cclause graph\u201d at least once and at most three times.\n\u25b7 By construction, each \u201cclause graph\u201d has exactly three Hamiltonian paths that\ncan be used to extend the \u201cmeta cycle\u201d into an Hamiltonian cycle.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Hamiltonian Cycle Problem 203 (567)\na\nb\nc\nd\nia\ni\u00acb\nic\noa\no\u00acb\noc\ni\u00aca\ni\u00acc\nid\no\u00aca\no\u00acc\nod\nia\nib\ni\u00acd\noa\nob\no\u00acd\nib\ni\u00acc\nid\nob\no\u00acc\nod\n\u25b7 The final graph. The graph can also be made undirected. See e.g. Sch \u00a8oning\nfor both proofs.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Clique/subgraph isomorphism 204 (568)\nx \u00acy z\nx\ny\nz \u00acx\n\u00acy\n\u00acz\n\u25b7 Here, the encoding is straight-forward:\n\u25b7 Every clause Li,1 \u2228 Li,2 \u2228 Li,3 gives rise to three nodes Li,1, Li,2, Li,3.\n\u25b7 There is an edge between Li,j and Lk,l iff\ni \u0338= k (i.e. different clauses) and Li,j \u2228 Lk,l is satisfiable (i.e. Li,j \u0338= Lk,l).\n\u25b7 If F has l clauses, a satisfying assignment thus gives rise to subgraph that is\nisomorphic to Kl (=l-clique).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Integer linear programming 205 (569)\nA B\nE\nC\nD\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1 0\n2 1\n\u22121 1\n\u22121 0\n0 \u22121\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nx \u2264\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n4\n11\n5\n0\n0\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u25b7 Linear program (LP) in normal form: Given A \u2208 R m\u00d7n, b\u2208 R m, c\u2208 R n\ndetermine some x \u2208 R n s.t. x \u2265 0, Ax \u2264 b and c\u22a4x is maximized.\n\u25b7 As we need to be able to represent A, b, con the computer, we usually have\nthat A, b, care rational and thus in fact integers.\n\u25b7 Any x \u2208 R n satisfying Ax \u2264 b and x \u2265 0 is called a feasible solution.\n\u25b7 \u201cAx \u2264 b and x \u2265 0\u201d describes a convex region whose vertices are the\nintersection of some of the contraints, and thus, in practice also rational.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Integer linear programming 205 (570)\nA B\nE\nC\nD\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1 0\n2 1\n\u22121 1\n\u22121 0\n0 \u22121\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nx \u2264\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n4\n11\n5\n0\n0\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u25b7 Linear program (LP) in normal form: Given A \u2208 R m\u00d7n, b\u2208 R m, c\u2208 R n\ndetermine some x \u2208 R n s.t. x \u2265 0, Ax \u2264 b and c\u22a4x is maximized.\n\u25b7 If the maximum of c\u22a4x exists, it will be a vertex of \u201c Ax \u2264 b and x \u2265 0\u201d.\n\u25b7 If we instead ask whether for a given z \u2208 R there is some solution s.t.\nc\u22a4x \u2265 z, then we intersect \u201c Ax \u2264 b and x \u2265 0\u201d with \u201c c\u22a4x \u2265 z\u201d and can use\nbinary search to find an optimal solution.\nSee also branch-and-bound (essentially quantitative DPLL) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Integer linear programming 205 (571)\nA B\nE\nC\nD\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1 0\n2 1\n\u22121 1\n\u22121 0\n0 \u22121\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nx \u2264\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n4\n11\n5\n0\n0\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u25b7 We actually know how to solve an LP in polynomial time using so-called\ninterior-point methods.\n\u25b7 But in practice often the simplex method is used which (depending on the\npivot heuristic) is known to run in PSPACE/exponential time in the\nworst-case.\n\u25b7 Yet, simplex is a very robust algorithm that \u201con-average\u201d runs in\npolynomial-time, see smoothed analysis.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Integer linear programming 205 (572)\nA B\nE\nC\nD\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1 0\n2 1\n\u22121 1\n\u22121 0\n0 \u22121\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nx \u2264\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n4\n11\n5\n0\n0\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u25b7 In an integer linear program (ILP) we only allow integer solutions which\nmakes the problem harder in general.\n\u25b7 Remark: Matiyasevich\u2019s theorem essentially states that we can encode a DTM\nM into a polynomial p \u2208 Z [X0, X1, . . . , Xk] so that n \u2208 L(M) iff\np(n, x1, . . . , xk) = 0 has an integer solution; i.e. Peano arithmetic is\nundecidable. But if we also consider real solutions (existential theory of the\nreals) the problem is in PSPACE.\n(Also Presburger arithmetic ( Z but only multiplication with constants) is decidable.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Integer linear programming 205 (573)\nA B\nE\nC\nD\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1 0\n2 1\n\u22121 1\n\u22121 0\n0 \u22121\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nx \u2264\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n4\n11\n5\n0\n0\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u25b7 It is straight-forward to encode a 3CNF formula into an \u201c0-1-ILP\u201d instance:\n\u25b7 For each propositional variable pi add two ILP variables xi and yi\n\u25b7 Then require 0 \u2264 xi \u2264 1, 0 \u2264 yi \u2264 1, and xi + yi = 1.\nI.e. yi = 1 \u2212 xi \u2208 {0, 1}.\n\u25b7 A clause {pi, \u00acpj, pk} yields the constraint xi + yj + xk = 1.\n\u25b7 \u201ca\u22a4x = b\u201d can be replaced by \u201c a\u22a4x \u2264 b, \u2212a\u22a4x \u2264 \u2212b\u201d.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Integer linear programming 205 (574)\nA B\nE\nC\nD\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1 0\n2 1\n\u22121 1\n\u22121 0\n0 \u22121\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nx \u2264\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n4\n11\n5\n0\n0\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u25b7 The difficult part is to show that there is always a polynomial-sized integer\nsolution if there is some integer solution at all.\n\u25b7 This can be done by using e.g. Cramer\u2019s rule, see e.g. Papadimitriou.\n\u25b7 Hence, if Ax \u2264 b has an integer solution, it also has an integer solution of\npolynomial length which we can guess and validate in NP.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Remark: optimization vs. decision 206 (575)\n\u25b7 Recall that f : {0, 1}\u2217 \u2192 {0, 1}\u2217 is computable in deterministic polynomial\ntime iff we can e.g. decide for input x in polynomial time whether the i-th\nbit of f(x) is 1.\n\u25b7 E.g. computing an satisfying assignment for a given propositional formula can\nbe reduced to deciding if some propositional formula is satisfiable.\n\u25b7 A special case is the question of computing some optimum, e.g.:\n\u25b7 If we can decide for a given graph G = (V, E) and k \u2208 N in polynomial time,\nif it has a node coloring using at most k colors (\u201ckCOL\u201d), then we can also\ncompute its chromatic number i.e. the minimal k for which a node coloring\nexits by means of binary search on {0, 1, . . . ,|V |}.\n\u25b7 If we can decide for a given graph G = (V, E) and k \u2208 N in polynomial time,\nif it has a clique of size at least k (\u201ckCOL\u201d), then we can also compute the\nmaximal k for which G contains a k-clique again by means of binary on\n{0, 1, . . . ,|V |}.",
      "\u25b7 If we can decide for a given graph G = (V, E) and k \u2208 N in polynomial time,\nif it has a clique of size at least k (\u201ckCOL\u201d), then we can also compute the\nmaximal k for which G contains a k-clique again by means of binary on\n{0, 1, . . . ,|V |}.\n(Note that if we fix the value of k, i.e. k is not part of the input, then deciding\nwhether G has a k-clique can be done in polynomial time.)\n\u25b7 Similarly, the computing the optimal solution of an ILP can reduced to\ndeciding if an ILP has a solution.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Remark: reductions in cryptography 207 (576)\n\u25b7 In modern cryptography the security of cryptographic schemes (e.g. for\nencryption or digital signatures) is assessed by means of (probablisitc)\npolynomial-time reductions to long-standing open problems:\n\u25b7 E.g. the security of the Rabin cryptosystems can be reduced to the problem of\nfinding a (non-trivial) factorization of a composite integer n.\n\u25b7 Currently, we do not know how factorize an integer n in time polynomial in\nlog2 n.\n\u25b7 In particular, it is conjectured that factorizing n = pq with p, qtwo distinct\nprime numbers with log2 p \u2248 log2 q is a worst case \u2013 at least if p, qare chosen\nuniformly at random.\n\u25b7 The reduction underlying a \u201cproof of security\u201d of a Rabin cryptosystems thus\nallows to turn an attack into a factorization algorithm.\n\u25b7 As we conjecture that there is no (deterministic) efficient factorization\nalgorithm, we necessarily also have to conjecture that there is no efficient",
      "\u25b7 The reduction underlying a \u201cproof of security\u201d of a Rabin cryptosystems thus\nallows to turn an attack into a factorization algorithm.\n\u25b7 As we conjecture that there is no (deterministic) efficient factorization\nalgorithm, we necessarily also have to conjecture that there is no efficient\nattack on a Rabin cryptosystem. (But see Shor\u2019s algorithm.)\n\u25b7 One can show that cryptography requires the existence of one-way functions\nwhich can only exist if P \u0338= coNP.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: Learning with errors 208 (577)\n\u25b7 Generation of problem instances:\n\u25b7 Input: Positive integers q, n\u2208 N , vector v \u2208 Z n\nq , distribution \u03d5 on R\n\u25b7 Output: Samples (u1, t1), . . . ,(ul, tl) \u2208 Z n\nq \u00d7 [0, q) with l polynomial wrt. q, n\nwhere each sample is obtained as follows:\n\u25b7 Choose ui \u2208 Z n\nq uniformly at random.\n\u25b7 Choose ei \u2208 R according to \u03d5 at random (error/pertubation).\n\u25b7 Set ti := (\u27e8ui,v\u27e92\nq + ei) mod 1 with x mod 1 := x \u2212 \u230ax\u230b\n\u25b7 Learning problem: Given q, n, \u03d5and samples (u1, t1), . . . ,(ul, tl) recover v.\n\u25b7 Decision problem: Given q, n, \u03d5and samples (u1, t1), . . . ,(ul, tl) decide\nwhether the samples have been generated as described above or whether they\nwere generated uniformly at random.\n\u25b7 Oded Regev: If q is a prime polynomial in n and \u03d5 is a Gaussian distribution,\nboth problems can be reduced to each other in prob. polynomial time.\n\u25b7 Extension RLWE: GF(qn) instead of Z n\nq for q prime.\n\u25b7 Used in some post-quantum cryptographic schemes.",
      "were generated uniformly at random.\n\u25b7 Oded Regev: If q is a prime polynomial in n and \u03d5 is a Gaussian distribution,\nboth problems can be reduced to each other in prob. polynomial time.\n\u25b7 Extension RLWE: GF(qn) instead of Z n\nq for q prime.\n\u25b7 Used in some post-quantum cryptographic schemes.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "8 Basics of information theory: Shannon entropy, Huffman codes, compression\nPrefix codes and Huffman\u2019s algorithm\nOptimal codes and Shannon\u2019s entropy\nEntropy and information\nArithmetic codes\nData compression\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Uniquely decodable codes 211 (580)\nD A (binary) code for \u03a3 is a homomorphism c: \u03a3\u2217 \u2192 {0, 1}\u2217.\n\u25b7 I.e. c(\u03b5) = \u03b5 and c(uv) = c(u)c(v) s.t. it suffices to define c(a) for all a \u2208 \u03a3.\nc is said to uniquely decodable if c is injective.\nF Every code c with |c(\u03a3)| < |\u03a3| is not uniquely decodable.\nEvery code c with c(\u03a3\u2264n) \u2286 \u03a3<n for some n \u2208 N is not uniquely decodable.\nD c is a prefix code if c(\u03a3) is prefix-free.\nc is a block code if c(\u03a3) \u2286 {0, 1}m for some m.\nF All prefix codes and all block codes are uniquely decodable if |c(\u03a3)| = |\u03a3|.\n\u25b7 The PCP can be understood as the question if two codes c, c\u2032 map the same\nword w to the same code word c(w) = c\u2032(w).\nIn contrast to the PCP, we can decide whether a single code c is\ninjective/uniquely decodable \u2013 in fact in polynomial time.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Uniquely decodable codes 211 (581)\nD A (binary) code for \u03a3 is a homomorphism c: \u03a3\u2217 \u2192 {0, 1}\u2217.\n\u25b7 I.e. c(\u03b5) = \u03b5 and c(uv) = c(u)c(v) s.t. it suffices to define c(a) for all a \u2208 \u03a3.\nc is said to uniquely decodable if c is injective.\n\u25b7 Remark:\nL Let c: \u03a3\u2217 \u2192 {0, 1}\u2217 be a code and set C := c(\u03a3). wlog |C| = |\u03a3|.\nS1 := {v | w = uv \u2208 C, u\u2208 C} \\ {\u03b5}\nSi+1 := {v | w = uv \u2208 Si, u\u2208 C} \u222a {v | w = uv \u2208 C, u\u2208 Si}\nc is uniquely decodable iff \u03b5 \u0338\u2208 Si for all i \u2208 N 0.\n\u25b7 Informally, the Si consist of the possible differences between c(a1 . . . al) and\nc(a\u2032\n1 . . . a\u2032\nl\u2032) depending on which of the two words is longer.\n\u25b7 We can always assume that a1 \u0338= a\u2032\n1 and thus also c(a1) \u0338= c(a\u2032\n1).\n\u25b7 S1 consists exactly of these initial differences; Si+1 is then obtained by either\nextending a1 . . . al or a\u2032\n1 . . . a\u2032\nl\u2032.\n\u25b7 As Si only consists of suffixes of C, eventually Si = Sj for some 0 \u2264 i < j.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Prefix codes and Huffman\u2019s algorithm 212 (582)\n\u03b5\n0 1\n00 01 10 11\n000 001 010 011 100 101 110 111\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\na1 a2 a3 a4 a5\n\u25b7 Recall: any finite alphabet \u03a3 can be encoded using binary alphabet.\n\u25b7 The simplest approach is uniform encoding (\u201cblock code\u201d):\n\u25b7 Fix an enumeration \u03a3 = {a1, . . . , ak}\n\u25b7 Fix n s.t. k \u2264 2n.\n\u25b7 Map ak e.g. to the n-bit MSBF representation of k \u2212 1:\na1 7\u2192 0n\u2212200 a2 7\u2192 0n\u2212201 a3 7\u2192 0n\u2212210 . . .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Prefix codes and Huffman\u2019s algorithm 212 (583)\n\u03b5\n0 1\n00 01 10 11\n000 001 010 011 100 101 110 111\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\na1 a2 a3 a4 a5\n\u25b7 In other words: We use the leaves {0, 1}n of the perfect binary tree of height\nn for representing a1, . . . , ak.\n\u25b7 If k <2n we therefore waste leaves (=code words).\n\u25b7 The better approach is to prune the tree down to exactly k leaves.\n\u25b7 Of which there are exactly Ck\u22121 = 1\nk\n\u00002(k\u22121)\nk\u22121\n\u0001\n.\n(k \u2212 1 is the number of inner nodes.)\nSee DS/Catalan numbers or prove the recursion Ck+1 = Pk\nl=0 ClCk\u2212l.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Prefix codes and Huffman\u2019s algorithm 212 (584)\n\u03b5\n0 1\n00 01\n000 001 010 011\n0 1\n0 1\n0 1 0 1\na1 a4 a5 a3\na2\n\u03b5\n0 1\n00 01 10 11\n100 101\n0 1\n0 1 0 1\n0 1\na1\na2\na3\na4a5\n\u25b7 In other words: We use the leaves {0, 1}n of the perfect binary tree of height\nn for representing a1, . . . , ak.\n\u25b7 If k <2n we therefore waste leaves (=code words).\n\u25b7 The better approach is to prune the tree down to exactly k leaves.\n\u25b7 Of which there are exactly Ck\u22121 = 1\nk\n\u00002(k\u22121)\nk\u22121\n\u0001\n.\n(k \u2212 1 is the number of inner nodes.)\nSee DS/Catalan numbers or prove the recursion Ck+1 = Pk\nl=0 ClCk\u2212l.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Prefix codes and Huffman\u2019s algorithm 212 (585)\n\u03b5\n0 1\n00 01\n000 001 010 011\n0 1\n0 1\n0 1 0 1\na1 a4 a5 a3\na2\n\u03b5\n0 1\n00 01 10 11\n100 101\n0 1\n0 1 0 1\n0 1\na1\na2\na3\na4a5\n\u25b7 Every finite binary tree corresponds to a finite prefix-closed subset of {0, 1}\u2217,\n\u25b7 Every inner node u has exactly two children u0 and u1.\nand its leaves give rise to prefix (free) code which can be used as alphabet.\n\u25b7 The tree gives rise to a DFA/finite state transducer for decoding:\n\u25b7 MyHill-Nerode: The tree yields a minimal DFA with initial state \u03b5 and finial\nstates the leaves/code words wi.\n\u25b7 For the transducer: add \u03b5-transitions from the leaves/code words wi to the\nroot that emit the corresponing symbol ai.\n(Contract the \u03b5-transitions e.g. by skipping the leaves.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Prefix codes and Huffman\u2019s algorithm 212 (586)\n\u03b5\n0 1\n00 01\n000 001 010 011\n0 1\n0 1\n0 1 0 1\nE(13)\nT(9)\nA(8)O(7) I(7)\n(114) \u03b5\n0 1\n00 01 10 11\n100 101\n0 1\n0 1 0 1\n0 1\nE(13)\nT(9)\nA(8)\nO(7)I(7)\n(109)\n\u25b7 Often, each symbol ai will be assigned some weight \u03b3i e.g. its absolute or\nrelative frequency.\n\u25b7 Already single letter frequencies can be used e.g. to break simple\ncryptographic schemes (cf. frequency analysis).\n\u25b7 See e.g. letter frequencies on wikipedia\n\u25b7 Remember, the concept of \u201calphabet\u201d is quite general: the ai can be lexemes,\nwords, vectors, tuples, n-grams themselves which can be used to model some\ncontext-dependent frequencies.\nI.e. instead of \u03a3 = {A, B, . . . ,Z} you can also use \u03a32 = {AA, AB, . . . ,ZZ} as\n\u201csource alphabet\u201d which can help to reduce the average/expected code word\nlength.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Prefix codes and Huffman\u2019s algorithm 212 (587)\n\u03b5\n0 1\n00 01\n000 001 010 011\n0 1\n0 1\n0 1 0 1\nE(13)\nT(9)\nA(8)O(7) I(7)\n(114) \u03b5\n0 1\n00 01 10 11\n100 101\n0 1\n0 1 0 1\n0 1\nE(13)\nT(9)\nA(8)\nO(7)I(7)\n(109)\nD Assume with every symbol ai a positive weight \u03b3i \u2208 R >0 is associated.\n\u25b7 Often, relative frequencies or probabilities are used: \u03b3i 7\u2192 \u03b3i\n\u03b31+...+\u03b3k\nLet wi be the leaf/code word assigned to ai. Then\nkX\ni=1\n\u03b3i|wi|\nis the weighted code (word) length resp. weighted (tree) height.\nIf Pk\ni=1 \u03b3i = 1, then this is the expected/average code (word) length resp.\nexpected/average (tree) height.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Prefix codes and Huffman\u2019s algorithm 212 (588)\n\u03b5\n0 1\n00 01 10 11\n100 101\n0 1\n0 1 0 1\n0 1\nE(13)\nT(9)\nA(8)\nO(7)I(7)\n(109) \u03b5\n0 1\n00 01 10 11\n0 1\n0 1 0 1\nT(9) [AE](21) O(7)I(7)\n(88)\n\u25b7 The question is thus how to determine a tree of minimal weighted height:\n\u25b7 Consider a minimal tree for given weights \u03b31 \u2265 . . .\u2265 \u03b3k.\nwlog we may assume that \u03b3k\u22121 and \u03b3k are assigned to two leaves wk\u22121 = u0\nand wk = u1 with a common parent u.\nTreat ak\u22121 and ak as a single symbol [ak\u22121, ak] encoded by u.\nThe weighted height of the reduced tree is then:\nX\ni\u2208[k]\n|wi|\u03b3i \u2212 (|u0|\u03b3k\u22121 + |u1|\u03b3k) +|u|(\u03b3k\u22121 + \u03b3k) =\nX\ni\u2208[k]\n|wi|\u03b3i \u2212 (\u03b3k\u22121 + \u03b3k)\nAs the original tree is minimal, also the reduced tree has to be minimal as\nsplitting u again into u0 and u1 increases the weight always by \u03b3k\u22121 + \u03b3k.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Prefix codes and Huffman\u2019s algorithm 212 (589)\n\u03b5\n0 1\n00 01 10 11\n100 101\n0 1\n0 1 0 1\n0 1\nE(13)\nT(9)\nA(8)\nO(7)I(7)\n(109)\nE(13) T(9) A(8) I(7) O(7)\n[IO](14) E(13) T(9) A(8)\n[AT](17) [IO](14) E(13)\n[[IO]E](27) [AT](17)\n[[[IO]E][AT]](44)\nD This leads to the algorithm by Huffman:\n\u25b7 Sort the symbols a1, . . . , ak\u22121, ak s.t. \u03b31 \u2265 . . .\u2265 \u03b3k\u22121 \u2265 \u03b3k.\n\u25b7 If k \u2265 2, reduce the symbols to a1, . . . , ak\u22122, [ak\u22121, ak] with [ak\u22121, ak] a new\nsymbol of weight \u03b3k\u22121 + \u03b3k and proceed recursively.\n\u25b7 If k = 1, the nesting of the brackets (Dyck word) of the final single combined\nsymbol encodes the binary tree of the prefix code ( wlog initially k \u2265 2).\n\u25b7 Not required, but useful: I.e. extend the weights so that the second component\nremembers the height of the resulting subtree: \u03b3i 7\u2192 (\u03b3i, 0) with\n(\u03b3, h) + (\u03b3\u2032, h\u2032) := (\u03b3 + \u03b3\u2032, max(h, h\u2032) + 1) and \u201c\u2264\u201d now the lexico. order.\nThe resulting code is called a Huffman code.\nF The Huffman algorithm yields a binary tree of minimal weighted height.",
      "remembers the height of the resulting subtree: \u03b3i 7\u2192 (\u03b3i, 0) with\n(\u03b3, h) + (\u03b3\u2032, h\u2032) := (\u03b3 + \u03b3\u2032, max(h, h\u2032) + 1) and \u201c\u2264\u201d now the lexico. order.\nThe resulting code is called a Huffman code.\nF The Huffman algorithm yields a binary tree of minimal weighted height.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Prefix codes and Huffman\u2019s algorithm 212 (590)\n\u03b5\n0 1\n00 01 10 11\n100 101\n0 1\n0 1 0 1\n0 1\nE(13)\nT(9)\nA(8)\nO(7)I(7)\n(109) \u03b5\n0 1\n00 01 10 11\n000 001\n0 1\n0 1 0 1\n0 1 E(13) A(8) T(9)\nO(7)I(7)\n(102)\nD This leads to the algorithm by Huffman:\n\u25b7 Sort the symbols a1, . . . , ak\u22121, ak s.t. \u03b31 \u2265 . . .\u2265 \u03b3k\u22121 \u2265 \u03b3k.\n\u25b7 If k \u2265 2, reduce the symbols to a1, . . . , ak\u22122, [ak\u22121, ak] with [ak\u22121, ak] a new\nsymbol of weight \u03b3k\u22121 + \u03b3k and proceed recursively.\n\u25b7 If k = 1, the nesting of the brackets (Dyck word) of the final single combined\nsymbol encodes the binary tree of the prefix code ( wlog initially k \u2265 2).\n\u25b7 Not required, but useful: I.e. extend the weights so that the second component\nremembers the height of the resulting subtree: \u03b3i 7\u2192 (\u03b3i, 0) with\n(\u03b3, h) + (\u03b3\u2032, h\u2032) := (\u03b3 + \u03b3\u2032, max(h, h\u2032) + 1) and \u201c\u2264\u201d now the lexico. order.\nThe resulting code is called a Huffman code.\nF The Huffman algorithm yields a binary tree of minimal weighted height.",
      "remembers the height of the resulting subtree: \u03b3i 7\u2192 (\u03b3i, 0) with\n(\u03b3, h) + (\u03b3\u2032, h\u2032) := (\u03b3 + \u03b3\u2032, max(h, h\u2032) + 1) and \u201c\u2264\u201d now the lexico. order.\nThe resulting code is called a Huffman code.\nF The Huffman algorithm yields a binary tree of minimal weighted height.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Huffman codes: examples 213 (591)\n\u25b7 The Huffman algorithm is nondeterministic in the choice of the least weights;\nthus, in general there will be several non-isomorphic solutions, e.g.:\nA(2) B(1) C(1) D(1) E(1)\ncan be combined to both\n[[A[DE]][BC]](6) [A[[DE][BC]]](6)\n\u03b5\n0 1\n00 01 10 11\n010 011\n0 1\n0 1 0 1\n0 1A(2)\nD(1) E(1)\nO(1) I(1)\n(14)\n\u03b5\n0 1\n10 11\n100 101 110 111\n0 1\n0 1\n0 1 0 1\nA(2)\nD(1) E(1) O(1) I(1)\n(14)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Huffman codes and Fibonacci numbers 214 (592)\n! Assume \u03b31 \u2264 . . .\u2264 \u03b3k (for simplicity). If Pr\ni=1 \u03b3i < \u03b3r+2, then the Huffman\nalgorithm will deterministically merge the weights in the given order.\n\u25b7 For instance [[[[20 + 20] + 21] + . . .] + 2r] = 2r+1 < 2r+2.\n\u25b7 If \u03b3i \u2208 N , the least possible choice for \u03b3r+2 is Pr\ni=1 \u03b3i + 1.\nStarting with \u03b31 := 1 this yields the Fibonacci numbers:\nF0 := 0 F1 := 1 Fk+2 = Fk+1 + Fk\nkX\ni=0\nFi = Fk+2 \u2212 1\n\u03b5\n0 1\n00 01\n000 001\n0000 0001\n00000 00001\n0 1\n0 1\n0 1\n0 1\n0 1\n(8)\n(5)\n(3)\n(2)\n(1) (1)\n(45)\n\u03b5\n0 1\n00 01 10 11\n100 101 110 111\n0 1\n0 1 0 1\n0 1 0 1(8) (5)\n(3) (2) (1) (1)\n(47)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Huffman codes and Fibonacci numbers 215 (593)\nL (cf. here)\nAssuming that \u03b3 = Pk\ni=1 \u03b3i and \u03b31 \u2265 . . .\u2265 \u03b3k:\nIf FK+2 < \u03b3\n\u03b3k\n\u2264 FK+3, then\n\u25b7 the longest code word of a Huffman code will always have length at most K,\n\u25b7 the remaining weights \u03b31, . . . , \u03b3k\u22121 can be chosen in such way that at least\none code word will have length K.\n\u25b7 As \u03a6k\u22122 < Fk < \u03a6k\u22121 for \u03a6 = 1+\n\u221a\n5\n2 we have that in the worst-case the\nlongest code word will have length\n\u2248 1 + log\u03a6\n\u03b3\n\u03b3k\n\u2248 1 + 1.4404 log2\n\u03b3\n\u03b3k\n\u25b7 As we will see, there is always a (prefix) code with code word lengths\n\u2308log2\n\u03b3\n\u03b3i\n\u2309, i.e. a Huffman code will in general not minimize the longest code\nword length (absolute height), but only the weighted code word length\n(weighted height).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "8 Basics of information theory: Shannon entropy, Huffman codes, compression\nPrefix codes and Huffman\u2019s algorithm\nOptimal codes and Shannon\u2019s entropy\nEntropy and information\nArithmetic codes\nData compression\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Outline 217 (595)\n\u25b7 Simpler to use pa = \u03b3aP\na \u03b3a\nrelative frequencies/probabilities from now on.\n\u25b7 Only re-scales the minimum, so wlog.\n\u25b7 (pa)a\u2208\u03a3 with pa \u2208 [0, 1], P\na pa = 1 is called a distribution.\n\u25b7 Story so far:\nGiven a distribution (pa), Huffman\u2019s algorithm yields a prefix-code\nc: \u03a3 \u2192 {0, 1}\u2217 that minimizes the expected code word length/tree heightP\na pa|c(a)|.\n\u25b7 1. Question: Optimality of Huffman codes\nIs there some uniquely decodable code that can achieve a better expected\ncode word length?\n\u25b7 2. Question: Minimal expected code word length\nWhat is the best we can obtain for P\na pa|c(a)|?\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Outline: Optimality of Huffman codes 218 (596)\n\u25b7 1. Question: Is there some uniquely decodable code that can achieve a better\nexpected code word length?\n1 Kraft\u2019s inequality:\nFor every uniquely decodable code c: \u03a3 \u2192 {0, 1}\u2217 we have P\na 2\u2212|c(a)| \u2264 1.\n2 Given code word lengths la \u2208 N 0 with P\na 2\u2212la \u2264 1,\nwe can always construct a prefix code c: \u03a3 \u2192 {0, 1}\u2217 so that |c(a)| = la.\n3 Consequence: For every uniquely decodable code \u02c6c there is a prefix code c so\nthat |c(a)| = |\u02c6c(a)|.\nThus, Huffman codes also obtain the minimal expected code word length wrt.\nall uniquely decodable codes.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kraft\u2019s inequality 219 (597)\nL Let c: \u03a3\u2217 \u2192 {0, 1}\u2217 be a uniquely decodable code. Then P\na\u2208\u03a3 2\u2212|c(a)| \u2264 1.\n\u25b7 Set L := max{|c(a)|: a \u2208 \u03a3}. For m \u2208 N consider\n X\na\u2208\u03a3\n2\u2212|c(a)|\n!m\n=\nX\nw=a1...am\u2208\u03a3m\n2\u2212|c(w)| =\nm\u00b7LX\nl=1\nnm,l2\u2212l\nwhere nm,l = |{w \u2208 c(\u03a3m): |w| = l} is then number of words in \u03a3m that are\nmapped on a code word of length l.\nIf c is uniquely decodable, then nm,l \u2264 2l has to hold, and thus\n X\na\u2208\u03a3\n2\u2212|c(a)|\n!m\n\u2264\nm\u00b7LX\nl=1\n2l2\u2212l = m \u00b7 L i.e.\nX\na\u2208\u03a3\n2\u2212|c(a)| \u2264 (m \u00b7 L)1/m\nNote that this has to hold for all m, but the LHS does not depend on m.\nAs (Lm)1/m = e1/m log m+1/m log L m\u2192\u221e\n\u2212 \u2212 \u2212 \u2212 \u2192e0 = 1 the claim follows.\n! This is only a necessary condition for a code to be uniquely decodable.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kraft\u2019s inequality 219 (598)\nL Let c: \u03a3\u2217 \u2192 {0, 1}\u2217 be a uniquely decodable code. Then P\na\u2208\u03a3 2\u2212|c(a)| \u2264 1.\nL For an alphabet \u03a3 let la \u2208 N s.t. P\na\u2208\u03a3 2\u2212la \u2264 1.\nThen there also exists a prefix code c with |c(a)| \u2264la.\n\u25b7 Set nl = {a \u2208 \u03a3 | la = l} and L := maxa la.\nWe define greedily a prefix code by pruning the perfect binary tree of height L\nwhich has initially 2L leaves, namely {0, 1}L:\nFor l from 0 to L, simply turn nl nodes on level l into leaves by pruning the\ncorresponding subtrees.\nThis removes nl2L\u2212l leaves from the perfect tree as a node on level l has\nexactly 2L\u2212l leaves as descendants.\nAs PL\nl=0 nl2L\u2212l = 2L Pl\nl=0 nl2\u2212l \u2264 2L we cannot run out of leaves.\nC Huffman codes are optimal wrt. to all uniquely decodable codes.\n\u25b7 For every uniquely decodable code c\u2032 there exists a prefix code c s.t.\n|c(a)| \u2264 |c\u2032(a)| for all a \u2208 \u03a3.\nThus P\na \u03b3a|c(a)| \u2264P\na \u03b3a|c\u2032(a)| for all (\u03b3a) \u2208 R \u03a3\n\u22650.",
      "As PL\nl=0 nl2L\u2212l = 2L Pl\nl=0 nl2\u2212l \u2264 2L we cannot run out of leaves.\nC Huffman codes are optimal wrt. to all uniquely decodable codes.\n\u25b7 For every uniquely decodable code c\u2032 there exists a prefix code c s.t.\n|c(a)| \u2264 |c\u2032(a)| for all a \u2208 \u03a3.\nThus P\na \u03b3a|c(a)| \u2264P\na \u03b3a|c\u2032(a)| for all (\u03b3a) \u2208 R \u03a3\n\u22650.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Outline: Minimal expected code word length 220 (599)\n\u25b7 2. Question: What is the best we can obtain for P\na pa|c(a)|?\n1 First observation: for every distribution (pa) we have\n1 =\nX\na\npa =\nX\na\n2log2 pa =\nX\na\n2\u2212log2 p\u22121\na \u2265\nX\na\n2\u2212\u2308log2 p\u22121\na \u2309\ni.e. there is a prefix code c\u2032 with |c\u2032(a)| = \u2308log2 p\u22121\na \u2309.\nThus for every Huffman code c we have\nX\na\npa|c(a)| \u2264\nX\na\npa|c\u2032(a)| =\nX\na\npa\u2308log2 p\u22121\na \u2309 \u22641 +\nX\na\npa log2 p\u22121\na\nH(p) := P\na pa log2 p\u22121\na is called the entropy of p.\n2 For every uniquely decodable code c we have\nH(p) \u2264\nX\na\npa|c(a)|\nRequires Gibbs\u2019 inequality, Jensen\u2019s inequality and the convexity of x log x.\n3 Consequence: (Shannon\u2019s source coding theorem) for every Huffman code\nH(p) \u2264\nX\na\npa|c(a)| \u22641 + H(p)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Convex functions and Jensen\u2019s inequality 221 (600)\nD A function f : [a, b] \u2192 R is convex if for all \u03bb \u2208 [0, 1] and x1, x2 \u2208 [a, b]\nf(\u03bbx1 + (1 \u2212 \u03bb)x2) \u2264 \u03bbf(x1) + (1\u2212 \u03bb)f(x2)\nIf \u2264 can be strengthened to <, then f is strictly convex.\nF If f : [a, b] \u2192 R is convex, it is continuous on (a, b) and every local minimum\nis globally minimal; if f is strictly convex, it has a unique global minimum.\nF \u2212log x is strictly convex on [a, 1] for any a \u2208 (0, 1]:\nx log x is strictly convex on [0, 1] (with limx\u21920+ x log x = 0).\nx\ny\nx\ny\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Convex functions and Jensen\u2019s inequality 221 (601)\nD A function f : [a, b] \u2192 R is convex if for all \u03bb \u2208 [0, 1] and x1, x2 \u2208 [a, b]\nf(\u03bbx1 + (1 \u2212 \u03bb)x2) \u2264 \u03bbf(x1) + (1\u2212 \u03bb)f(x2)\nIf \u2264 can be strengthened to <, then f is strictly convex.\nL Jensen\u2019s inequality: If f : [a, b] \u2192 R is convex, then also\nf(\nkX\ni=1\n\u03bbixi) \u2264\nkX\ni=1\n\u03bbif(xi)\nfor all xi \u2208 [a, b] and \u03bbi \u2208 [0, 1] with Pk\ni=1 \u03bbi = 1\n\u25b7 Induction on k: f(Pk\ni=1 \u03bbixi + \u03bbk+1xk+1) \u2264 f(Pk\ni=1 \u03bbixi) + \u03bbk+1f(xk+1).\n\u25b7 Convexity generalizes linearity.\n\u25b7 Jensen\u2019s inequality extends to the expected value of random variables over\narbitrary probability spaces.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Convex functions and Jensen\u2019s inequality 221 (602)\nD A function f : [a, b] \u2192 R is convex if for all \u03bb \u2208 [0, 1] and x1, x2 \u2208 [a, b]\nf(\u03bbx1 + (1 \u2212 \u03bb)x2) \u2264 \u03bbf(x1) + (1\u2212 \u03bb)f(x2)\nIf \u2264 can be strengthened to <, then f is strictly convex.\nD For X a finite set D(X) := {(pa)a\u2208X \u2208 [0, 1]X | P\na\u2208X pa = 1}.\nC Let f : [a, b] \u2192 R be convex and x1, . . . , xk \u2208 [a, b]. Then for all p \u2208 D([k]):\nf(\nkX\ni=1\nxipi) \u2264\nkX\ni=1\npif(xi)\nFurther (for k fixed)\nF : D([k]) \u2192 R , p7\u2192\nkX\ni=1\nf(pixi)\nis also convex, i.e. for all \u03bb \u2208 [0, 1] and p, q\u2208 D([k])\nF(\u03bbp+\u00b5q) \u2264\nkX\ni=1\nf(\u03bbpixi+\u00b5qixi) \u2264\nkX\ni=1\n\u03bbf(pixi)+\u00b5f(qixi) = \u03bbF(p)+\u00b5F(q)\nIf f is strictly convex, so is F.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Gibbs\u2019 inequality and entropy 222 (603)\nL Gibbs\u2019 inequality: For every fixed p \u2208 D([k]) the function\nHp : D([k]) \u2192 R , q7\u2192 \u2212\nkX\ni=1\npi log qi\nhas a global maximum at p with Hp(p) \u2264 log(k).\n\u25b7 wlog q >0 as \u2212log 0 = +\u221e. Let f(x) := x log(x). Then\n0 = f(1) = f(\nkX\ni=1\npi) = f(\nkX\ni=1\nqi\npi\nqi\n) \u2264\nkX\ni=1\nqif(pi\nqi\n) =\nkX\ni=1\npi (log pi \u2212 log qi)\ni.e. Hp(p) \u2264 Hp(q). In particular thus Hp(p) \u2264 Hp(1/k, . . . ,1/k) = log k.\nD Let \u03a3 be a finite alphabet and p \u2208 D(\u03a3). The entropy of p is\nH(p) := Hp(p) = \u2212\nX\na\u2208\u03a3\npa log pa \u2264 log2 |\u03a3|\nH(p) = log2 |\u03a3| iff pa = 1\n|\u03a3| for all a \u2208 \u03a3. (\u2212H is a strictly convex on D([k]).)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Minimal expected code word length 223 (604)\nT Let c: \u03a3 \u2192 {0, 1}\u2217 be a uniquely decodable code. Then\nX\na\npa|c(a)| \u2265H(p)\nhas to hold for every distribution p \u2208 D(\u03a3).\n\u25b7 Set \u03b3a = 2\u2212|c(a)| and \u03b3 = P\na \u03b3a.\nBy Kraft\u2019s inequality we have \u03b3 \u2264 1, i.e. log2 \u03b3 \u2264 0.\nThen choose qi := \u03b3i/\u03b3 in Gibbs\u2019 inequality:\nH(p) = Hp(p) \u2264 Hp(q) =\nX\na\u2208\u03a3\npa log2 q\u22121\ni =\nX\na\u2208\u03a3\npa(|c(a)| + log2 \u03b3)\nHence:\nX\na\u2208\u03a3\npa|c(a)| \u2265 \u2212log2 \u03b3 \u2212\nX\na\u2208\u03a3\npa log2 pa \u2265 \u2212\nX\na\u2208\u03a3\npa log2 pa\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Shannon\u2019s source coding theorem for symbol codes 224 (605)\nT For every finite alphabet \u03a3 and p \u2208 D(\u03a3) there exists a prefix code with\nH(p) \u2264\nX\na\npa|c(a)| < 1 + H(p)\nIn particular, this has to hold for any (optimal) Huffman code for p.\n\u25b7 Set la := \u2308log2 p\u22121\na \u2309 = \u2212\u230alog2 pa\u230b. Then\nX\na\u2208\u03a3\n2\u2212la =\nX\na\u2208\u03a3\n2\u230alog2 pa\u230b \u2264\nX\na\u2208\u03a3\npa = 1\nHence, there exists a prefix code with |c(a)| \u2264la \u2264 1 + log2 p\u22121\na and\nX\na\u2208\u03a3\n|c(a)|pa \u2264\nX\na\u2208\u03a3\npa(1 \u2212 log2 pa) = 1 + H(p).\n(In fact, P\na pa|c(a)| < 1 + H(p) as la < 1 + log2 p\u22121\na for at least one a.)\n\u25b7 Recall that in general a Huffman code will node minimize the maximal code\nword length, i.e. |c(a)| > \u2308log2 p\u22121\na \u2309 is possible for some symbols.\n\u25b7 The uniform distribution maximizes the entropy, i.e. the expected code word\nlength.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Block alphabets for reducing expected code length 225 (606)\n\u25b7 Assuming that a sender randomly emits symbols from \u03a3 with fixed\nprobabilities pa, the average number of bits transmitted by symbol is\nH(p) \u2264\nX\na\npa|c(a)| < H(p) + 1\ni.e. on average we might transmit one bit more then required.\n\u25b7 If we instead compute a Huffman code c\u2032 for \u03a32 wrt. p\u2032\nab := papb, then\nH(p\u2032) = \u2212\nX\na,b\npapb log2(papb) = 2H(p)\nand thus\n2H(p) = H(p\u2032) \u2264\nX\na\np\u2032\nab|c\u2032(ab)| < H(p\u2032) + 1 = 2H(p) + 1\ni.e. now we transmit only 0.5 \u201cunnecessary bits\u201d per original symbol.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Block alphabets for reducing expected code length 225 (607)\n\u25b7 Assuming that a sender randomly emits symbols from \u03a3 with fixed\nprobabilities pa, the average number of bits transmitted by symbol is\nH(p) \u2264\nX\na\npa|c(a)| < H(p) + 1\ni.e. on average we might transmit one bit more then required.\n\u25b7 Wrt. \u03a3n (\u201cn-grams\u201d) let p\u2297n denote the distribution on \u03a3n with\np\u2297n\na1...an :=\nnY\ni=1\npai\nthen\nH(p\u2297n) = \u2212\nX\na1,...,an\u2208\u03a3\npa1 \u00b7\u00b7\u00b7 pan log2(pa1 \u00b7\u00b7\u00b7 pan) = nH(p)\nso that for every Huffman code cn for \u03a3n wrt. p\u2297n we have\nnH(p) = H(p\u2297n) \u2264 \u2212\nX\nw\u2208\u03a3n\np\u2297n\nw |cn(w)| < H(p\u2297n) + 1 = nH(p) + 1\ni.e. we can reduce the average overhead to 1/n.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Block alphabets for reducing expected code length 225 (608)\n\u25b7 Assuming that a sender randomly emits symbols from \u03a3 with fixed\nprobabilities pa, the average number of bits transmitted by symbol is\nH(p) \u2264\nX\na\npa|c(a)| < H(p) + 1\ni.e. on average we might transmit one bit more then required.\n\u25b7 Better models for a sender/source can be obtained by Markov chains:\n\u25b7 Assume L \u2286 \u03a3\u2217 is a finite/regular language of possible words to be sent.\n\u25b7 Construct a DFA M for (L{$})\u2217 and define for every state q of M the\nconditional probability pa|q that a \u2208 \u03a3 will be emitted.\n\u25b7 The DFA wrt. these probabilities is called a Markov chain: starting in the initial\nstate q0, the \u201crandom surfer\u201d chooses in state qi the next symbol ai \u2208 \u03a3 to be\nsent with probability pai|qi and then transitions to qiai \u2212 \u2192\u2217\nM qi+1.\n\u25b7 If there are no transmission errors, we can use for each state q a Huffman code\nwrt. pa|q as the receiver can reconstruct the run of the DFA.",
      "state q0, the \u201crandom surfer\u201d chooses in state qi the next symbol ai \u2208 \u03a3 to be\nsent with probability pai|qi and then transitions to qiai \u2212 \u2192\u2217\nM qi+1.\n\u25b7 If there are no transmission errors, we can use for each state q a Huffman code\nwrt. pa|q as the receiver can reconstruct the run of the DFA.\nAssuming that the sender generates an infinite stream of words we can at least\nuse the Cesaro limit to obtain better average probabilities for emitting a \u2208 \u03a3.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "8 Basics of information theory: Shannon entropy, Huffman codes, compression\nPrefix codes and Huffman\u2019s algorithm\nOptimal codes and Shannon\u2019s entropy\nEntropy and information\nArithmetic codes\nData compression\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Discrete probability spaces 227 (610)\nD A discrete probability space (\u2126, Pr[\u00b7]) consists of countable set \u2126 and a map\nPr[] : 2\u2126 \u2192 [0, 1] with Pr[\u2126] = 1 and Pr[A] =\nX\n\u03c9\u2208A\nPr[\u03c9] for all A \u2286 \u2126\n\u03c9 \u2208 \u2126 is called an elementary event and A \u2286 \u2126 an event.\nSet A = \u2126 \\ A s.t. Pr[\u00acA] := Pr\n\u0002\nA\n\u0003\n= 1 \u2212 Pr[A].\nGiven two events A, Bset Pr[A, B] := Pr[A \u2227 B] := Pr[A \u2229 B].\nThe (conditional) probability of A conditioned on B is\nPr[A|B] := Pr[A, B]\nPr[B] with 0\n0 := 1 Pr[ A|B] Pr[B] = Pr[B|A] Pr[A]\n(i.e. prob. that both A, Bhappen relative to prob. that B happens)\n\u25b7 If A \u2286 U\ni Bi, then Pr[A] = P\ni Pr[A|Bi] \u00b7 Pr[Bi].\nA, Bare (stochastically) independent if Pr[A, B] = Pr[A] Pr[B].\n\u25b7 I.e. if A, Bare independent, then Pr[A|B] = Pr[A].\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Discrete probability spaces 227 (611)\nD A discrete probability space (\u2126, Pr[\u00b7]) consists of countable set \u2126 and a map\nPr[] : 2\u2126 \u2192 [0, 1] with Pr[\u2126] = 1 and Pr[A] =\nX\n\u03c9\u2208A\nPr[\u03c9] for all A \u2286 \u2126\nA random variable is a map X : \u2126 \u2192 EX; its distribution is\nPr[X = x] = Pr[{\u03c9 \u2208 \u2126 | X(\u03c9) = x}]\n(event \u201c[X = x]\u201d: experiment X has the outcome x.)\nIf EX \u2286 R , its expected value and variance are\nE [X] :=\nX\nx\u2208EX\nx \u00b7Pr[X = x] Var[X] := E [(X \u2212E [X])2] = E [X2] \u2212E [X]2\n(E is linear: E [\u03bbX + \u00b5Y ] = \u03bbE [X] + \u00b5E [Y ] with E [1] = 1.)\nX, Yare independent if\nPr[X = x, Y= y] = Pr[X = x] Pr[Y = y] for all x \u2208 EX, y\u2208 EY\n(Jensen\u2019s inequality: if f : R \u2192 R is convex, then f(E [X]) \u2264 E [f(X)].)No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Discrete probability spaces 227 (612)\nD A discrete probability space (\u2126, Pr[\u00b7]) consists of countable set \u2126 and a map\nPr[] : 2\u2126 \u2192 [0, 1] with Pr[\u2126] = 1 and Pr[A] =\nX\n\u03c9\u2208A\nPr[\u03c9] for all A \u2286 \u2126\nD Laplacian probability space: \u2126 a finite set and Pr[A] := |A|\n|\u2126| (i.e. Pr[\u03c9] = 1\n|\u2126|).\n\u25b7 Example: To model two (independent and fair) dice rolls choose \u2126 = [6] \u00d7 [6],\nand let Xi : \u2126 \u2192 [6], (x1, x2) 7\u2192 xi be the (outcome of the) i-th dice roll.\nPr[\u201cTwo 6\u201d] = Pr[ X1 = X2 = 6] = |{(6,6)}|\n|[6]2| = 1\n36\nPr[\u201cDoublets\u201d] = Pr[ X1 = X2] = |{(i,i)|i\u2208[6]}|\n|[6]2| = 6\n36 = 1\n6\nPr[\u201cNo 6\u201d] = Pr[ X1 \u0338= 6, X2 \u0338= 6] = |[5]2|\n|[6]2| = 25\n36\nPr[\u201cAt least one 6\u201d ] = 1 \u2212 Pr[\u201cNo 6\u201d] = 1 \u2212 25\n36 = 11\n36\nThe dice rolls are indeed \u201cfair\u201d (equiprobable, uniformly distributed) :\nPr[X1 = i] = |{i}\u00d7[6]|\n|[6]2| = 1\n6 = Pr[X2 = i] E [Xi] = 1\n6\n6X\ni=1\ni = 7\n2\nand also (stochastically) independent:\nPr[X1 = i, X2 = j] = |{(i,j)}|\n|[6]2| = 1\n36 = 1\n6 \u00b7 1\n6 = Pr[X1 = i] \u00b7 Pr[X2 = j]",
      "36 = 11\n36\nThe dice rolls are indeed \u201cfair\u201d (equiprobable, uniformly distributed) :\nPr[X1 = i] = |{i}\u00d7[6]|\n|[6]2| = 1\n6 = Pr[X2 = i] E [Xi] = 1\n6\n6X\ni=1\ni = 7\n2\nand also (stochastically) independent:\nPr[X1 = i, X2 = j] = |{(i,j)}|\n|[6]2| = 1\n36 = 1\n6 \u00b7 1\n6 = Pr[X1 = i] \u00b7 Pr[X2 = j]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Probabilities on words 228 (613)\n\u25b7 Consider the discrete probability space with Pr[w] = 1/4 and\n\u2126 = {THESIS, THERMODYNAMICS, THEOREM, THEORY}\nLet Xi : \u2126 \u2192 {A, . . . ,Z, \u22a5} be the i-th letter, if defined, else \u22a5:\n(Visualize \u2126 as prefix tree, then Xi is the position ofter i steps.)\n\u25b7 Pr[X1 = T] = Pr[X2 = H] = Pr[X3 = E] = 1.\n\u25b7 Pr[X4 = S] = 1\n4 , Pr[X4 = R] = 1\n4 , Pr[X4 = O] = 1\n2 .\n\u25b7 Pr[X5 = R|X4 = O] = 1, Pr[X5 = I|X4 = S] = 1, Pr[X7 = M|X7 \u0338= \u22a5] = 1\n2\n\u25b7 X1, X2, X3 are trivially independent, but e.g. X4, X5 are dependent.\n\u25b7 Let L: \u2126 \u2192 N 0 be the random variable that returns the length:\n\u25b7 Pr[L = 6] = 1\n2 , Pr[L = 7] = 1\n4 , Pr[L = 14] = 1\n4 ,\nE [L] = 8.25, Var[L] = 11.1875\n\u25b7 Pr[X4 = S|L \u2264 6] = 1\n2 , Pr[X4 = S|L \u2264 7] = 1\n3 , Pr[X4 = S|L \u2265 7] = 0\n! The conditioning on some event narrows down the possible elementary events\n(=situations).No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: Random walk on N 0 229(614)\n0 1 2 3\n. . .\n1 \u2212 p 1 \u2212 p 1 \u2212 p 1 \u2212 p\np p p p\n\u25b7 One-way random walk on N 0 (to infinity and beyond!) :\nStart at 0.\nIn every time step with prob. 1 \u2212 p \u201cstay\u201d and with prob. p \u201cmove\u201d.\n(\u2126 is the set of all infinite paths.)\n\u25b7 Prob. to move from 0 to 1 after k time steps: (1 \u2212 p)k\u22121p.\n(first k \u2212 1 times \u201cstay\u201d, then \u201cmove\u201d; geometrically distributed; waiting time.)\n\u25b7 Prob. of being at n after k time steps:\n\u0000k\nn\n\u0001\npn(1 \u2212 p)k\u2212n\n(n out of k times \u201cmove\u201d, rest \u201cstay\u201d; binomially distributed.)\n\u25b7 Prob. of reaching n after k time steps for the first time:\n\u0000k\u22121\nn\u22121\n\u0001\n(1 \u2212 p)k\u2212npn.\n(need to move to n exactly in the k-th step, so choose n \u2212 1 out of k \u2212 1 times to \u201cmove\u201d,\nrest \u201cstay\u201d; negative-binomially distributed; sum of geometric distribution.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Probability as measure for belief/plausibility/trust 230 (615)\n\u25b7 The Bayesian interpretation of probabilities is that of a measure of \u201cbelief\u201d\nthat some proposition A is true in a given situation (quantitative logics).\n\u25b7 This interpretation is motivated by Cox\u2019s theorem:\nCox\u2019s postulates/axioms (excerpt):\n\u25b7 Plausibility of a proposition A given some proposition (information) I is some\nreal number denoted by p(A|I) with p(A) := p(A|true).\n\u25b7 The plausibility p(\u00acA|I) of the negation \u00acA given I satisfies\np(\u00acA|I) = f(p(A|I)) for some fixed f : R \u2192 R .\n\u25b7 The plausibility p(A, B|I) of the conjunction A \u2227 B given I satisfies\np(A, B|I) = g(p(A|I), p(B|A, I)) for some fixed associative g : R 2 \u2192 R .\n\u25b7 Both f and g are monotonic.\nAny notion of belief/plausibility that satisfies these axioms can be shown to\ngive rise to some probability space.\nIn particular, conditional probabilities Pr[A|B] are then interpreted as the\nbelief that A is true given/assuming B is true.",
      "\u25b7 Both f and g are monotonic.\nAny notion of belief/plausibility that satisfies these axioms can be shown to\ngive rise to some probability space.\nIn particular, conditional probabilities Pr[A|B] are then interpreted as the\nbelief that A is true given/assuming B is true.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Shannon\u2019s characterization of information 231 (616)\n\u25b7 Similar to Cox\u2019s axioms (cf. also det), Shannon also characterized what a\nmeaningful measurement of \u201cinformation\u201d should satisfy:\n\u25b7 The information (content) I(A) of an event A \u2286 \u2126 with prob. p = Pr[A]\nshould be (strictly) mon. decreasing in p, in particular I(A) = 0 if p = 1.\n\u25b7 The information I(A, B) of two stochastically independent events A, B\u2286 \u2126\nshould be I(A, B) = I(A) + I(B), i.e. the information of two independent\nevents can be measured by observing each event on its own.\nOne obvious choice for I(A) is thus \u2212logb Pr[A] for some basis b >0.\n\u25b7 In fact, one can show that except for the choice of the basis it is also the only\nsolution where b = 2 is natural choice wrt. the codes.\n\u25b7 The entropy can thus be interpreted as the expected information of a\ndistribution; in particular for a (discrete) random variable X : \u2126 \u2192 EX:\nH(X) := \u2212\nX\nx\u2208EX\nPr[X = x] log2 Pr[X = x]",
      "solution where b = 2 is natural choice wrt. the codes.\n\u25b7 The entropy can thus be interpreted as the expected information of a\ndistribution; in particular for a (discrete) random variable X : \u2126 \u2192 EX:\nH(X) := \u2212\nX\nx\u2208EX\nPr[X = x] log2 Pr[X = x]\nIf Pr[X = x] = |EX|\u22121, then H(X) = log2 |EX| (bits required to describe EX).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Discrete probability spaces: Entropy 232 (617)\nD Let (\u2126, Pr[]) be some discrete probability space and X : \u2126 \u2192 EX,\nY : \u2126 \u2192 EY two random variables.\nThe entropy of X is the entropy of its distribution:\nH(X) := \u2212\nX\nx\u2208EX\nPr[X = x] log2 Pr[X = x]\nThe conditional entropy of a X wrt. an event A \u2286 \u2126 is\nH(X|A) := \u2212\nX\nx\u2208EX\nPr[X = x|A] log2 Pr[X = x|A]\nThe joint entropy of X, Yis the entropy of their joint distribution:\nH(X, Y) := \u2212\nX\nx\u2208EX,y\u2208EY\nPr[X = x, Y= y] log2 Pr[X = x, Y= y]\nThe conditional entropy of X wrt. Y is\nH(X|Y ) :=\nX\ny\u2208EY\nPr[Y = y] H(X|Y = y) = H(X, Y) \u2212 H(Y )\n! If X, Yare independent: H(X, Y) = H(X) + H(Y ) and H(X|Y ) = H(X).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Entropy of random variables: example 233 (618)\n\u25b7 Consider again the discrete probability space with Pr[w] = 1/4 and\n\u2126 = {THESIS, THERMODYNAMICS, THEOREM, THEORY}\nAs before, let Xi : \u2126 \u2192 {A, . . . ,Z, \u22a5} be the i-th letter (if defined, else \u22a5) and\nL: \u2126 \u2192 N 0 the length. We then have\n\u25b7 H(X1) = H(X2) = H(X3) = 0\n\u25b7 H(X4) = H(X5) = \u22121\n4 log2\n1\n4 \u2212 1\n2 log2\n1\n2 \u2212 1\n4 log2\n1\n4 = 3\n2\n\u25b7 H(X6) = \u22124 \u00b7 1\n4 log2\n1\n4 = 2\n\u25b7 H(X6|X5 = I) = H(X6|X5 = M) = 0, H(X6|X5 = R) = \u22122 \u00b7 1\n2 log2 = 1\nIG(X6, X5 = a) := H(X6) \u2212 H(X6|X5 = a) is the information gained by\nobserving X5 = a. E.g. IG (X6, X5 = I) = 2 as X5 = I already implies X6 = S.\n\u25b7 H(X6|X5) = 1\n4 \u00b7 0 + 1\n4 \u00b7 0 + 1\n2 \u00b7 1 = 1\n2 . (Remaining uncertainty: RE vs RO.)\n\u25b7 H(X5, X6) = H(X6|X5) + H(X5) = 2 (four possible cases again) .\n! H(X5|X6) = H(X5, X6) \u2212H(X6) = 0. (We can infer X5 from X6 with certainty.)\nD IG(X, A) = H(X) \u2212 H(X|A) is the information gained from event A.",
      "\u25b7 H(X6|X5) = 1\n4 \u00b7 0 + 1\n4 \u00b7 0 + 1\n2 \u00b7 1 = 1\n2 . (Remaining uncertainty: RE vs RO.)\n\u25b7 H(X5, X6) = H(X6|X5) + H(X5) = 2 (four possible cases again) .\n! H(X5|X6) = H(X5, X6) \u2212H(X6) = 0. (We can infer X5 from X6 with certainty.)\nD IG(X, A) = H(X) \u2212 H(X|A) is the information gained from event A.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Huffman codes and decision trees 234 (619)\n\u25b7 Let X : \u2126 \u2192 [k] be some random number in [k].\n\u25b7 Assume we need to guess X and are allowed to ask question of the form\nX\n?\n\u2208 S where S \u2286 [k].\n\u25b7 Each question leads to a dichotomy (X \u2208 S vs. X \u2208 [k] \\ S) of the remaining\npossibilities, i.e. we build implicitly a binary decision tree (=prefix code) whose\nleaves are labeled by the correct answer.\n\u25b7 The expected number of questions we are going to ask is thus the expected\nheight of the decision tree.\n\u25b7 Hence, the expected height of such a decision tree is lower bounded by H(X).\n\u25b7 A Huffman tree will thus minimize the expected height.\n\u25b7 In concrete applications, the kind of questions (=predicates) that we are\nallowed to ask might be restricted:\n\u25b7 E.g. in case of multidimensional data, i.e. X = (X1, . . . , Xn) \u2208 R n, we may\nonly be able to ask whether the i-th component of X has a certain value.\nHere, the C4.5 algorithm splits the data by maximizing the average",
      "allowed to ask might be restricted:\n\u25b7 E.g. in case of multidimensional data, i.e. X = (X1, . . . , Xn) \u2208 R n, we may\nonly be able to ask whether the i-th component of X has a certain value.\nHere, the C4.5 algorithm splits the data by maximizing the average\ninformation gain H(X) \u2212 H(X|Xi) (in general, the tree will not be binary) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: decision trees via minimizing average entropy 235 (620)\n\u25b7 Example: Assume X1, X2, X3 are directly observable (for simplicity binary)\nproperties based on which we want to decide if some dependent property\nY = f(X1, X2, X3) is true.\n\u25b7 One simple heuristic to obtain a decision/classification procedure (to \u201clearn\u201d\nthe function/predicate Y from the data) is to recursively split the data wrt. that\nproperty Xi that minimizes the conditional entropy\nH(Y |Xi) =\nX\na\u2208EXi\nPr[Xi = a] H(Y |Xi = a)\nwhere relative frequencies are used to estimate the required probabilities.\nThe recursions stops e.g. if the entropy in the remaining data is sufficiently\nclose to 0.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: decision trees via minimizing average entropy 235 (621)\n\u25b7 Example: Assume X1, X2, X3 are directly observable (for simplicity binary)\nproperties based on which we want to decide if some dependent property\nY = f(X1, X2, X3) is true.\n\u25b7 Assume experimentally we have obtained the data:\nx1 x2 x3 y\n0 1 1 0\n1 0 1 1\n1 1 1 1\n0 0 1 1\n1 0 0 1\nWith \u03b7(p) = \u2212p log2 p \u2212 (1 \u2212 p) log2(1 \u2212 p) the estimated total entropy of Y\nis \u03b7(4/5) = 0.7219 . . .while the estimates for H(Y |Xi) are\ni estimated conditional entropy\n1 3/5 \u00b7 \u03b7(3/3) + 2/5 \u00b7 \u03b7(1/2) = 0.4\n2 2/5 \u00b7 \u03b7(1/2) + 3/5 \u00b7 \u03b7(3/3) = 0.4\n3 4/5 \u00b7 \u03b7(3/4) + 1/5 \u00b7 \u03b7(1/1) = 0.649 . . .\nSo splitting wrt. X1 or X2 reduces the estimated conditional entropy the\nmost.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: decision trees via minimizing average entropy 235 (622)\n\u25b7 Example: Assume X1, X2, X3 are directly observable (for simplicity binary)\nproperties based on which we want to decide if some dependent property\nY = f(X1, X2, X3) is true.\n\u25b7 Assume we split wrt. X2, then the (conditional) data sets are:\nx1 x2 x3 y\n0 1 1 0\n1 1 1 1\nx1 x2 x3 y\n1 0 1 1\n0 0 1 1\n1 0 0 1\nIn this example, the measured data suggests that Y = 1 in the event of\nX2 = 0, as the estimated value of H(Y |X2 = 0) is 0.\nWe thus proceed with H(Y |X2 = 1, Xi):\ni estimated conditional entropy\n1 1/2 \u00b7 \u03b7(1/1) + 1/2 \u00b7 \u03b7(1/1) = 0\n3 2/2 \u00b7 \u03b7(1/2) + 0/2 \u00b7 \u03b7(0/0) = 1\ni.e. splitting further wrt. X1 reduces the entropy to 0 as now y = x1 for the\nremaining data. So the decision tree/procedure becomes:\n\u201cIf x2 = 0, assume (predict) y = 1, else assume y = x1.\u201d\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: decision trees via minimizing average entropy 235 (623)\n\u25b7 Example: Assume X1, X2, X3 are directly observable (for simplicity binary)\nproperties based on which we want to decide if some dependent property\nY = f(X1, X2, X3) is true.\n! If the experimentally obtained data is biased (does not reflect the real world) , the\nobtained decision procedure will be biased, too, obviously.\n\u25b7 Note that the obtained tree/procedure depends on the order in which we split\nthe data recursively. (For comparison, check what happens if we split wrt. X1 first.)\n\u25b7 Minimizing the average entropy is just one approach to \u201clearn\u201d a\ndecision/classification procedure from a given set of observations.\nE.g. also splitting/clustering based on Bayes\u2019 theorem or (assuming a notion of\ndistance or similarity) principal axis or nearest neighbor is used (or simply DNF) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Sorting and entropy 236 (624)\n\u25b7 Consider a1 < a2 < . . . < an distinct numbers.\n\u25b7 Assume we are given the n numbers in some arbitrary order, i.e.\na\u03c0(1), . . . , a\u03c0(n)\nwhere \u03c0 : [n]\nbij\n\u2212 \u2192[n] is an a-priori unknown permutation.\n\u25b7 Sorting amounts to determining \u03c0 resp. \u03c0\u22121.\n\u25b7 Entropy/uncertainty is maximized under the assumption that every\npermutation is equiprobable:\nI.e. the permutation is a random variable X that is uniformly distributed over\nthe set Sn of all permutations of [n].\n\u25b7 A standard sorting algorithm uses binary comparisons of the form ai\n?\n< aj\nand thus generates implicitly a decision tree on Sn.\n\u25b7 The expected number of comparisons is thus lower bounded by\nH(X) = log2 n! \u2248 n log2 n\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Remark: Entropy in physics (*) 237 (625)\n\u25b7 Entropy is denoted by the letter H (cf. Heta) in reference to Boltzmann\u2019s H\ntheorem which is considered central for the development of statistical\nmechanics, in particular thermodynamics.\n\u25b7 Entropy is used in physics e.g. to measure the \u201cdisorder\u201d resp. \u201cuncertainty\u201d\nregarding the internal/microscopic state of a physical system that can only be\nobserved from a macroscopic point of view (like a gas):\n\u25b7 A \u201cdiscretized gas\u201d is a discrete probability space (\u2126, Pr[]) where an\nelementary event \u03c9 \u2208 \u2126 is called a microstate.\n\u25b7 The Gibbs entropy of the gas is S := \u2212kB\nP\n\u03c9\u2208\u2126 Pr[\u03c9] log Pr[\u03c9] where kB is\nthe Boltzmann constant. (i.e. up to kB\nlog 2 Shannon\u2019s entropy.)\n\u25b7 The second law of thermodynamics (\u201claw\u201d=axiom) postulates that in the\nequilibrium the distribution Pr[] maximizes entropy, i.e. Pr[\u03c9] = 1/|\u2126| so that\nS = kB log |\u2126| (called Boltzmann\u2019s entropy formula).\n\u25b7 So Gibbs formula relates Boltzmann\u2019s \u201cmacroscopic\u201d formula to the",
      "log 2 Shannon\u2019s entropy.)\n\u25b7 The second law of thermodynamics (\u201claw\u201d=axiom) postulates that in the\nequilibrium the distribution Pr[] maximizes entropy, i.e. Pr[\u03c9] = 1/|\u2126| so that\nS = kB log |\u2126| (called Boltzmann\u2019s entropy formula).\n\u25b7 So Gibbs formula relates Boltzmann\u2019s \u201cmacroscopic\u201d formula to the\ndistribution of the microstates so that uncertainty is maximized.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Remark: Entropy in physics (*) 237 (626)\n\u25b7 Entropy is denoted by the letter H (cf. Heta) in reference to Boltzmann\u2019s H\ntheorem which is considered central for the development of statistical\nmechanics, in particular thermodynamics.\n\u25b7 Entropy is used in physics e.g. to measure the \u201cdisorder\u201d resp. \u201cuncertainty\u201d\nregarding the internal/microscopic state of a physical system that can only be\nobserved from a macroscopic point of view (like a gas):\n\u25b7 As a consequence of the second law of thermodynamics any irreversible\ncomputation requires (additional) external energy:\nEvery non-injective map f : A \u2192 B reduces entropy as it maps at least two\nstates a, a\u2032 onto a single state f(a) = b = f(a\u2032) thereby reducing disorder (it\nprojects A onto A/ \u2261f ). In particular logic gates for disjunction and\nconjunction are themselves irreversible.\n\u25b7 It can be shown that every TM can be transformed into a reversible TM (cf.\nreversible computing).",
      "states a, a\u2032 onto a single state f(a) = b = f(a\u2032) thereby reducing disorder (it\nprojects A onto A/ \u2261f ). In particular logic gates for disjunction and\nconjunction are themselves irreversible.\n\u25b7 It can be shown that every TM can be transformed into a reversible TM (cf.\nreversible computing).\n\u25b7 In quantum computing, irreversible computation results from measuring\n(=projection) quantum states, whereas quantum logic gates are themselves\nreversible and modeled by unitary operators (cf. probabilistic FAs).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "8 Basics of information theory: Shannon entropy, Huffman codes, compression\nPrefix codes and Huffman\u2019s algorithm\nOptimal codes and Shannon\u2019s entropy\nEntropy and information\nArithmetic codes\nData compression\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 239 (628)\n\u25b7 Reminder: Given a distribution (pa) a Huffman code achieves an expected\ncode word length of\nH(p) \u2264\nX\na\npa|c(a)| \u2264H(p) + 1\ni.e. we might \u201cwaste\u201d one bit on average.\nBuilding a Huffman code for n-grams \u03a3n, i.e. for the n-fold distribution p\u2297n\n(i.e. assuming stochastic idependence) reduces the overhead averaged over \u03a3 to\n1/n \u2013 but this requires to fix n.\n\u25b7 Codes based on homomorphisms (e.g. block and prefix codes) are sometimes also\ncalled symbol codes (see e.g. MacKay) .\n\u25b7 In contrast to block and prefix codes, an arithmetic code 1 associates with\nan input word w an interval [\u03b1w, \u03b2w) (with 0 \u2264 \u03b1w < \u03b2w \u2264 1) and 2 then\nmaps w to a shortest binary word u = u1 . . . ul \u2208 {0, 1}\u2217 that represents a\nnumber Pl\ni=1 2\u2212iui \u2208 [\u03b1w, \u03b2w).\nArithmetic codes asymptotically also achieve an expected code word length\nof H(p) but without the need to fix n.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (629)\n\u03b5\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\n\u25b7 Assume an ordered alphabet a1, . . . , ak and a distribution p \u2208 D([k]).\n(wlog pi > 0 for all i \u2208 [k].)\n\u25b7 E.g. A, B, C, D, E and p = (1/8, 3/8, 1/4, 1/8, 1/8)\nLet Pj := Pj\ni=1 pi the accumulated probabilities.\nWe then can identify the i-th symbol ai with the interval [Pi\u22121, Pi).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (630)\n\u03b5\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\n\u25b7 Using the binary representation of [0, 1] every interval [\u03b1, \u03b2) \u2286 [0, 1]\ncorresponds to a subset of (countably) infinite binary sequences/words:\nx = (xi)i\u2208N \u2208 {0, 1}\u03c9 represents the number JxK2 := P\u221e\ni=1 xi2\u2212i.\n\u25b7 Typically, the set of (countably) infinite words is denoted by \u03a3\u03c9 and\na\u03c9 = aaa . . .denotes the (countably) infinite repetition of the symbol a.\n\u25b7 x = (xi) \u2208 {0, 1}\u03c9 is a path through the infinite binary tree {0, 1}\u2217.\n\u25b7 The paths u01\u03c9 and u10\u03c9 meet at infinity at the same real number again.\nE.g. J010\u03c9K2 = 1/4 = J001\u03c9K2.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (631)\n\u03b5\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\n\u25b7 Using the binary representation of [0, 1] every interval [\u03b1, \u03b2) \u2286 [0, 1]\ncorresponds to a subset of (countably) infinite binary sequences/words:\nx = (xi)i\u2208N \u2208 {0, 1}\u03c9 represents the number JxK2 := P\u221e\ni=1 xi2\u2212i.\n\u25b7 A Huffman code c prunes the infinite tree and assigns each symbol a implicitly\nthe interval [Jc(a)0\u03c9K2, Jc(a)1\u03c9K).\n! An arithmetic code instead encodes a word as an infinite path (\u201cbit stream\u201d)\nthrough the tree: it is thus in general not an homomorphism.\n(Sometimes \u201csymbol code\u201d (homomorphism, Huffman) and \u201cstream code\u201d\n(arithmetic code) are used.)No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (632)\n\u03b5\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\nD Arithmetic code for an order alphabet a1, . . . , ak and p \u2208 D([k]):\n\u25b7 For input w = ai1 . . . ail, initially set \u03b10 := 0 and \u03b20 := 1.\n\u25b7 For m from 0 to l \u2212 1 set\n\u03b1m+1 := \u03b1m +(\u03b2m \u2212\u03b1m)\u00b7P(im+1 \u22121) \u03b2m+1 := \u03b1m +(\u03b2m \u2212\u03b1m)\u00b7P(im+1)\ni.e. rescale [\u03b1m, \u03b2m) to [0, 1) and descend into the interval assigned to aim+1 .\n(By construction, |\u03b2m \u2212 \u03b1m| = 1 \u00b7 pi1 \u00b7\u00b7\u00b7 pim.)\n\u25b7 Finally output a shortest word u with Ju0\u03c9K2 \u2208 [\u03b1l, \u03b2l)\ne.g.: [0, 1)\nB\n\u2212 \u2192[1/8, 1/2)\nA\n\u2212 \u2192[5/16, 13/32)\nD\n\u2212 \u2192[83/256, 23/64) \u220b J010110\u03c9K2\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (633)\n\u03b5\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\n\u25b7 In order to decode the original word from w its binary representation u\nsimplify recursively split [0, 1) wrt. the partition defined by the distribution p\nand descend into the interval that contains Ju0\u03c9K2.\n! The only problem is when to stop: Either introduce a termination symbol $\nand encode w$ or remember |w|.\n\u25b7 If we know u and |w|, we simply stop after |w| recursive calls.\n\u25b7 If we use e.g. $ as \u201cterminator\u201d, we simply stop as soon as we would descend\ninto the interval assigned to $.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (634)\n\u03b5\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\n\u25b7 For instance, w = AAA would lead to the interval [0, 1/512) in the example;\nso we could use u = \u03b5 as its encoding together with |w| = 3.\n\u25b7 If we use E as \u201cterminator\u201d, we would instead encode wE = AAAE which\nwould result in the interval [7/4096, 1/512) instead with u = 09111.\n! In order to send u = 0913 we actually need to transmit 12 bits except we use\nrun-length encoding (RLE).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (635)\n\u03b5\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\n\u25b7 For comparison, a Huffman code in our example would be\nc(B) = 00 c(A) = 010 c(D) = 011 c(C) = 10 c(E) = 11\nso that e.g.\nc(AAA) = 010 010 010 c(AAAE) = 010 010 010 11\n! If we fix the input length n, we can still use an arithmetic code\n\u03b1: \u03a3\u2217 \u2192 {0, 1}\u2217 as \u201csymbol\u201d code for the n-grams \u03a3n:\nThe resulting code still has to transmit at least nH(p) bits on average as it is\na uniquely decodable code.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (636)\n\u03b5\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\n\u25b7 Encoding a word w = w1 . . . wn yields an interval [\u03b1w, \u03b1w + \u03b4w) with\n\u03b4w := p\u2297n\nw = pw1 pw2 \u00b7\u00b7\u00b7 pwn\nso that\n\u2212log2 \u03b4w = \u2212\nnX\ni=1\nlog2 pwi\nAveraging over all words of length n we obtain as expected value\n\u2212\nX\nw\u2208\u03a3n\np\u2297n\nw log2 \u03b4w = nH(p)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (637)\n\u03b5\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\n\u25b7 A shortest word u \u2208 {0, 1}\u2217 with Ju0\u03c9K2 \u2208 [\u03b1w, \u03b1w + \u03b4w) has\n\u25b7 at least length |u| \u2265 \u2212\u2308log2 \u03b4w\u2309 as\nJ0m\u2212110\u03c9K2 = 2\u2212m \u2264 \u03b4w iff m \u2265 log2 \u03b4\u22121\nw\n\u25b7 at most length |u| \u2264 \u2212\u2308log2 \u03b4w\u2309 + 1 as the strings u \u2208 {0, 1}\u2212\u2308log2 \u03b4w\u2309+1\npartition [0, 1) into intervals of length\n2\u2308log2 \u03b4w\u2309\u22121 \u2264 2\u230alog2 \u03b4w\u230b < \u03b4w\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (638)\n\u03b5\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\n\u25b7 Hence, if we encode w using a shortest word ca(w) := u, then\n\u2212log2 \u03b4w \u2264 \u2212\u2308log2 \u03b4w\u2309 \u2264 |ca(w)| < \u2212\u2308log2 \u03b4w\u2309 + 1 \u2264 \u2212log2 \u03b4w + 2\nSo, averaging again over all w \u2208 \u03a3n obtain as expected code word length\nnH(p) \u2264\nX\nw\u2208\u03a3n\np\u2297n\nw |ca(w)| < nH(p) + 2\ni.e. we have an average overhead of 2/n for fixed input length n.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (639)\n\u03b5\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\nL Every arithmetic code c: \u03a3n \u2192 {0, 1}\u2217 wrt. a given p \u2208 D(\u03a3) and fixed input\nlength n achieves an expected code word length of\nn \u00b7 H(p) \u2264\nX\nw\u2208\u03a3n\np\u2297n\nw |c(w)| < n\u00b7 H(p) + 2\nand is thus asymptotically optimal wrt. the average number of bits\ntransmitted per original symbol.\n\u25b7 A Huffman code for \u03a3n will reduce the expected overhead to 1/n, but an\narithmetic code does not require to fix n.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "8 Basics of information theory: Shannon entropy, Huffman codes, compression\nPrefix codes and Huffman\u2019s algorithm\nOptimal codes and Shannon\u2019s entropy\nEntropy and information\nArithmetic codes\nData compression\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Data compression 242 (641)\n\u25b7 For an a priori unknown word w \u2208 \u03a3, relative frequencies and a\ncorresponding Huffman or arithmetic code can be re-computed (in principle)\nfrom time to time: as long as transmission errors can be ruled out, the\nreceiver can simply re-compute the frequencies itself.\n\u25b7 In contrast, in case of a fixed input w \u2208 \u03a3\u2217 (text, image, music) the relative\nfrequencies can be directly computed in order to \u201ccompress\u201d the \u201cdatum\u201d\n(=the given) w.\nMost compression algorithms combine several different approaches for\nreducing redundancy, e.g.:\n\u25b7 gzip resp. the underlying DEFLATE first apply LZ77 and then apply a\nHuffman code.\n\u25b7 bzip2 uses a sequence of multiple transformations/encodings:\nfirst, a run-length encoding (e.g. aaaaabbbbccc = a5b4c3 is sent as a5b4c3),\nthen Burrows-Wheeler transform, followed by a Move-to-front transform, and\nfinally a Huffman code (and some further transformations, see the references).",
      "\u25b7 bzip2 uses a sequence of multiple transformations/encodings:\nfirst, a run-length encoding (e.g. aaaaabbbbccc = a5b4c3 is sent as a5b4c3),\nthen Burrows-Wheeler transform, followed by a Move-to-front transform, and\nfinally a Huffman code (and some further transformations, see the references).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Subword sharing, SLPs, smallest grammar problem 243 (642)\n\u25b7 As discussed, we either use an arithmetic code or an Huffman code for \u03a3n to\nobtain an asymptotically optimal code (for n \u2192 \u221e).\n\u25b7 In case of an Huffman code, we need to use a fixed n; another approach is,\nto introduce auxiliary symbols for subwords that occur very frequently, a\nsimple example is\n\u25b7 Run-length encoding (RLE): instead of transmitting a2n\nexplicitly using 2n\nbits, extend the decoder so that it can handle the implicit representation, e.g.\nlet [a, 2n] stand for a2n\n. Note that we only need n = log2 2n bits for\ntransmitting 2n this way.\n\u25b7 RLE can be understood as encoding the word a2n\nusing the regular grammar\nS \u2212 \u2192G [a, k], [a, k] \u2212 \u2192G [a, k\u2212 1], . . ., [a, 1] \u2212 \u2192G a.\n\u25b7 Using a context-free grammar, we can use also repeated squaring S \u2212 \u2192G Xk,\nXk \u2212 \u2192G Xk\u22121Xk\u22121, . . ., X0 \u2212 \u2192G a. Now, we only need to send Xn which\nrequires only log2 n = log2 log2 2n many bits.",
      "using the regular grammar\nS \u2212 \u2192G [a, k], [a, k] \u2212 \u2192G [a, k\u2212 1], . . ., [a, 1] \u2212 \u2192G a.\n\u25b7 Using a context-free grammar, we can use also repeated squaring S \u2212 \u2192G Xk,\nXk \u2212 \u2192G Xk\u22121Xk\u22121, . . ., X0 \u2212 \u2192G a. Now, we only need to send Xn which\nrequires only log2 n = log2 log2 2n many bits.\n\u25b7 In general, we can ask what is the least/smallest context-free grammar (SLP)\nG with L(G) = {w} (where least is measured wrt. the number non-terminals).\nIt can be shown that this \u201csmallest grammar problem\u201d is NP-complete (cf. eg.\nhere).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "SLPs for compression 244 (643)\nD For w = w1 . . . wn \u2208 \u03a3n and indices 1 \u2264 k \u2264 l \u2264 n set w[k:l] = wk . . . wl.\nD A Cut-SLP (V, \u03a3, P, Xn) is an SLP where we allow in also rules of the form\nXi \u2212 \u2192G Xj[k:l] for i > jwhere Xj[k:l] generates w[k:l] if Xj generates w.\nD The (simple) LZ77 factorization of a word w \u2208 \u03a3\u2217 is the unique factorization\nw = w(1) . . . w(l) satisfying\n1 w(j) = a \u2208 \u03a3 and a does not occur in w(1) . . . w(j\u22121), or\n2 w(j) = w(1) . . . w(j\u22121)[k:l] \u2208 \u03a3+ with l \u2212 k maximal.\n\u25b7 E.g. for w = a b a aba baaba abwe obtain the grammar\nX1 \u2212 \u2192G a X 2 \u2212 \u2192G X1b X 3 \u2212 \u2192G X2a X 4 \u2212 \u2192G X3X3\nX5 \u2212 \u2192G X4Y4 Y4 \u2212 \u2192G X4[2:6] X6 \u2212 \u2192G X5Y5 Y5 \u2212 \u2192G X5[1:2]\nThe actual compression results from the encoding of the rules:\nab(1, 1)(1, 3)(2, 5)(1, 2)\nMore efficient variants are used in practice, e.g. we can also allow to repeat\nsubwords that are longer than the so-far reconstructed prefix by means of\nrecursive expansion: ab(1, 3) = aba(2, 2) = abab(3, 1) = ababa",
      "ab(1, 1)(1, 3)(2, 5)(1, 2)\nMore efficient variants are used in practice, e.g. we can also allow to repeat\nsubwords that are longer than the so-far reconstructed prefix by means of\nrecursive expansion: ab(1, 3) = aba(2, 2) = abab(3, 1) = ababa\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "SLPs for compression 244 (644)\nD For w = w1 . . . wn \u2208 \u03a3n and indices 1 \u2264 k \u2264 l \u2264 n set w[k:l] = wk . . . wl.\nD A Cut-SLP (V, \u03a3, P, Xn) is an SLP where we allow in also rules of the form\nXi \u2212 \u2192G Xj[k:l] for i > jwhere Xj[k:l] generates w[k:l] if Xj generates w.\nD The LZ78 factorization of a word w \u2208 \u03a3\u2217 is the unique factorization\nw = w(1) . . . w(l) satisfying (with w(0) := \u03b5):\n1 w(l) = w(i) for some 0 \u2264 i < land for all 0 \u2264 i < j < l: w(i) \u0338= w(j)\n2 for all 1 \u2264 i < lthere exists a 0 \u2264 j < is.t. w(i) = w(j)a for some a \u2208 \u03a3.\n\u25b7 E.g. for w = a b aa ba baa baabwe obtain the grammar:\nX1 \u2212 \u2192G a X 2 \u2212 \u2192G b\nX3 \u2212 \u2192G X1a X 4 \u2212 \u2192G X2a X 5 \u2212 \u2192G X4a X 6 \u2212 \u2192G X5b\nX9 \u2212 \u2192G X1X2X3X4X5X6\nwhich can be encoded e.g. as\n(0, a)(0, b)(1, a)(2, a)(4, a)(5, b)\n\u25b7 If w is generated by a Markov model, LZ78 can be shown to be\nasymptotically optimal wrt. the stationary distribution/Cesaro limit.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "SLPs for compression 244 (645)\nD For w = w1 . . . wn \u2208 \u03a3n and indices 1 \u2264 k \u2264 l \u2264 n set w[k:l] = wk . . . wl.\nD A Cut-SLP (V, \u03a3, P, Xn) is an SLP where we allow in also rules of the form\nXi \u2212 \u2192G Xj[k:l] for i > jwhere Xj[k:l] generates w[k:l] if Xj generates w.\n\u25b7 For a comparision, consider w = a26\u22121.\nLZ77 factorization:\na a a2 a4 a8 a16 a31 = a(1, 1)(1, 2)(1, 4)(1, 8)(1, 16)(1, 31) = a(1, 63)\nLZ78 factorization:\na a2 a3 a4 a5 a6 a7 a8 a9 a10 a8 = (0, a)(1, a)(2, a)(3, a) . . .(9, a)(7, a)\n\u25b7 \u201cLZ\u201d stands for Abraham Lempel and Jacob Ziv who proposed both\nalgorithms in 1977 and 1978, respectively.\n\u25b7 For more examples, see e.g. the articles by Charikar et al. and Lohrey et al.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Remark: Kolmogorov complexity (*) 245 (646)\nD Fix any Turing complete programming language L.\n\u25b7 E.g. the set of all D1TMs with \u0393 = {0, 1, \u25a1} and \u03a3 = {0, 1}.\nGiven a ( wlog binary) string w \u2208 {0, 1}\u2217 its Kolmogorov complexity KL(w)\nis the length of a shortest program wrt. L that terminates and outputs w.\n\u25b7 Some results:\n\u25b7 KL is uncomputable, KL(w) = n is undecidable (see busy beavers).\n\u25b7 Any Turing complete language can expand LZ77 and LZ78 compressed strings\nor decode a Huffman code yielding another upper bound for KL(w).\nSo KL(w) is upper bounded by the length of a compression algorithm and the\ncompressed representation of w.\n\u25b7 For every L there exists a constant cL s.t. KL(w) \u2264 |w| + cL. (\u201creturn w\u201d)\n\u25b7 For every n \u2208 N there exists a string wn s.t. KL(wn) \u2265 n.\n(There are only finitely many programs of length n.)\n\u25b7 For every two L, L\u2032 there exists a constant s.t. KL(w) \u2264 KL\u2032(w) + cL,L\u2032:\n(Simply include a \u201ccompiler/interpreter\u201d.)",
      "\u25b7 For every n \u2208 N there exists a string wn s.t. KL(wn) \u2265 n.\n(There are only finitely many programs of length n.)\n\u25b7 For every two L, L\u2032 there exists a constant s.t. KL(w) \u2264 KL\u2032(w) + cL,L\u2032:\n(Simply include a \u201ccompiler/interpreter\u201d.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "9 Appendix: (under construction)\nTuring Machines in Python\nHalting probem in pseudo-python\nString rewriting systems and formal grammars in Python\nFrom 1TM to grammar and PCP: example\nFinite Automata in Python\nPushdown Automata in Python (accept on final state)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM in Python 248 (649)\nimport networkx as nx\nfrom c o l l e c t i o n s import deque # Python \u2019 s double = ended queue\nfrom t y p i n g import *\nS t a t e = Any # Q\nSymbol = Any # e l e m e n t s o f\u03a3 and \u0393\new = ( ) # \u03b5 = ()\nWord = Tuple [ Symbol ] # \u03a3\u2217 , \u0393\u2217 ; r e c a l l words a r e a c t u a l l y t u p l e s\nRule = Tuple [ Tuple [ State , Symbol ] , Tuple [ State , Symbol , i n t] ] # (Q \u00d7 \u0393) \u00d7 (Q \u00d7 \u0393 \u00d7 Z )\nC o n f i g = Tuple [ Word , State , Word ] # \u0393\u2217 \u00d7 Q \u00d7 \u0393\u2217\nc f g 2 s t r = lambda c : \u2019 \u2019 . j o i n ( s t r( x ) f o rx i n c [ 0 ] + ( c [ 1 ] , ) + c [ 2 ] )",
      "# Draws the \u2019 \u2019 c a l l graph \u2019 \u2019 o f the t r a n s i t i o n r u l e s o f a 1TM\ndef callgraphTM ( r u l e s : Set [ Rule ] ) :\nG = nx . DiGraph ( )\ne d g e l a b e l s = d i c t( )\nf o rt i n r u l e s :\nqa , rbd = t\nq , a = qa\nr , b , d = rbd\nG . add edge ( s t r( q ) , s t r( r ) )\ne d g e l a b e l s [ (s t r( q ) , s t r( r ) ) ] = f \u201d {a}:{b}/{d}\u201d\npos = nx . s p r i n g l a y o u t (G)\nnx . draw (G, pos=pos , w i t h l a b e l s=True )\nnx . d r a w n e t w o r k xe d g e l a b e l s (G, pos=pos , e d g e l a b e l s=e d g e l a b e l s )\nr e t u r n\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM in Python: single run 249 (650)\ndef applyTMrule ( a l p h a : Word , beta : Word , r : State , b : Symbol , d : i n t, b la n k : Symbol ) = > C o n f i g :\na s s e r t = 1 <= d <= 1\ni f d == 0 :\nr e t u r n( alpha , r , ( b , ) + beta [ 1 : ] ) # \u03b1qa\u03b2\u2032 \u2212 \u2192M \u03b1rb\u03b2\u2032\ni f d == 1 :\nr e t u r n( a l p h a + ( b , ) , r , beta [ 1 : ] ) # \u03b1qa\u03b2\u2032 \u2212 \u2192M \u03b1br\u03b2\u2032\nz = a l p h a [ = 1] i f l e n( a l p h a ) e l s eb la nk\nr e t u r n( a l p h a [:= 1] , r , ( z , b ) + beta [ 1 : ] ) # \u03b1\u2032zqa\u03b2\u2032 \u2212 \u2192M \u03b1\u2032rzb\u03b2\u2032",
      "# I f the t r a n s i t i o n r u l e s a r e n o n d e t e r m i n i s t i c , ask the u s e r which r u l e to a p p l y\ndef runTM(w: Word , r u l e s : Set [ Rule ] , q0 : State , f i n a l : Set [ S t a t e ] , b la nk : s t r, max steps : i n t=10)= >bool :\nc u r c o n f i g = (ew , q0 , w)\nw h i l emax steps :\np r i n t( c f g 2 s t r ( c u r c o n f i g ) , c u r c o n f i g )\nmax steps = = 1\nalpha , q , beta = c u r c o n f i g\na = beta [ 0 ] i f l e n( beta ) e l s ebl a nk\na p p l i c a b l e r u l e s = [ t [ 1 ] f o rt i n r u l e s i f t [ 0 ] == ( q , a ) ] # {(r, b, d) | ((q, a), (r, b, d)) \u2208 \u03b4}\ni f l e n( a p p l i c a b l e r u l e s ) == 0 :\nr e t u r nq i n f i n a l\ni d x = 0 i f l e n( a p p l i c a b l e r u l e s ) == 1 e l s e i n t( i n p u t( \u2019 choose one : \u2019 + r e p r( l i s t( enumerate (\na p p l i c a b l e r u l e s ) ) ) ) )\nr , b , d = a p p l i c a b l e r u l e s [ i d x ]\nc u r c o n f i g = applyTMrule ( alpha , beta , r , b , d , b l an k )",
      "i d x = 0 i f l e n( a p p l i c a b l e r u l e s ) == 1 e l s e i n t( i n p u t( \u2019 choose one : \u2019 + r e p r( l i s t( enumerate (\na p p l i c a b l e r u l e s ) ) ) ) )\nr , b , d = a p p l i c a b l e r u l e s [ i d x ]\nc u r c o n f i g = applyTMrule ( alpha , beta , r , b , d , b l an k )\nr e t u r nF a l s e\nr u l e s = {\n( ( \u2019A \u2019 , \u2019 \u25a1\u2019 ) , ( \u2019A \u2019 , \u2019 \u25a1\u2019 , 0 ) ) , ( ( \u2019A \u2019 , \u2019 a \u2019 ) , ( \u2019A \u2019 , \u2019 a \u2019 ,1 ) ) , ( ( \u2019A \u2019 , \u2019 a \u2019 ) , ( \u2019B \u2019 , \u2019 a \u2019 ,1 ) ) ,\n( ( \u2019B \u2019 , \u2019 \u25a1) , ( \u2019B \u2019 , \u2019 \u25a1\u2019 , 0) ) , ( ( \u2019B \u2019 , \u2019 a \u2019 ) , ( \u2019B \u2019 , \u2019 a \u2019 , 0 ) ) , ( ( \u2019B \u2019 , \u2019 b \u2019 ) , ( \u2019H \u2019 , \u2019 b \u2019 ,0 ) ) ,\n}\nrunTM( t u p l e ( \u2019 aab \u2019 ) , r u l e s , \u2019A \u2019 , {\u2019H \u2019 }, \u2019 \u25a1\u2019 )\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM in Python: reachable configurations (BFS) 250 (651)",
      "# DFS can get trapped in a nonterminating run even if there is an accepting run; BFS avoids this\ndef bfsTM (w: Word , r u l e s : Set [ Rule ] , q0 : State , f i n a l : Set [ S t a t e ] , bl an k : Symbol , m a x s i z e : i n t=20)= >bool :\nc u r c o n f i g = (ew , q0 , w)\ntodo = deque ( [ c u r c o n f i g ] ) #\na c c e p t = F a l s e\nG = nx . DiGraph ( )\nG . add node ( c f g 2 s t r ( c u r c o n f i g ) )\nw h i l eG . number of nodes ( ) < m a x s i z e and l e n( todo ) :\nc u r c o n f i g = todo . p o p l e f t ( )\nalpha , q , beta = c u r c o n f i g\na = beta [ 0 ] i f l e n( beta ) e l s ebl a nk\na p p l i c a b l e r u l e s = [ t [ 1 ] f o rt i n r u l e s i f t [ 0 ] == ( q , a ) ]\na c c e p t = a c c e p t or ( l e n( a p p l i c a b l e r u l e s ) == 0 and q i n f i n a l )\nf o rr , b , d i n a p p l i c a b l e r u l e s :\nn x t c o n f i g = applyTMrule ( alpha , beta , r , b , d , bl an k )\ni f c f g 2 s t r ( n x t c o n f i g ) not i n G :",
      "a c c e p t = a c c e p t or ( l e n( a p p l i c a b l e r u l e s ) == 0 and q i n f i n a l )\nf o rr , b , d i n a p p l i c a b l e r u l e s :\nn x t c o n f i g = applyTMrule ( alpha , beta , r , b , d , bl an k )\ni f c f g 2 s t r ( n x t c o n f i g ) not i n G :\ntodo . append ( n x t c o n f i g )\nG . add edge ( c f g 2 s t r ( c u r c o n f i g ) , c f g 2 s t r ( n x t c o n f i g ) ) # TODO: s t r i p l e a d i n g& t r a i l i n g b l a n k s\nnx . draw (G, w i t h l a b e l s=True )\nr e t u r na c c e p t\nr u l e s = {\n( ( \u2019A \u2019 , \u2019 \u25a1\u2019 ) , ( \u2019A \u2019 , \u2019 \u25a1\u2019 , 0 ) ) , ( ( \u2019A \u2019 , \u2019 a \u2019 ) , ( \u2019A \u2019 , \u2019 a \u2019 ,1 ) ) , ( ( \u2019A \u2019 , \u2019 a \u2019 ) , ( \u2019B \u2019 , \u2019 a \u2019 ,1 ) ) ,\n( ( \u2019B \u2019 , \u2019 \u25a1\u2019 ) , ( \u2019B \u2019 , \u2019 \u25a1\u2019 , 0 ) ) , ( ( \u2019B \u2019 , \u2019 a \u2019 ) , ( \u2019B \u2019 , \u2019 a \u2019 ,0 ) ) , ( ( \u2019B \u2019 , \u2019 b \u2019 ) , ( \u2019H \u2019 , \u2019 b \u2019 ,0 ) ) ,\n}\nbfsTM ( t u p l e( \u2019 aab \u2019 ) , r u l e s , \u2019A \u2019 , {\u2019H \u2019 }, \u2019 \u25a1\u2019 )\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "9 Appendix: (under construction)\nTuring Machines in Python\nHalting probem in pseudo-python\nString rewriting systems and formal grammars in Python\nFrom 1TM to grammar and PCP: example\nFinite Automata in Python\nPushdown Automata in Python (accept on final state)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Special halting problem in pseudo-python 252 (653)\n# g mod . py :\ndef h ( p y s r c : b y t e s ) = > bool :\n# unknown i m p l e m e n t a t i o n o fh: \u03a3\u2217 \u2192 {0, 1} with \u201d h(w) = 1 IF Mw h a l t s onw ELSE h(w) = 0\u201d\n# E s s e n t i a l l y : \u201dw\u201d i s the s o u r c e code as a ( b i n a r y ) s t r i n g , w h i l e \u201dMw\u201d i s the e x e c u t a b l e\n# F o r m a l l yw has to d e s c r i b e a d e t e r m i n i s t i c one= tape TM Mw\n# Here , we assume w to be the s o u r c e code o f a python module\ndef g ( p y s r c : b y t e s ) :\ni f h ( p y s r c ) i s True :\nw h i l eTrue :\npass\nr e t u r n",
      "# r u s s e l l sp a r a d o x . py :\nfrom g mod import h , g\ndef c o n t r a d i c t i o n ( ) :\ng s r c = b u f f e r f i l e ( \u201dg mod . py\u201d )\nr e t u r nh ( g s r c ) # h ( gs r c ) r e t u r n s True IFF g ( gs r c ) r e t u r n s IFF h ( gs r c ) r e t u r n s F a l s e\n! In general, for a specific predicate (e.g. h: \u201cprogram halts on a specific input\u201d) there is\nno algorithm (source code, D1TM) that takes the source code w and then decides\nwhether the implemented function Mw satifies the predicate or not.\n(I.e. computing a single bit of information on the implemented function can be impossible.)\n\u25b7 But we can still \u201csemidecide\u201d whether Mw halts on w:\nsimply run Mw on w and return True (only) after Mw has halted.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Recursively enumerable from semidecidable/recognaziable 253 (654)\nfrom t y p i n g import *\nimport m u l t i p r o c e s s i n g\nfrom m u l t i p r o c e s s i n g . s h a r e d c t y p e simport Value\nimport i t e r t o o l s\na l p h a b e t = [ 0 , 1 ] # \u03a3\ndef f ( x : bytes , r e t : Value ) :\n. . . # a c t u a l i m p l e m e n t a t i o n o ff : \u03a3\u2217 ,\u2192 {0, 1}\nr e t . v a l u e = i n t( a c c e p t ) # need to use a s h a r e d v a r i a b l e i n Python\nr e t u r n\ndef r u n f o n x f o r n s e c o n d s ( f : C a l l a b l e [ [ bytes , Value ] , None ] , x : bytes , n : i n t)= >bool :\nr e t = Value ( \u2019 i \u2019 , 0) # s h a r e d \u2018 i n t \u2018 v a r i a b l e f o r r e t u r n v a l u e o f f\nfoo = m u l t i p r o c e s s i n g . P r o c e s s ( t a r g e t=f , name=\u201d foo \u201d , a r g s =(x , r e t ) )\nfoo . s t a r t ( )\nfoo . j o i n ( n ) # run \u2018 f ( x ) \u2018 f o r \u2018 n \u2018 s e c o n d s\ni f foo . i s a l i v e ( ) : # s i m u l a t i o n has not t e r m i n a t e d\nfoo . t e r m i n a t e ( )",
      "foo . s t a r t ( )\nfoo . j o i n ( n ) # run \u2018 f ( x ) \u2018 f o r \u2018 n \u2018 s e c o n d s\ni f foo . i s a l i v e ( ) : # s i m u l a t i o n has not t e r m i n a t e d\nfoo . t e r m i n a t e ( )\nfoo . j o i n ( )\nr e t u r nF a l s e\nr e t u r nr e t . v a l u e == 1",
      "# dove = t a i l i n g : i t e r a t e d BFS on a l l i n p u t s f o r i n c r e a s i n g depth / time= bound\ndef c r e a t e f e n u m e r a t i o n ( f : C a l l a b l e [ [ bytes , Value ] , None ] ) :\nn = 0 # time bound\nw h i l eTrue :\nf o rL i n range ( n ) :\nf o rx i n i t e r t o o l s . product ( alphabet , r e p e a t=L ) :\nx = b y t e s ( \u2019 \u2019 . j o i n ( x ) , \u2019 u t f 8 \u2019 ) # L= t u p l e to word to u t f 8 encoded b i n a r y word\ni f r u n f o n x f o r n s e c o n d s ( f , x , n ) i s True :\ny i e l d x # same x can be output m u l t i p l e times , i n f a c t i n f i n i t e l y o f t e n\nn += 1\n! {w \u2208 \u03a3\u2217 | L(Mw) \u0338= \u2205} is semidecidable: enumerate L(Mw).\n(But L(Mw) = \u2205 is not semidecidable.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem and friends 254 (655)\ndef f ( x : b y t e s ) :\n. . . # i n l i n e s o u r c e code o f a g i v e n f u n c t i o nf : \u03a3\u2217 ,\u2192 \u03a3\u2217 h e r e\n# s o u r c e code now as s t r i n g , t a k e s the p l a c e o fw i n the p r o o f s\ns r c f = rb \u201d\u201d\u201d d e f f ( x : s t r ) :\n. . . # i n l i n e s o u r c e code a g a i n\n\u201d\u201d\u201d\ndef r f ( x : b y t e s ) = > bool :\nf ( s r c f ) # output does not matter ( f o r m a l l y : run Mw on w)\nr e t u r nTrue # a c c e p t i n p u tx IFF f h a l t s on i t s own s o u r c e code\n# r f ( x ) does NOT h a l t / r e t u r n / a c c e p t x ( i n d e p e n d e n t o f i n p u t x )\n# IFF f ( s r c f ) does NOT h a l t / r e t u r n",
      "# r f ( x ) does NOT h a l t / r e t u r n / a c c e p t x ( i n d e p e n d e n t o f i n p u t x )\n# IFF f ( s r c f ) does NOT h a l t / r e t u r n\n# IFF h ( f s r c ) r e t u r n s F a l s e /0\n! Given w (and w\u2032), it is not semidecidable, if\n\u25b7 Mw does not halt on \u03b5 (resp. some given input x) (i.e. \u03b5 \u0338\u2208 L(Mw)?)\n\u25b7 there is exists some x \u2208 \u03a3\u2217 on which Mw does not halt (i.e. L(Mw) \u0338= \u03a3\u2217?)\n\u25b7 for all x \u2208 \u03a3\u2217 we have that Mw does not halt. (i.e. L(Mw) = \u2205?)\nNote:\n\u25b7 LH,\u03b5 = {w \u2208 \u03a3\u2217 | \u03b5 \u2208 L(Mw)} is semidecidable: run Mw on \u03b5.\n\u25b7 LH,? = {w \u2208 \u03a3\u2217 | L(Mw) \u0338= \u2205} is semidecidable: enumerate L(Mw).\n\u25b7 LH,\u2217 = {w \u2208 \u03a3\u2217 | L(Mw) = \u03a3\u2217} is also not semidecidable: next slide.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem and friends (2) 255 (656)\nimport m u l t i p r o c e s s i n g\nimport time\ndef f ( x : b y t e s ) :\n. . . # i n l i n e s o u r c e code o f a g i v e n f u n c t i o nf : \u03a3\u2217 ,\u2192 \u03a3\u2217 h e r e\n# s o u r c e code now as s t r i n g , t a k e s the p l a c e o fw i n the p r o o f s\ns r c f = rb \u201d\u201d\u201d d e f f ( x : b y t e s ) :\n. . . # i n l i n e s o u r c e code a g a i n\n\u201d\u201d\u201d\ndef r 2 f ( x : b y t e s ) = > bool :\nfoo = m u l t i p r o c e s s i n g . P r o c e s s ( t a r g e t=f , name=\u201d foo \u201d , a r g s =( s r c f , ) )\nfoo . s t a r t ( )\nfoo . j o i n ( l e n( x ) ) # run \u2018 f ( s r ff ) \u2018 f o r|x| s e c o n d s\ni f foo . i s a l i v e ( ) : # run has not t e r m i n a t e d\nfoo . t e r m i n a t e ( )\nfoo . j o i n ( )\nr e t u r nTrue\nw h i l eTrue :\npass\n# r 2 f ( x ) does h a l t / r e t u r n / a c c e p t x ( i n d e p e n d e n t o f i n p u t x )\n# IFF f ( s r c f ) does NOT h a l t / r e t u r n",
      "# r 2 f ( x ) does h a l t / r e t u r n / a c c e p t x ( i n d e p e n d e n t o f i n p u t x )\n# IFF f ( s r c f ) does NOT h a l t / r e t u r n\n# IFF h ( f s r c ) r e t u r n s F a l s e /0\n! Given w, it is not semidedicable, if Mw does halt for all x \u2208 \u03a3\u2217 (i.e. L(Mw) = \u03a3\u2217?)\n\u25b7 Both LH,\u2217 = {w \u2208 \u03a3\u2217 | L(Mw) = \u03a3\u2217} and LH,\u2217 are undecidable\n\u25b7 Both L= = {(w, w\u2032) \u2208 \u03a3\u2217 \u00d7 \u03a3\u2217 | L(Mw) = L(Mw\u2032)} and L= are undecidable.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "9 Appendix: (under construction)\nTuring Machines in Python\nHalting probem in pseudo-python\nString rewriting systems and formal grammars in Python\nFrom 1TM to grammar and PCP: example\nFinite Automata in Python\nPushdown Automata in Python (accept on final state)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "SRS and grammars in Python 257 (658)\nimport networkx as nx\nfrom c o l l e c t i o n s import deque # Python \u2019 s double = ended queue\nfrom t y p i n g import *\nSymbol = Any # s t r i n g s w i l l be used as symbols , a l l o w s to use \u2018 [ x ] \u2018 as symbol\new = ( ) # \u03b5 = ()\nWord = Tuple [ Symbol ] # words a r e t u p l e s o v e r s t r i n g s\nRule = Tuple [ Word , Word ]\nw o r d 2 s t r = lambda c : \u2019 \u2019 . j o i n ( s t r( x ) f o rx i n c ) # f l a t t e n t u p l e to s t r i n g\ndef p r e t t y p r i n t g r a m m a r r u l e s ( r u l e s : s e t[ Rule ] ) :\np r i n t( \u2019G : \u2019 )\nf o rl h s , r h s i n s o r t e d( r u l e s ) :\np r i n t( \u2019 \\t %s = > %s \u2019 % ( w o r d 2 s t r ( l h s ) , w o r d 2 s t r ( r h s ) ) )\nr e t u r n\ndef bfsSRS (w: Word , r u l e s : s e t[ Rule ] , m a x s i z e : i n t=10) :\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\nc u r c o n f i g = w\ntodo = deque ( [ c u r c o n f i g ] )\nG = nx . DiGraph ( )",
      "r e t u r n\ndef bfsSRS (w: Word , r u l e s : s e t[ Rule ] , m a x s i z e : i n t=10) :\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\nc u r c o n f i g = w\ntodo = deque ( [ c u r c o n f i g ] )\nG = nx . DiGraph ( )\nG . add node ( w o r d 2 s t r ( c u rc o n f i g ) )\nw h i l eG . number of nodes ( ) < m a x s i z e and l e n( todo ) :\nc u r c o n f i g = todo . p o p l e f t ( )\ni f l e n( c u r c o n f i g ) == 0 :\ncontinue\nf o rl h s , r h s i n r u l e s :\nf o r i i n range ( l e n( c u r c o n f i g )= l e n( l h s ) +1) :\ni f c u r c o n f i g [ i : i+ l e n( l h s ) ] == l h s :\nn x t c o n f i g = c u r c o n f i g [ : i ] + r h s + c u r c o n f i g [ i+l e n( l h s ) : ]\ni f w o r d 2 s t r ( n x tc o n f i g ) not i n G :\ntodo . append ( n x t c o n f i g )\nG . add edge ( w o r d 2 s t r ( c u r c o n f i g ) , w o r d 2 s t r ( n x t c o n f i g ) )\npass\nnx . draw (G, w i t h l a b e l s=True )\nr e t u r n",
      "i f w o r d 2 s t r ( n x tc o n f i g ) not i n G :\ntodo . append ( n x t c o n f i g )\nG . add edge ( w o r d 2 s t r ( c u r c o n f i g ) , w o r d 2 s t r ( n x t c o n f i g ) )\npass\nnx . draw (G, w i t h l a b e l s=True )\nr e t u r n\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kuroda NF in Python (1/3) 258 (659)\ndef kurodaNF ( r u l e s : s e t[ Rule ] ) :\nv a r i a b l e s = s e t( )\nt e r m i n a l s = s e t( )\nf o rl h s , r h s i n r u l e s :\nv a r i a b l e s |= s e t( l h s )\nt e r m i n a l s|= s e t( r h s )\nt e r m i n a l s= = v a r i a b l e s # Assumption : a symbol i s n o n t e r m i n a l i f i t o c c u r s on the LHS\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\n# A u x i l i a r y v a r i a b l e s f o r t e r m i n a l s :[a] \u2212 \u2192G a , [\u03b5]\u2212 > \u03b5\n# Might break , i f e . g . [ a ] i s a l r e a d y used a v a r i a b l e or [ i s used a t e r m i n a l\n# A l t e r n a t i v e : use n e s t e d words / terms\nn e w r u l e s = {(( \u2019 [% s ] \u2019 % x , ) , ( x , ) ) f o rx i n t e r m i n a l s}\ni f any ( l e n( l h s )>l e n( r h s ) f o rl h s , r h s i n r u l e s ) :\nn e w r u l e s . add ( ( ( \u2019 [ ew ] \u2019 , ) , ew ) )\nf o rl h s , r h s i n r u l e s :",
      "# Replace ABC \u2212 \u2192G aB by ABC \u2212 \u2192G [a]B[\u03b5] , [a] \u2212 \u2192G a , [\u03b5] \u2212 \u2192G \u03b5\nr h s = l i s t( r h s )\nf o r i i n range ( l e n( r h s ) ) :\ni f r h s [ i ] i n t e r m i n a l s :\nr h s [ i ] = \u2019 [% s ] \u2019 % r h s [ i ]\nr h s += [ \u2019 [ ew ] \u2019 f o r i n range ( l e n( l h s )= l e n( r h s ) ) ]\nn e w r u l e s . add ( ( l h s ,t u p l e( r h s ) ) )\n. . .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kuroda NF in Python (2/3) 259 (660)\ndef kurodaNF ( r u l e s : s e t[ Rule ] ) :\n. . .\nr u l e s = n e w r u l e s\nn e w r u l e s = s e t( )\nf o rl h s , r h s i n r u l e s :\ni f l e n( r h s ) <= 2 or l e n( l h s ) == l e n( r h s ) :\nn e w r u l e s . add ( ( l h s , r h s ) )\ne l s e:\n# Replace A \u2212 \u2192G CDE by AB \u2212 \u2192G C[DE] , [DE] \u2212 \u2192G DE\n# Replace AB \u2212 \u2192G CDE by AB \u2212 \u2192G C[DE] , [DE] \u2212 \u2192G DE",
      "# Replace A \u2212 \u2192G CDE by AB \u2212 \u2192G C[DE] , [DE] \u2212 \u2192G DE\n# Replace AB \u2212 \u2192G CDE by AB \u2212 \u2192G C[DE] , [DE] \u2212 \u2192G DE\n# ( This s k i p s the c h a i n r u l e used i n the s l i d e s )\na s s e r t l e n( r h s ) > l e n( l h s )\ns t a r t s u f f i x = 1 i f l e n( l h s ) == 1 e l s e l e n( l h s ) = 1\nn e w r u l e s . add ( (\nl h s ,\nr h s [ : s t a r t s u f f i x ] + ( \u2019 [% s ] \u2019 % w o r d 2 s t r ( r h s [ s t a r t s u f f i x : ] ) , )\n) )\nf o r i i n range ( s t a r t s u f f i x , l e n( r h s )= 2) :\nn e w r u l e s . add ( (\n( \u2019 [% s ] \u2019 % w o r d 2 s t r ( r h s [ i : ] ) , ) ,\n( r h s [ i ] , \u2019 [% s ] \u2019 % w o r d 2 s t r ( r h s [ i + 1 : ] ) )\n) )\nn e w r u l e s . add ( (\n( \u2019 [% s ] \u2019 % w o r d 2 s t r ( r h s [= 2:]) , ) ,\nr h s [= 2] , r h s [ = 1])\n) )\n. . .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kuroda NF in Python (3/3) 260 (661)\ndef kurodaNF ( r u l e s : s e t[ Rule ] ) :\n. . .\nr u l e s = n e w r u l e s\nn e w r u l e s = s e t( )\nf o rl h s , r h s i n r u l e s :\ni f l e n( l h s ) <= 2 :\nn e w r u l e s . add ( ( l h s , r h s ) )\ne l s e:",
      "# Replace ABC \u2212 \u2192G DEF by AB \u2212 \u2192G D[C; EF ] , [C; EF ]C \u2212 \u2192G EF\na s s e r t l e n( l h s ) == l e n( r h s )\nn e w r u l e s . add ( (\n( l h s [ 0 ] , l h s [ 1 ] ) ,\n( r h s [ 0 ] , \u2019 [% s ;% s ] \u2019 % ( w o r d 2 s t r ( l h s [ 2 : ] ) , w o r d 2 s t r ( r h s [ 1 : ] ) ) , )\n) )\nf o r i i n range ( 1 , l e n( l h s )= 2) :\nn e w r u l e s . add ( (\n( \u2019 [% s ;% s ] \u2019 % ( w o r d 2 s t r ( l h s [ i + 1 : ] ) , w o r d 2 s t r ( r h s [ i : ] ) ) , l h s [ i +1]) ,\n( r h s [ i ] , \u2019 [% s ;% s ] \u2019 % ( w o r d 2 s t r ( l h s [ i + 2 : ] ) , w o r d 2 s t r ( r h s [ i + 1 : ] ) ) ) ,\n) )\nn e w r u l e s . add ( (\n( \u2019 [% s ;% s ] \u2019 % ( w o r d 2 s t r ( l h s [= 1:]) , w o r d 2 s t r ( r h s [ = 2:]) ) , l h s [ = 1]) ,\n( r h s [= 2] , r h s [ = 1]) ,\n) )\np r e t t y p r i n t g r a m m a r r u l e s ( n e w r u l e s )\nr e t u r nn e w r u l e s\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Simplifying context-free grammars in Python (1/2) 261 (662)\nfrom i t e r t o o l s import product , c o m b i n a t i o n s\ndef s i m p l i f yc o n t e x t f r e e g r a m m a r ( r u l e s : s e t[ Rule ] , axiom : Symbol , c o n t r a c t c h a i n s : bool =F a l s e ) :\na s s e r t a l l( l e n( l h s ) == 1 f o rl h s , r h s i n r u l e s )\nv a r i a b l e s = s e t( )\nt e r m i n a l s = s e t( )\nf o rl h s , r h s i n r u l e s :\nv a r i a b l e s |= s e t( l h s )\nt e r m i n a l s|= s e t( r h s )\nt e r m i n a l s= = v a r i a b l e s\np r i n t( \u2019 T e r m i n a l s : \u2019 , t e r m i n a l s )\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\ncan produce empty word = {l h s [ 0 ] f o rl h s , r h s i n r u l e s i f r h s == ew }\nw h i l eTrue :\nc u r l e n = l e n( can produce empty word )\ncan produce empty word |= {l h s [ 0 ] f o rl h s , r h s i n r u l e s i f a l l( x i n can produce empty word f o rx i n r h s )\n}",
      "can produce empty word = {l h s [ 0 ] f o rl h s , r h s i n r u l e s i f r h s == ew }\nw h i l eTrue :\nc u r l e n = l e n( can produce empty word )\ncan produce empty word |= {l h s [ 0 ] f o rl h s , r h s i n r u l e s i f a l l( x i n can produce empty word f o rx i n r h s )\n}\ni f c u r l e n == l e n( can produce empty word ) :\nbreak\np r i n t( \u2019 Can produce empty word : \u2019 , can produce empty word )\nn e w r u l e s = s e t( )\nf o rl h s , r h s i n r u l e s :\nf o rk i n range ( 1 , l e n( r h s ) ) : # s e l e c t at l e a s t one , but not a l l ; i f i n Kuroda NF : k = 1\nf o r s e l e c t i o n i n c o m b i n a t i o n s (range ( l e n( r h s ) ) , r=k ) :\ni f a l l( r h s [ i ] i n can produce empty word f o r i i n s e l e c t i o n ) :\nn e w r u l e s . add ( ( l h s ,t u p l e( r h s [ i ] f o r i i n range ( l e n( r h s ) ) i f i not i n s e l e c t i o n ) ) )\nr u l e s = = {r f o rr i n r u l e s i f r [ 1 ] == ew }\nr u l e s |= n e w r u l e s",
      "n e w r u l e s . add ( ( l h s ,t u p l e( r h s [ i ] f o r i i n range ( l e n( r h s ) ) i f i not i n s e l e c t i o n ) ) )\nr u l e s = = {r f o rr i n r u l e s i f r [ 1 ] == ew }\nr u l e s |= n e w r u l e s\ni f axiom i n can produce empty word :\nnew axiom = \u2019 [% s ] \u2019 % axiom\nr u l e s |= {(( new axiom , ) , ( axiom , ) ) , ( ( new axiom , ) , ew ) }\naxiom = new axiom\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\n. . .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Simplifying context-free grammars in Python (2/2) 262 (663)\ndef s i m p l i f yc o n t e x t f r e e g r a m m a r ( r u l e s : s e t[ Rule ] , axiom : Symbol , c o n t r a c t c h a i n s : bool =F a l s e ) :\n. . .\ni f c o n t r a c t c h a i n s :\nc h a i n r u l e s = {( l h s , r h s ) f o rl h s , r h s i n r u l e s i f l e n( r h s ) == 1 and r h s [ 0 ] not i n t e r m i n a l s}\nw h i l eTrue :\nc u r l e n = l e n( c h a i n r u l e s )\nc h a i n r u l e s |= {(e1 [ 0 ] , e2 [ 1 ] ) f o re1 , e2 i n product ( c h a i n r u l e s , r e p e a t =2) i f e1 [ 1 ] == e2 [0] }\ni f c u r l e n == l e n( c h a i n r u l e s ) :\nbreak\nr u l e s = = c h a i n r u l e s\nf o rr i n c h a i n r u l e s :\nr u l e s |= {( r [ 0 ] , r h s ) f o rl h s , r h s i n r u l e s i f l h s == r [1] }\np r i n t( \u2019 Chains : \u2019 , c h a i n r u l e s )\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\ni s p r o d u c t i v e = {x f o rx i n t e r m i n a l s}\nw h i l eTrue :",
      "r u l e s |= {( r [ 0 ] , r h s ) f o rl h s , r h s i n r u l e s i f l h s == r [1] }\np r i n t( \u2019 Chains : \u2019 , c h a i n r u l e s )\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\ni s p r o d u c t i v e = {x f o rx i n t e r m i n a l s}\nw h i l eTrue :\nc u r l e n = l e n( i s p r o d u c t i v e )\ni s p r o d u c t i v e |= {l h s [ 0 ] f o rl h s , r h s i n r u l e s i f a l l( x i n i s p r o d u c t i v e f o rx i n r h s )}\ni f c u r l e n == l e n( i s p r o d u c t i v e ) :\nbreak\np r i n t( \u2019 I s p r o d u c t i v e : \u2019 , i s p r o d u c t i v e )\nr u l e s = {( l h s , r h s ) f o rl h s , r h s i n r u l e s i f l h s [ 0 ] i n i s p r o d u c t i v e and a l l( x i n i s p r o d u c t i v e f o rx i n\nr h s )}\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\ni s r e a c h a b l e = {axiom }\nw h i l eTrue :\nc u r l e n = l e n( i s r e a c h a b l e )\nf o rl h s , r h s i n r u l e s :\ni f l h s [ 0 ] i n i s r e a c h a b l e :",
      "r h s )}\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\ni s r e a c h a b l e = {axiom }\nw h i l eTrue :\nc u r l e n = l e n( i s r e a c h a b l e )\nf o rl h s , r h s i n r u l e s :\ni f l h s [ 0 ] i n i s r e a c h a b l e :\ni s r e a c h a b l e |= {x f o rx i n r h s}\ni f c u r l e n == l e n( i s r e a c h a b l e ) :\nbreak\np r i n t( \u2019 I s r e a c h a b l e : \u2019 , i s r e a c h a b l e )\nr u l e s = {( l h s , r h s ) f o rl h s , r h s i n r u l e s i f l h s [ 0 ] i n i s r e a c h a b l e}\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\nr e t u r nr u l e s\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "SRS and Grammars in Python: examples 263 (664)\nr u l e s = {\n( t u p l e( \u2019 S \u2019 ) , t u p l e( \u2019ASBC \u2019 ) ) , # \u2018 t u p l e ( \u2019ASBC \u2019 ) \u2018 becomes \u2018 ( \u2019A \u2019 , \u2019 S \u2019 , \u2019B \u2019 , \u2019C \u2019 ) \u2018 i . e . Python f a c t o r s the\ns t r i n g i n t o i t s symbols\n( t u p l e( \u2019 S \u2019 ) , t u p l e( \u2019R \u2019 ) ) ,\n( t u p l e( \u2019RB \u2019 ) , t u p l e( \u2019BR \u2019 ) ) ,\n( t u p l e( \u2019RCB \u2019 ) , t u p l e( \u2019BCR \u2019 ) ) ,\n( t u p l e( \u2019RC \u2019 ) , t u p l e( \u2019CR \u2019 ) ) ,\n( t u p l e( \u2019RC \u2019 ) , t u p l e( \u2019 Lc \u2019 ) ) ,\n( t u p l e( \u2019CL \u2019 ) , t u p l e( \u2019 Lc \u2019 ) ) ,\n( t u p l e( \u2019BL \u2019 ) , t u p l e( \u2019 Lb \u2019 ) ) ,\n( t u p l e( \u2019AL \u2019 ) , t u p l e( \u2019 La \u2019 ) ) ,\n( t u p l e( \u2019AL \u2019 ) , t u p l e( \u2019 a \u2019 ) )\n}\nbfsSRS ( t u p l e( \u2019 S \u2019 ) , r u l e s , m a x s i z e =20)\nkurodaNF ( r u l e s )\nr u l e s = {\n( t u p l e( \u2019 S \u2019 ) , t u p l e( \u2019TT \u2019 ) ) ,\n( t u p l e( \u2019 S \u2019 ) , t u p l e( \u2019U \u2019 ) ) ,\n( t u p l e( \u2019T \u2019 ) , t u p l e( \u2019 aSb \u2019 ) ) ,\n( t u p l e( \u2019T \u2019 ) , ew ) ,\n( t u p l e( \u2019U \u2019 ) , t u p l e( \u2019 c \u2019 ) ) ,\n}",
      "kurodaNF ( r u l e s )\nr u l e s = {\n( t u p l e( \u2019 S \u2019 ) , t u p l e( \u2019TT \u2019 ) ) ,\n( t u p l e( \u2019 S \u2019 ) , t u p l e( \u2019U \u2019 ) ) ,\n( t u p l e( \u2019T \u2019 ) , t u p l e( \u2019 aSb \u2019 ) ) ,\n( t u p l e( \u2019T \u2019 ) , ew ) ,\n( t u p l e( \u2019U \u2019 ) , t u p l e( \u2019 c \u2019 ) ) ,\n}\ns i m p l i f yc o n t e x t f r e e g r a m m a r ( r u l e s , axiom=\u2019 S \u2019 , c o n t r a c t c h a i n s=True )\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "9 Appendix: (under construction)\nTuring Machines in Python\nHalting probem in pseudo-python\nString rewriting systems and formal grammars in Python\nFrom 1TM to grammar and PCP: example\nFinite Automata in Python\nPushdown Automata in Python (accept on final state)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From 1TM to grammar 265 (666)\nTuring machine M: \u03a3 = {a, b}, \u0393 = {a, b,\u25a1}, L(M) = {\u03b5, au| u \u2208 \u03a3\u2217}\nq0start qf\n\u25a1: a/0\na: \u25a1/ + 1\nGrammar rules to guess word in L(M):\nS \u2212 \u2192G BIB | B\n\u0014q0\u25a1\n\u25a1\n\u0015\nB B \u2212 \u2192G\n\u0014\u25a1\n\u25a1\n\u0015\nB |\n\u0014\u25a1\n\u25a1\n\u0015\nI \u2212 \u2192G I\n\u0014x\nx\n\u0015\n|\n\u0014q0x\nx\n\u0015\n(x \u2208 \u03a3)\nGrammar rules to simulate run in the upper part of the nonterminals:\n\u0014q0\u25a1\nx\n\u0015\n\u2212 \u2192G\n\u0014q0a\nx\n\u0015 \u0014 q0a\nx\n\u0015\u0014z\ny\n\u0015\n\u2212 \u2192G\n\u0014\u25a1\nx\n\u0015\u0014qf z\ny\n\u0015\n(x, y, z\u2208 {a, b,\u25a1})\nGrammar rules to forget upper part (if the final state has been reached) :\n\u0014z\nx\n\u0015\n\u2212 \u2192G x\n\u0014qf z\nx\n\u0015\n\u2212 \u2192G x\n\u0014z\n\u25a1\n\u0015\n\u2212 \u2192G \u03b5\n\u0014qf z\n\u25a1\n\u0015\n\u2212 \u2192G \u03b5 (z \u2208 \u0393, x\u2208 \u03a3)\nSimulation of a run on \u03b5:\nS \u2212 \u2192G B\n\u0014q0\u25a1\n\u25a1\n\u0015\nB \u2212 \u21922\nG\n\u0014\u25a1\n\u25a1\n\u0015\u0014q0\u25a1\n\u25a1\n\u0015\u0014\u25a1\n\u25a1\n\u0015\n\u2212 \u2192G\n\u0014\u25a1\n\u25a1\n\u0015\u0014q0a\n\u25a1\n\u0015\u0014\u25a1\n\u25a1\n\u0015\n\u2212 \u2192G\n\u0014\u25a1\n\u25a1\n\u0015\u0014\u25a1\n\u25a1\n\u0015\u0014qf \u25a1\n\u25a1\n\u0015\n\u2212 \u2192\u2217\nG \u03b5 \u2208 \u03a3\u2217\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From 1TM to grammar 265 (667)\nTuring machine M: \u03a3 = {a, b}, \u0393 = {a, b,\u25a1}, L(M) = {\u03b5, au| u \u2208 \u03a3\u2217}\nq0start qf\n\u25a1: a/0\na: \u25a1/ + 1\nGrammar rules to guess word in L(M):\nS \u2212 \u2192G BIB | B\n\u0014q0\u25a1\n\u25a1\n\u0015\nB B \u2212 \u2192G\n\u0014\u25a1\n\u25a1\n\u0015\nB |\n\u0014\u25a1\n\u25a1\n\u0015\nI \u2212 \u2192G I\n\u0014x\nx\n\u0015\n|\n\u0014q0x\nx\n\u0015\n(x \u2208 \u03a3)\nGrammar rules to simulate run in the upper part of the nonterminals:\n\u0014q0\u25a1\nx\n\u0015\n\u2212 \u2192G\n\u0014q0a\nx\n\u0015 \u0014 q0a\nx\n\u0015\u0014z\ny\n\u0015\n\u2212 \u2192G\n\u0014\u25a1\nx\n\u0015\u0014qf z\ny\n\u0015\n(x, y, z\u2208 {a, b,\u25a1})\nGrammar rules to forget upper part (if the final state has been reached) :\n\u0014z\nx\n\u0015\n\u2212 \u2192G x\n\u0014qf z\nx\n\u0015\n\u2212 \u2192G x\n\u0014z\n\u25a1\n\u0015\n\u2212 \u2192G \u03b5\n\u0014qf z\n\u25a1\n\u0015\n\u2212 \u2192G \u03b5 (z \u2208 \u0393, x\u2208 \u03a3)\nSimulation of a run on a:\nS \u2212 \u2192G BIB \u2212 \u21923\nG\n\u0014\u25a1\n\u25a1\n\u0015\u0014q0a\na\n\u0015\u0014\u25a1\n\u25a1\n\u0015\n\u2212 \u2192G\n\u0014\u25a1\n\u25a1\n\u0015\u0014\u25a1\na\n\u0015\u0014qf \u25a1\n\u25a1\n\u0015\n\u2212 \u2192\u2217\nG a \u2208 \u03a3\u2217\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From 1TM to grammar 265 (668)\nTuring machine M: \u03a3 = {a, b}, \u0393 = {a, b,\u25a1}, L(M) = {\u03b5, au| u \u2208 \u03a3\u2217}\nq0start qf\n\u25a1: a/0\na: \u25a1/ + 1\nGrammar rules to guess word in L(M):\nS \u2212 \u2192G BIB | B\n\u0014q0\u25a1\n\u25a1\n\u0015\nB B \u2212 \u2192G\n\u0014\u25a1\n\u25a1\n\u0015\nB |\n\u0014\u25a1\n\u25a1\n\u0015\nI \u2212 \u2192G I\n\u0014x\nx\n\u0015\n|\n\u0014q0x\nx\n\u0015\n(x \u2208 \u03a3)\nGrammar rules to simulate run in the upper part of the nonterminals:\n\u0014q0\u25a1\nx\n\u0015\n\u2212 \u2192G\n\u0014q0a\nx\n\u0015 \u0014 q0a\nx\n\u0015\u0014z\ny\n\u0015\n\u2212 \u2192G\n\u0014\u25a1\nx\n\u0015\u0014qf z\ny\n\u0015\n(x, y, z\u2208 {a, b,\u25a1})\nGrammar rules to forget upper part (if the final state has been reached) :\n\u0014z\nx\n\u0015\n\u2212 \u2192G x\n\u0014qf z\nx\n\u0015\n\u2212 \u2192G x\n\u0014z\n\u25a1\n\u0015\n\u2212 \u2192G \u03b5\n\u0014qf z\n\u25a1\n\u0015\n\u2212 \u2192G \u03b5 (z \u2208 \u0393, x\u2208 \u03a3)\nSimulation of a run on b (nonterminal containing state deadlocks) :\nS \u2212 \u2192G BIB \u2212 \u21923\nG\n\u0014\u25a1\n\u25a1\n\u0015\u0014q0b\nb\n\u0015\u0014\u25a1\n\u25a1\n\u0015\n\u2212 \u2192\u2217\nG\n\u0014q0b\nb\n\u0015\n\u0338\u2208 \u03a3\u2217\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From 1TM to PCP 266 (669)\nTuring machine M: \u03a3 = {a, b}, \u0393 = {a, b,\u25a1}, L(M) = {\u03b5, au| u \u2208 \u03a3\u2217}\nq0start qf\n\u25a1: a/0\na: \u25a1/ + 1\nStart, end, clean-up (left/right), copy, rules (top = pre/bot = post; treat # as \u25a1):\n\u0014 #\n#q0#\n\u0015\n,\n\u0014qf ##\n#\n\u0015\n,\n\"\nxqf\nqf\n#\n,\n\"\nqf x\nqf\n#\n,\n\u0014x\nx\n\u0015\n,\n\u0014q0\u25a1\nq0a\n\u0015\n,\n\u0014 q0#\nq0a#\n\u0015\n,\n\u0014q0a\n\u25a1qf\n\u0015\nAccepting run on \u03b5:\nq0\u25a1 \u2212 \u2192M q0a \u2212 \u2192M \u25a1qf\ntranslates into the solution:\n\u0014 #\n#q0#\n\u0015\u0014 q0#\nq0a#\n\u0015\u0014q0a\n\u25a1qf\n\u0015\u0014#\n#\n\u0015\"\n\u25a1qf\nqf\n#\u0014#\n#\n\u0015\u0014qf ##\n#\n\u0015\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From 1TM to PCP 266 (670)\nTuring machine M: \u03a3 = {a, b}, \u0393 = {a, b,\u25a1}, L(M) = {\u03b5, au| u \u2208 \u03a3\u2217}\nq0start qf\n\u25a1: a/0\na: \u25a1/ + 1\nStart, end, clean-up (left/right), copy, rules (top = pre/bot = post; treat # as \u25a1):\n\u0014 #\n#q0#\n\u0015\n,\n\u0014qf ##\n#\n\u0015\n,\n\"\nxqf\nqf\n#\n,\n\"\nqf x\nqf\n#\n,\n\u0014x\nx\n\u0015\n,\n\u0014q0\u25a1\nq0a\n\u0015\n,\n\u0014 q0#\nq0a#\n\u0015\n,\n\u0014q0a\n\u25a1qf\n\u0015\nIn general, for a given input w change the start to\n\u0014 #\n#q0w#\n\u0015\n, e.g. for w = a:\nq0a\u25a1 \u2212 \u2192M \u25a1qf\ntranslates into the solution:\n\u0014 #\n#q0a#\n\u0015\u0014q0a\n\u25a1qf\n\u0015\u0014#\n#\n\u0015\"\n\u25a1qf\nqf\n#\u0014#\n#\n\u0015\u0014qf ##\n#\n\u0015\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From 1TM to PCP 266 (671)\nTuring machine M: \u03a3 = {a, b}, \u0393 = {a, b,\u25a1}, L(M) = {\u03b5, au| u \u2208 \u03a3\u2217}\nq0start qf\n\u25a1: a/0\na: \u25a1/ + 1\nStart, end, clean-up (left/right), copy, rules (top = pre/bot = post; treat # as \u25a1):\n\u0014 #\n#q0#\n\u0015\n,\n\u0014qf ##\n#\n\u0015\n,\n\"\nxqf\nqf\n#\n,\n\"\nqf x\nqf\n#\n,\n\u0014x\nx\n\u0015\n,\n\u0014q0\u25a1\nq0a\n\u0015\n,\n\u0014 q0#\nq0a#\n\u0015\n,\n\u0014q0a\n\u25a1qf\n\u0015\nIf there is no accepting run, we do not reach qf and thus the upper part will trail\nbehind forever:\n\u0014 #\n#q0b#\n\u0015\u0014q0\nq0\n\u0015\u0014b\nb\n\u0015\u0014#\n#\n\u0015\u0014q0\nq0\n\u0015\u0014b\nb\n\u0015\u0014#\n#\n\u0015\n. . .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From 1TM to PCP 266 (672)\nTuring machine M: \u03a3 = {a, b}, \u0393 = {a, b,\u25a1}, L(M) = {\u03b5, au| u \u2208 \u03a3\u2217}\nq0start qf\n\u25a1: a/0\na: \u25a1/ + 1\nStart, end, clean-up (left/right), copy, rules (top = pre/bot = post; treat # as \u25a1):\n\u0014 #\n#q0#\n\u0015\n,\n\u0014qf ##\n#\n\u0015\n,\n\"\nxqf\nqf\n#\n,\n\"\nqf x\nqf\n#\n,\n\u0014x\nx\n\u0015\n,\n\u0014q0\u25a1\nq0a\n\u0015\n,\n\u0014 q0#\nq0a#\n\u0015\n,\n\u0014q0a\n\u25a1qf\n\u0015\nNote that in a general PCP, we may start with any pair.\nThis can be enforced as follows:\n\u0014 |#|\n|#|q0|#\n\u0015\n,\n\u0014qf |#|#|\n|#\n\u0015\n,\n\"\nx|qf |\n|qf\n#\n,\n\"\nqf |x|\n|qf\n#\n,\n\u0014x|\n|x\n\u0015\n,\n\u0014$\n|$\n\u0015\n,\n\u0014q0|\u25a1|\n|q0|a\n\u0015\n,\n\u0014 q0|#|\n|q0|a|#\n\u0015\n,\n\u0014q0|a|\n|\u25a1|qf\n\u0015\nwith |, $ so-far unsused symbols that act a separator resp. terminator.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "9 Appendix: (under construction)\nTuring Machines in Python\nHalting probem in pseudo-python\nString rewriting systems and formal grammars in Python\nFrom 1TM to grammar and PCP: example\nFinite Automata in Python\nPushdown Automata in Python (accept on final state)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "FA in Python 268 (674)\nimport networkx as nx\nfrom c o l l e c t i o n s import deque # Python \u2019 s double = ended queue\nfrom t y p i n g import *\nS t a t e = Any # Q\nSymbol = Any # e l e m e n t s o f\u03a3\new = ( ) # \u03b5 = ()\nWord = Tuple [ Symbol ] # \u03a3\u2217 r e c a l l words a r e a c t u a l l y t u p l e s\nRule = Tuple [ Tuple [ State , Symbol ] , S t a t e ] # (Q \u00d7 (\u03a3 \u222a {ew})) \u00d7 Q\nc f g 2 s t r = lambda c : \u2019 \u2019 . j o i n ( s t r( x ) f o rx i n ( c [ 0 ] , ) + c [ 1 ] )\n# The s t a n d a r d g r a p h i c a l r e p r e s e n t a t i o n o f an FA",
      "# The s t a n d a r d g r a p h i c a l r e p r e s e n t a t i o n o f an FA\n# TODO: h i g h l i g h t f i n a l s t a t e s .\ndef c a l l g r a p h F A ( r u l e s : Set [ Rule ] ) :\nG = nx . DiGraph ( )\ne d g e l a b e l s = d i c t( )\nf o rt i n r u l e s :\nqa , r = t\nq , a = qa\nG . add edge ( s t r( q ) , s t r( r ) )\ne d g e l a b e l s [ (s t r( q ) , s t r( r ) ) ] = f \u201d {a}\u201d\npos = nx . s p r i n g l a y o u t (G)\nnx . draw (G, pos=pos , w i t h l a b e l s=True )\nnx . d r a w n e t w o r k xe d g e l a b e l s (G, pos=pos , e d g e l a b e l s=e d g e l a b e l s )\nr e t u r n\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "FA in Python: reachable configurations (BFS) 269 (675)",
      "# I n c a s e o f an FA , t h e r e a r e o n l y f i n i t e l y many c o n f i g u r a t i o n s f o r a g i v e n i n p u t\ndef bfsFA (w: Word , r u l e s : Set [ Rule ] , q0 : State , f i n a l : Set [ S t a t e ] ) = >bool :\nc u r c o n f i g = ( q0 ,w)\ntodo = deque ( [ c u r c o n f i g ] )\na c c e p t = F a l s e\nG = nx . DiGraph ( )\nG . add node ( c f g 2 s t r ( c u r c o n f i g ) )\nw h i l e l e n( todo ) :\nc u r c o n f i g = todo . p o p l e f t ( )\nq , beta = c u r c o n f i g\na c c e p t = a c c e p t or ( q i n f i n a l and l e n( beta ) == 0) # a FA has to c o m p l e t e l y read i t s i n p u t\na = beta [ 0 ] i f l e n( beta ) e l s eew\na p p l i c a b l e r u l e s = [ ( t [ 0 ] [ 1 ] , t [ 1 ] ) f o rt i n r u l e s i f t [ 0 ] == ( q , a ) or t [ 0 ] == ( q , ew ) ]\nf o ra , r i n a p p l i c a b l e r u l e s :\nn x t c o n f i g = ( r , beta [ 1 : ] ) i f a != ew e l s e( r , beta )\ni f c f g 2 s t r ( n x t c o n f i g ) not i n G :",
      "f o ra , r i n a p p l i c a b l e r u l e s :\nn x t c o n f i g = ( r , beta [ 1 : ] ) i f a != ew e l s e( r , beta )\ni f c f g 2 s t r ( n x t c o n f i g ) not i n G :\ntodo . append ( n x t c o n f i g )\nG . add edge ( c f g 2 s t r ( c u r c o n f i g ) , c f g 2 s t r ( n x t c o n f i g ) )\npass\nnx . draw (G, w i t h l a b e l s=True )\nr e t u r na c c e p t\nr u l e s = {\n( ( \u2019A \u2019 , ew ) , \u2019A \u2019 ) , ( ( \u2019A \u2019 , \u2019 a \u2019 ) , \u2019A \u2019 ) , ( ( \u2019A \u2019 , \u2019 a \u2019 ) , \u2019B \u2019 ) ,\n( ( \u2019B \u2019 , ew ) , \u2019B \u2019 ) , ( ( \u2019B \u2019 , \u2019 a \u2019 ) , \u2019B \u2019 ) , ( ( \u2019B \u2019 , \u2019 b \u2019 ) , \u2019H \u2019 ) ,\n}\nbfsFA ( t u p l e( \u2019 aab \u2019 ) , r u l e s , \u2019A \u2019 , {\u2019H \u2019})\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "9 Appendix: (under construction)\nTuring Machines in Python\nHalting probem in pseudo-python\nString rewriting systems and formal grammars in Python\nFrom 1TM to grammar and PCP: example\nFinite Automata in Python\nPushdown Automata in Python (accept on final state)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "PDA in Python (accept on final state) 271 (677)\nimport networkx as nx\nfrom c o l l e c t i o n s import deque # Python \u2019 s double = ended queue\nfrom t y p i n g import *\nS t a t e = Any # Q\nSymbol = Any # e l e m e n t s o f\u03a3 , \u0393\new = ( ) # \u03b5 = ()\nWord = Tuple [ Symbol ] # \u03a3\u2217 , \u0393\u2217 r e c a l l words a r e a c t u a l l y t u p l e s\nRule = Tuple [ Tuple [ State , Symbol , Symbol ] , Tuple [ State , Word ] ] # (Q \u00d7 (\u03a3 \u222a {ew}) \u00d7 \u0393) \u00d7 (Q \u00d7 \u0393\u2217)\nc f g 2 s t r = lambda c : \u2019 \u2019 . j o i n ( s t r( x ) f o rx i n c [ 0 ] + ( c [ 1 ] , ) + c [ 2 ] )\n# C a l l g r a p h o f the PDA",
      "# C a l l g r a p h o f the PDA\n# TODO: h i g h l i g h t f i n a l s t a t e s .\ndef callgraphPDA ( r u l e s : Set [ Rule ] ) :\nG = nx . DiGraph ( )\ne d g e l a b e l s = d i c t( )\nf o rt i n r u l e s :\nqaX , rYZ = t\nq , a , X = qaX\nr , YZ = rYZ\nG . add edge ( s t r( q ) , s t r( r ) )\ne d g e l a b e l s [ (s t r( q ) , s t r( r ) ) ] = f \u201d {a},{X}:{ \u2019 \u2019. j o i n ( s t r ( x ) f o r x i n YZ) }\u201d\npos = nx . s p r i n g l a y o u t (G)\nnx . draw (G, pos=pos , w i t h l a b e l s=True )\nnx . d r a w n e t w o r k xe d g e l a b e l s (G, pos=pos , e d g e l a b e l s=e d g e l a b e l s )\nr e t u r n\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "PDA in Python: reachable configurations (BFS) 272 (678)\n# J u s t as normal r e c u r s i v e programs i n g e n e r a l ,\n# PDAs can r e c u r s e f o r e v e r , i . e . the ( c a l l ) s t a c k can grow unboundedly .",
      "# TODO: I n c o n t r a s t to TMs, we can a c t u a l d e c i d e whether a run i s n o n t e r m i n a t i n g .\ndef bfsPDA (w: Word , r u l e s : Set [ Rule ] , q0 : State , f i n a l : Set [ S t a t e ] , bot : Symbol , m a x s i z e : i n t=20)= >bool :\nc u r c o n f i g = ( t u p l e( [ bot ] ) , q0 ,w)\ntodo = deque ( [ c u r c o n f i g ] )\na c c e p t = F a l s e\nG = nx . DiGraph ( )\nG . add node ( c f g 2 s t r ( c u r c o n f i g ) )\nw h i l eG . number of nodes ( ) < m a x s i z e and l e n( todo ) :\nc u r c o n f i g = todo . p o p l e f t ( )\nalpha , q , beta = c u r c o n f i g\na c c e p t = a c c e p t or ( q i n f i n a l and l e n( beta ) == 0) # a c c e p t on f i n a l s t a t e\ni f l e n( a l p h a ) == 0 :\ncontinue\na = beta [ 0 ] i f l e n( beta ) e l s eew # l e f t= most l e t t e r o f i n p u t\nX = a l p h a [ = 1] # top = most s t a c k symbol",
      "a c c e p t = a c c e p t or ( q i n f i n a l and l e n( beta ) == 0) # a c c e p t on f i n a l s t a t e\ni f l e n( a l p h a ) == 0 :\ncontinue\na = beta [ 0 ] i f l e n( beta ) e l s eew # l e f t= most l e t t e r o f i n p u t\nX = a l p h a [ = 1] # top = most s t a c k symbol\na p p l i c a b l e r u l e s = [ ( t [ 0 ] [ 1 ] , * t [ 1 ] ) f o rt i n r u l e s i f t [ 0 ] == ( q , a , X) or t [ 0 ] == ( q , ew , X) ]\nf o ra , r , gamma i n a p p l i c a b l e r u l e s :\ngamma = t u p l e( r e v e r s e d(gamma) )\nn x t c o n f i g = ( a l p h a [: = 1] + gamma , r , beta [ 1 : ] ) i f a != ew e l s e( a l p h a [:= 1]+gamma , r , beta )\ni f c f g 2 s t r ( n x t c o n f i g ) not i n G :\ntodo . append ( n x t c o n f i g )\nG . add edge ( c f g 2 s t r ( c u r c o n f i g ) , c f g 2 s t r ( n x t c o n f i g ) )\npass\nnx . draw (G, w i t h l a b e l s=True )\nr e t u r na c c e p t\nr u l e s = {",
      "i f c f g 2 s t r ( n x t c o n f i g ) not i n G :\ntodo . append ( n x t c o n f i g )\nG . add edge ( c f g 2 s t r ( c u r c o n f i g ) , c f g 2 s t r ( n x t c o n f i g ) )\npass\nnx . draw (G, w i t h l a b e l s=True )\nr e t u r na c c e p t\nr u l e s = {\n( ( \u2019 q \u2019 , \u2019 c \u2019 , \u2019 \u22a5\u2019 ) , ( \u2019 f \u2019 , t u p l e( \u2019 \u2019 ) ) ) , ( ( \u2019 q \u2019 , \u2019 c \u2019 , \u2019 c \u2019 ) , ( \u2019 q \u2019 , t u p l e( \u2019 \u2019 ) ) ) ,\n( ( \u2019 q \u2019 , \u2019 a \u2019 , \u2019 \u22a5\u2019 ) , ( \u2019 q \u2019 , t u p l e( \u2019 c\u22a5\u2019 ) ) ) , ( ( \u2019 q \u2019 , \u2019 a \u2019 , \u2019 c \u2019 ) , ( \u2019 q \u2019 , t u p l e( \u2019 cc \u2019 ) ) ) ,\n( ( \u2019 q \u2019 , \u2019 b \u2019 , \u2019 \u22a5\u2019 ) , ( \u2019 q \u2019 , t u p l e( \u2019 \u22a5\u2019 ) ) ) ,\n}\nbfsPDA ( t u p l e( \u2019 acacbc \u2019 ) , r u l e s , \u2019 q \u2019 , {\u2019 f \u2019 }, \u2019 \u22a5\u2019 )\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS"
    ],
    "tags": [],
    "vector_store_path": "data/vector_stores/Info_-58193115880710748"
  },
  {
    "id": "45a04cb1-479b-4742-90d3-4b7bca31a54c",
    "title": "Bio",
    "upload_date": "2025-02-03",
    "file_name": "Biology 101.pdf",
    "chunks": [
      "Biology 101: Cellular Respiration \nKey Concepts: Mitochondria, ATP , Glycolysis, Krebs Cycle, Electron Transport Chain \n \n1. Overview \nCellular respiration is the process by which cells convert glucose and oxygen into ATP \n(adenosine triphosphate), the cell\u2019s energy currency. \nEquation: \nC6H12O6+6O2\u21926CO2+6H2O+ATPC6H12O6+6O2\u21926CO2+6H2O+ATP \n \n2. Stages of Cellular Respiration \na. Glycolysis \n\u2022 Occurs in the cytoplasm. \n\u2022 Breaks 1 glucose molecule into 2 pyruvate molecules. \n\u2022 Produces 2 ATP and 2 NADH. \nb. Krebs Cycle (Citric Acid Cycle) \n\u2022 Takes place in the mitochondrial matrix. \n\u2022 Generates 2 ATP, 6 NADH, and 2 FADH\u2082 per glucose. \nc. Electron Transport Chain (ETC) \n\u2022 Located in the inner mitochondrial membrane. \n\u2022 Uses NADH/FADH\u2082 to produce ~34 ATP via oxidative phosphorylation. \n \n3. Mitochondria Structure \nLabeled diagram (imagine an image here): \n1. Outer membrane \n2. Inner membrane (cristae) \n3. Matrix \nFunction: The \"powerhouse of the cell\" where the Krebs Cycle and ETC occur.",
      "\u2022 Uses NADH/FADH\u2082 to produce ~34 ATP via oxidative phosphorylation. \n \n3. Mitochondria Structure \nLabeled diagram (imagine an image here): \n1. Outer membrane \n2. Inner membrane (cristae) \n3. Matrix \nFunction: The \"powerhouse of the cell\" where the Krebs Cycle and ETC occur. \n \n4. Key Terms",
      "\u2022 ATP Synthase: Enzyme that produces ATP in the ETC. \n\u2022 Anaerobic Respiration: Occurs without oxygen (e.g., fermentation). \n\u2022 Chemiosmosis: Movement of protons to drive ATP synthesis"
    ],
    "tags": [],
    "vector_store_path": "data/vector_stores/Bio_5295260443130547705"
  },
  {
    "id": "a465abc3-d8f8-4b3a-88e3-6f63294b5f10",
    "title": "Math",
    "upload_date": "2025-02-03",
    "file_name": "math.pdf",
    "chunks": [
      "Calculus Lecture Notes\nProfessor John Doe\nJanuary 30, 2025\n1 Limits and Continuity\nDefinition 1 (Limit). Let f be a function defined on some open interval containing a\n(except possibly at a). We say the limit of f(x) as x approaches a is L, written:\nlim\nx\u2192a\nf(x) = L\nif for every \u03f5 > 0, there exists a \u03b4 > 0 such that whenever 0 < |x \u2212 a| < \u03b4, then\n|f(x) \u2212 L| < \u03f5.\nExample 1. Find the limit:\nlim\nx\u21922\nx2 \u2212 4\nx \u2212 2\nSolution:\nlim\nx\u21922\nx2 \u2212 4\nx \u2212 2 = lim\nx\u21922\n(x \u2212 2)(x + 2)\nx \u2212 2 = lim\nx\u21922\n(x + 2) = 4\n2 Derivatives\nDefinition 2 (Derivative). The derivative of a function f at x is:\nf\u2032(x) = lim\nh\u21920\nf(x + h) \u2212 f(x)\nh\nprovided this limit exists.\nTheorem 1 (Power Rule). For any real number n,\nd\ndxxn = nxn\u22121\nExample 2. Find the derivative of f(x) = 3x4 + 2x2 \u2212 5\nSolution:\nf\u2032(x) = 12x3 + 4x\n1",
      "3 Integrals\nDefinition 3 (Definite Integral). The definite integral of f from a to b is:\nZ b\na\nf(x)dx = lim\nn\u2192\u221e\nnX\ni=1\nf(x\u2217\ni )\u2206x\nTheorem 2 (Fundamental Theorem of Calculus) . If f is continuous on [a, b] and F is\nan antiderivative of f, then\nZ b\na\nf(x)dx = F(b) \u2212 F(a)\nExample 3. Calculate: Z 1\n0\nx2dx\nSolution: Z 1\n0\nx2dx =\n\u0014x3\n3\n\u00151\n0\n= 1\n3 \u2212 0 = 1\n3\nMatrix Examples\nA =\n\uf8eb\n\uf8ed\n1 2 3\n4 5 6\n7 8 9\n\uf8f6\n\uf8f8, B =\n\u0014a b\nc d\n\u0015\nConclusion\nThese notes cover basic concepts in calculus. Practice problems are recommended to\nreinforce these concepts.\n2"
    ],
    "tags": [],
    "vector_store_path": "data/vector_stores/Math_1832720698319514409"
  }
]