[
  {
    "id": "186c3b92-b90d-4e1f-91bd-1bd9eb969327",
    "title": "math",
    "upload_date": "2025-02-02",
    "file_name": "math.pdf",
    "chunks": [
      "Calculus Lecture Notes\nProfessor John Doe\nJanuary 30, 2025\n1 Limits and Continuity\nDefinition 1 (Limit). Let f be a function defined on some open interval containing a\n(except possibly at a). We say the limit of f(x) as x approaches a is L, written:\nlim\nx→a\nf(x) = L\nif for every ϵ > 0, there exists a δ > 0 such that whenever 0 < |x − a| < δ, then\n|f(x) − L| < ϵ.\nExample 1. Find the limit:\nlim\nx→2\nx2 − 4\nx − 2\nSolution:\nlim\nx→2\nx2 − 4\nx − 2 = lim\nx→2\n(x − 2)(x + 2)\nx − 2 = lim\nx→2\n(x + 2) = 4\n2 Derivatives\nDefinition 2 (Derivative). The derivative of a function f at x is:\nf′(x) = lim\nh→0\nf(x + h) − f(x)\nh\nprovided this limit exists.\nTheorem 1 (Power Rule). For any real number n,\nd\ndxxn = nxn−1\nExample 2. Find the derivative of f(x) = 3x4 + 2x2 − 5\nSolution:\nf′(x) = 12x3 + 4x\n1",
      "3 Integrals\nDefinition 3 (Definite Integral). The definite integral of f from a to b is:\nZ b\na\nf(x)dx = lim\nn→∞\nnX\ni=1\nf(x∗\ni )∆x\nTheorem 2 (Fundamental Theorem of Calculus) . If f is continuous on [a, b] and F is\nan antiderivative of f, then\nZ b\na\nf(x)dx = F(b) − F(a)\nExample 3. Calculate: Z 1\n0\nx2dx\nSolution: Z 1\n0\nx2dx =\n\u0014x3\n3\n\u00151\n0\n= 1\n3 − 0 = 1\n3\nMatrix Examples\nA =\n\n\n1 2 3\n4 5 6\n7 8 9\n\n, B =\n\u0014a b\nc d\n\u0015\nConclusion\nThese notes cover basic concepts in calculus. Practice problems are recommended to\nreinforce these concepts.\n2"
    ],
    "tags": [],
    "vector_store_path": "data/vector_stores/math_-292681241191205858"
  },
  {
    "id": "3e41c9a2-5662-4ef5-92a0-1614518103a6",
    "title": "Info",
    "upload_date": "2025-02-03",
    "file_name": "temp_theo23w (1).pdf",
    "chunks": [
      "Organization: 23w 1 (1)\n! See the course page on www.moodle.tum.de for more details (information\non moodle has precedence; this slide might become outdated!)\n▷ Lectures each Thursday starting at 15:30 (sharp) till approx. 18:00\n1.5 lectures (135min) plus a short break (10-15min) in between.\n(Because of Dies Academicus, 07.12.23, lecture might be moved to 08.12.23, 9:45-11:45)\n▷ Main tutorials: C. Welzel-Mohr, 2 groups on Thursdays, 8:15 resp. 10:15.\nOptional tutorial: N. Kienzle, 1 group on Mondays, 8:15.\n(Registration/assignment see moodle; TA: Christoph Welzel-Mohr)\n▷ Grade bonus: three graded exercises on Thursdays (probably 16.11.23, 21.12.23,\n01.01.24); (at most) 0.3, applies only after passing, (as before)\n▷ Exams: endterm and retake as usual, registration required, 150min, auxiliary\nmeans as usual (single hand-written cheat sheet, non-programmable calculator)\nRead the moodle course page; and use the forum or Zulip in case\nof questions!",
      "▷ Exams: endterm and retake as usual, registration required, 150min, auxiliary\nmeans as usual (single hand-written cheat sheet, non-programmable calculator)\nRead the moodle course page; and use the forum or Zulip in case\nof questions!\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Theory of Computation and Information Theory\n(INHN0013)\n© M. Luttenberger (23w)\nDepartment of Computer Science\nTUM CIT\n24. November 2023\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\nIntroduction\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Effective calculation, systems of calculation (calculi) 6 (6)\n▷ Classic notion of calculation/computation as known from school:\nFinite system of rigorous rules/instructions for solving some problem, e.g.:\n▷ Originally, “algorithm” was used for rules for computing with integers, e.g.\nEuclidean division or Euclidean algorithm.\n▷ Evaluating an arithmetic expression.\n▷ Computing the derivative of elementary functions.\n▷ Solving a system of linear equations by means of Gaussian elemination.\n▷ Rewriting a propositional formula into CNF (clause set).\n▷ Deciding the (un)satisfiability of a clause set using DPLL (resolution).\nRules describe in general a nondeterministic algorithm:\n▷ No predetermined order, but still guaranteed to reach/arrive at the correct\nresult/answer after a finite number of steps if applied correctly.\n▷ Rules can be applied mechanically using pen and paper; no understanding of\ntheir correctness or meaning required.",
      "▷ No predetermined order, but still guaranteed to reach/arrive at the correct\nresult/answer after a finite number of steps if applied correctly.\n▷ Rules can be applied mechanically using pen and paper; no understanding of\ntheir correctness or meaning required.\n(But clever use of rules might simplify/speed up the computation.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Etymology of “algorithm” 7 (7)\n(taken from wikipeida)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Mechanical calculating machines 8 (8)\n▷ E.g. when evaluating arithmetic expressions\n5 · 3 − 3 · 4\nwe still need to decide the order of evaluation:\n5 · 3 − 3 · 4 → 15 − 3 · 4 → 15 − 12 → 3\nvs.\n5 · 3 − 3 · 4 → 3 · (5 − 4) → 3 · 1 → 3\nBut: (i) as long as we reduce the number of involved operations, we are\nguaranteed to terminate, and (ii) as long as we apply only valid rules we\narrive at the same result no matter the order of evaluation.\n▷ Only requires rules for addition/substraction/multiplication by pen and paper.\n▷ Rules can be physically realized, e.g. multiplication by means of a Leibniz\nwheel, leading to the first mechanical calculating machines.\n▷ Heuristics as “tie breaker” if several rules can be applied.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rewriting: Gaussian elimination 9 (9)\n\n\n6 −2 0 1\n21 −7 1 −1\n18 −6 1 −1\n\n\n| ·1\n6\n\n\n1 0 0\n0 1 0\n0 0 1\n\n\n| ·1\n6\n▷ Many “mathematical procedures” are nondeterministic in the sense that they\noften require to make some suitable choice,\n▷ like the choice of the pivot in Gaussian elimination or in the simplex algorithm\nor quick sort\n▷ or the choice of the variable in DPLL or the choice of the clauses in resolution.\nbut the concrete choice is unimportant for the end result.\n! Yet, the precise choice still might influence the length of the calculation\n(runtime). For many important problems making the optimal choice is often\njust as difficult as solving the problem at hand.\nHence, we need to resort to heuristics to resolve nondeterminism.\n(See e.g. later: self-reducibility of SAT.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rewriting: Gaussian elimination 9 (10)\n\n\n1 −1\n3 0 1\n6\n21 −7 1 −1\n18 −6 1 −1\n\n ← −\n−21\n+\n← − − − − −\n−18\n+\n\n\n1\n6 0 0\n0 1 0\n0 0 1\n\n ← −\n−21\n+\n← − − − − −\n−18\n+\n▷ Many “mathematical procedures” are nondeterministic in the sense that they\noften require to make some suitable choice,\n▷ like the choice of the pivot in Gaussian elimination or in the simplex algorithm\nor quick sort\n▷ or the choice of the variable in DPLL or the choice of the clauses in resolution.\nbut the concrete choice is unimportant for the end result.\n! Yet, the precise choice still might influence the length of the calculation\n(runtime). For many important problems making the optimal choice is often\njust as difficult as solving the problem at hand.\nHence, we need to resort to heuristics to resolve nondeterminism.\n(See e.g. later: self-reducibility of SAT.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rewriting: Gaussian elimination 9 (11)\n\n\n1 −1\n3 0 1\n6\n0 0 1 −9\n2\n0 0 1 −4\n\n | ·1\n\n\n1\n6 0 0\n−7\n2 1 0\n−3 0 1\n\n | ·1\n▷ Many “mathematical procedures” are nondeterministic in the sense that they\noften require to make some suitable choice,\n▷ like the choice of the pivot in Gaussian elimination or in the simplex algorithm\nor quick sort\n▷ or the choice of the variable in DPLL or the choice of the clauses in resolution.\nbut the concrete choice is unimportant for the end result.\n! Yet, the precise choice still might influence the length of the calculation\n(runtime). For many important problems making the optimal choice is often\njust as difficult as solving the problem at hand.\nHence, we need to resort to heuristics to resolve nondeterminism.\n(See e.g. later: self-reducibility of SAT.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rewriting: Gaussian elimination 9 (12)\n\n\n1 −1\n3 0 1\n6\n0 0 1 −9\n2\n0 0 1 −4\n\n\n← −\n−1\n+\n\n\n1\n6 0 0\n−7\n2 1 0\n−3 0 1\n\n\n← −\n−1\n+\n▷ Many “mathematical procedures” are nondeterministic in the sense that they\noften require to make some suitable choice,\n▷ like the choice of the pivot in Gaussian elimination or in the simplex algorithm\nor quick sort\n▷ or the choice of the variable in DPLL or the choice of the clauses in resolution.\nbut the concrete choice is unimportant for the end result.\n! Yet, the precise choice still might influence the length of the calculation\n(runtime). For many important problems making the optimal choice is often\njust as difficult as solving the problem at hand.\nHence, we need to resort to heuristics to resolve nondeterminism.\n(See e.g. later: self-reducibility of SAT.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rewriting: Gaussian elimination 9 (13)\n\n\n1 −1\n3 0 1\n6\n0 0 1 −9\n2\n0 0 0 1\n2\n\n\n| ·2\n\n\n1\n6 0 0\n−7\n2 1 0\n1\n2 −1 1\n\n\n| ·2\n▷ Many “mathematical procedures” are nondeterministic in the sense that they\noften require to make some suitable choice,\n▷ like the choice of the pivot in Gaussian elimination or in the simplex algorithm\nor quick sort\n▷ or the choice of the variable in DPLL or the choice of the clauses in resolution.\nbut the concrete choice is unimportant for the end result.\n! Yet, the precise choice still might influence the length of the calculation\n(runtime). For many important problems making the optimal choice is often\njust as difficult as solving the problem at hand.\nHence, we need to resort to heuristics to resolve nondeterminism.\n(See e.g. later: self-reducibility of SAT.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rewriting: Gaussian elimination 9 (14)\n\n\n1 −1\n3 0 1\n6\n0 0 1 −9\n2\n0 0 0 1\n\n\n← −\n−1\n6\n+\n← −\n9\n2\n+\n\n\n1\n6 0 0\n−7\n2 1 0\n1 −2 2\n\n\n← −\n−1\n6\n+\n← −\n9\n2\n+\n▷ Many “mathematical procedures” are nondeterministic in the sense that they\noften require to make some suitable choice,\n▷ like the choice of the pivot in Gaussian elimination or in the simplex algorithm\nor quick sort\n▷ or the choice of the variable in DPLL or the choice of the clauses in resolution.\nbut the concrete choice is unimportant for the end result.\n! Yet, the precise choice still might influence the length of the calculation\n(runtime). For many important problems making the optimal choice is often\njust as difficult as solving the problem at hand.\nHence, we need to resort to heuristics to resolve nondeterminism.\n(See e.g. later: self-reducibility of SAT.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rewriting: Gaussian elimination 9 (15)\n\n\n1 −1\n3 0 0\n0 0 1 0\n0 0 0 1\n\n\n\n\n0 1\n3 −1\n3\n1 −8 9\n1 −2 2\n\n\n▷ Many “mathematical procedures” are nondeterministic in the sense that they\noften require to make some suitable choice,\n▷ like the choice of the pivot in Gaussian elimination or in the simplex algorithm\nor quick sort\n▷ or the choice of the variable in DPLL or the choice of the clauses in resolution.\nbut the concrete choice is unimportant for the end result.\n! Yet, the precise choice still might influence the length of the calculation\n(runtime). For many important problems making the optimal choice is often\njust as difficult as solving the problem at hand.\nHence, we need to resort to heuristics to resolve nondeterminism.\n(See e.g. later: self-reducibility of SAT.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPLL (basic version) 10 (16)\n{{¬p, q,¬r, s}, {¬q, ¬r, s}, {r}, {¬p, ¬s}, {¬p, r}}\n{{q, ¬r, s}, {¬q, ¬r, s}, {r}, {¬s}} {{¬q, ¬r, s}, {r}}\n{{¬r, s}, {r}, {¬s}} {{¬r, s}, {r}, {¬s}}\n{{s}, {¬s}} {{}, {¬s}}\n{{}} {{}}\n{{s}, {¬s}} {{}, {¬s}}\n{{}} {{}}\n{{¬r, s}, {r}}\n{{s}}\n{}\np := true p := false\nq := true\nq := false\nr := true r := false\ns := true s := false\nr := true r := false\ns := true s := false\nq := true\nr := true\ns := true\n▷ Variable order in the example: alphabetic order\n▷ Variable order influences running time, but finding optimal order just as hard as SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPLL (basic version) 10 (17)\n{{¬p, q,¬r, s}, {¬q, ¬r, s}, {r}, {¬p, ¬s}, {¬p, r}}\n{{q, ¬r, s}, {¬q, ¬r, s}, {r}, {¬s}} {{¬q, ¬r, s}, {r}}\n{{¬r, s}, {r}, {¬s}} {{¬r, s}, {r}, {¬s}}\n{{s}, {¬s}} {{}, {¬s}}\n{{}} {{}}\n{{s}, {¬s}} {{}, {¬s}}\n{{}} {{}}\n{{¬r, s}, {r}}\n{{s}}\n{}\np := true p := false\nq := true\nq := false\nr := true r := false\ns := true s := false\nr := true r := false\ns := true s := false\nq := true\nr := true\ns := true\n▷ Variable order in the example: alphabetic order\n▷ Variable order influences running time, but finding optimal order just as hard as SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPLL (basic version) 10 (18)\n{{¬p, q,¬r, s}, {¬q, ¬r, s}, {r}, {¬p, ¬s}, {¬p, r}}\n{{q, ¬r, s}, {¬q, ¬r, s}, {r}, {¬s}} {{¬q, ¬r, s}, {r}}\n{{¬r, s}, {r}, {¬s}} {{¬r, s}, {r}, {¬s}}\n{{s}, {¬s}} {{}, {¬s}}\n{{}} {{}}\n{{s}, {¬s}} {{}, {¬s}}\n{{}} {{}}\n{{¬r, s}, {r}}\n{{s}}\n{}\np := true p := false\nq := true\nq := false\nr := true r := false\ns := true s := false\nr := true r := false\ns := true s := false\nq := true\nr := true\ns := true\n▷ Variable order in the example: alphabetic order\n▷ Variable order influences running time, but finding optimal order just as hard as SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPLL (basic version) 10 (19)\n{{¬p, q,¬r, s}, {¬q, ¬r, s}, {r}, {¬p, ¬s}, {¬p, r}}\n{{q, ¬r, s}, {¬q, ¬r, s}, {r}, {¬s}} {{¬q, ¬r, s}, {r}}\n{{¬r, s}, {r}, {¬s}} {{¬r, s}, {r}, {¬s}}\n{{s}, {¬s}} {{}, {¬s}}\n{{}} {{}}\n{{s}, {¬s}} {{}, {¬s}}\n{{}} {{}}\n{{¬r, s}, {r}}\n{{s}}\n{}\np := true p := false\nq := true\nq := false\nr := true r := false\ns := true s := false\nr := true r := false\ns := true s := false\nq := true\nr := true\ns := true\n▷ Variable order in the example: alphabetic order\n▷ Variable order influences running time, but finding optimal order just as hard as SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPLL (basic version) 10 (20)\n{{¬p, q,¬r, s}, {¬q, ¬r, s}, {r}, {¬p, ¬s}, {¬p, r}}\n{{q, ¬r, s}, {¬q, ¬r, s}, {r}, {¬s}} {{¬q, ¬r, s}, {r}}\n{{¬r, s}, {r}, {¬s}} {{¬r, s}, {r}, {¬s}}\n{{s}, {¬s}} {{}, {¬s}}\n{{}} {{}}\n{{s}, {¬s}} {{}, {¬s}}\n{{}} {{}}\n{{¬r, s}, {r}}\n{{s}}\n{}\np := true p := false\nq := true\nq := false\nr := true r := false\ns := true s := false\nr := true r := false\ns := true s := false\nq := true\nr := true\ns := true\n▷ Variable order in the example: alphabetic order\n▷ Variable order influences running time, but finding optimal order just as hard as SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPLL (basic version) 10 (21)\n{{¬p, q,¬r, s}, {¬q, ¬r, s}, {r}, {¬p, ¬s}, {¬p, r}}\n{{q, ¬r, s}, {¬q, ¬r, s}, {r}, {¬s}} {{¬q, ¬r, s}, {r}}\n{{¬r, s}, {r}, {¬s}} {{¬r, s}, {r}, {¬s}}\n{{s}, {¬s}} {{}, {¬s}}\n{{}} {{}}\n{{s}, {¬s}} {{}, {¬s}}\n{{}} {{}}\n{{¬r, s}, {r}}\n{{s}}\n{}\np := true p := false\nq := true\nq := false\nr := true r := false\ns := true s := false\nr := true r := false\ns := true s := false\nq := true\nr := true\ns := true\n▷ Variable order in the example: alphabetic order\n▷ Variable order influences running time, but finding optimal order just as hard as SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPLL (basic version) 10 (22)\n{{¬p, q,¬r, s}, {¬q, ¬r, s}, {r}, {¬p, ¬s}, {¬p, r}}\n{{q, ¬r, s}, {¬q, ¬r, s}, {r}, {¬s}} {{¬q, ¬r, s}, {r}}\n{{¬r, s}, {r}, {¬s}} {{¬r, s}, {r}, {¬s}}\n{{s}, {¬s}} {{}, {¬s}}\n{{}} {{}}\n{{s}, {¬s}} {{}, {¬s}}\n{{}} {{}}\n{{¬r, s}, {r}}\n{{s}}\n{}\np := true p := false\nq := true\nq := false\nr := true r := false\ns := true s := false\nr := true r := false\ns := true s := false\nq := true\nr := true\ns := true\n▷ Variable order in the example: alphabetic order\n▷ Variable order influences running time, but finding optimal order just as hard as SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPLL (basic version) 10 (23)\n{{¬p, q,¬r, s}, {¬q, ¬r, s}, {r}, {¬p, ¬s}, {¬p, r}}\n{{q, ¬r, s}, {¬q, ¬r, s}, {r}, {¬s}} {{¬q, ¬r, s}, {r}}\n{{¬r, s}, {r}, {¬s}} {{¬r, s}, {r}, {¬s}}\n{{s}, {¬s}} {{}, {¬s}}\n{{}} {{}}\n{{s}, {¬s}} {{}, {¬s}}\n{{}} {{}}\n{{¬r, s}, {r}}\n{{s}}\n{}\np := true p := false\nq := true\nq := false\nr := true r := false\ns := true s := false\nr := true r := false\ns := true s := false\nq := true\nr := true\ns := true\n▷ Variable order in the example: alphabetic order\n▷ Variable order influences running time, but finding optimal order just as hard as SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Hilbert’s program 11 (24)\n▷ David Hilbert’s program in short: “mechanical proofs”\n▷ I.e. find a (countables) set of (proof) rules so that a computer can decide if a\nmathematical statement is valid (always true) or invalid (some\ncounterexample).\n▷ Propositional logics can describe arithmetic of n-bit integers Z 2n (e.g. CLA):\n(z0 ↔ (x0 ⊕ y0)) ∧ (z1 ↔ (x0 ∧ y0))\n▷ Hilbert’s tenth problem (1900): given an arithmetic expression over the\nintegers Z (i.e. a polynomial), decide if there is an integer solution, e.g.\nthere exist x, y∈ Z with x2 + y2 = 1\nis true as we can choose e.g. x = 1 and y = 0 (decision problem).\n! While validity of propositional formulas can be decided “mechanically” using\ne.g. a truth table, Hilbert’s tenth problem (essentially Z instead of Z n) cannot\nbe deciced by a (Turing) program as shown by Matiyasevich (1970).\n(Every (Turing) program can be encoded into an instance of Hilbert’s 10th problem.)",
      "e.g. a truth table, Hilbert’s tenth problem (essentially Z instead of Z n) cannot\nbe deciced by a (Turing) program as shown by Matiyasevich (1970).\n(Every (Turing) program can be encoded into an instance of Hilbert’s 10th problem.)\n(Both the existential theory of the reals and Presburger arithmetic are decidable.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Models of computation 12 (25)\n▷ Proving the undecidability of Hilbert’s tenth problem, the more general\nEntscheidungsproblem, and the related Halting problem requires first to\nprecisely define what “computation” and “calculation” means.\n▷ To this end, several models of computation have been proposed, e.g.:\n▷ 1933: µ-recursive functions (G ¨odel,Herbrand,Kleene)\n▷ 1936: Lambda calculus (Church)\n▷ 1936: Turing machines (Turing)\n▷ (string) rewriting systems (Thue, Post, Markov),\nabstract rewriting systems, grammars (Chomsky)\n▷ Register/counter machines and Random access machines\n▷ Cf. also History of the Church-Turing thesis on wikipedia.\n! So far it could always be shown that all models can simulate each other,\ni.e. if we can solve a problem using a particular model of computation, we\ncan translate this solution/algorithm into every other models.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Church-Turing thesis/conjecture 13 (26)\nWe shall use the expression “computable function” to mean a function\ncalculable by a [Turing] machine, and let “effectively calculable” refer to\nthe intuitive idea without particular identification with any one of these\ndefinitions (Turing, Systems of Logic Based on Ordinals).\n! Church-Turing thesis/conjecture:\nEvery effectively calculable function is also computable by a Turing machine,\n▷ I.e. the conjecture is that, if we can compute something using any kind of\nmodel of computation, then we can also compute it using a Turing machine\n(or µ-recursion or lambda calculus or register machines or ...).\n▷ We will restrict ourselves here to two important classic models\n▷ Turing machines: intentionally simplistic model of a computer, simpler than\nvon Neumann architecture (CPU). Allows to precisely define the behavior in\ncontrast to most CPUs/programming languages (cf. the JVM/byte code).",
      "▷ We will restrict ourselves here to two important classic models\n▷ Turing machines: intentionally simplistic model of a computer, simpler than\nvon Neumann architecture (CPU). Allows to precisely define the behavior in\ncontrast to most CPUs/programming languages (cf. the JVM/byte code).\n▷ Grammars/string rewriting systems: can be understood as a formalization of\ncalculating by means of rewriting expressions.\nand show that the two can simulate each other.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: alphabets and words 15 (28)\nD [n] = {1, 2, . . . , n} for n ∈ N 0 with [0] = ∅; Z n := {0, 1, . . . , n− 1}.\nD An alphabet A is a set so that a tuple (a1, . . . , an) (ai ∈ A) can be uniquely\nrecovered from the word a1 . . . an with ε := () the empty word (also, λ for ()).\n▷ e.g. {0, 1}, {00, 01, 10, 11}, {0, 10, 11} are alphabets; but not {1, 01, 011}.\n▷ e.g. {0, 1, . . . ,10, . . . ,15} vs. {0, 1, . . . , A, . . . , F}\n▷ e.g. {A, B, C, . . . , Z} and {∧, ∨, →, (, )} and {if, while, do} are alphabets.\n▷ Also often Σ (“symbol”) or Γ (γραϕω,“grapheme”) for alphabets.\nD If A is an alphabet, then (tuple as word) :\nA0 = {ε} An = {a1 . . . an | ai ∈ A for all i ∈ [n]} A∗ =\n[\ni∈N 0\nAi\n|u|: length of the word (as tuple) u ∈ A∗ with |u| ∈N 0 and |ε| = 0\nuv: concatenation of two words u, v∈ A∗\nD (A∗, ◦, ε) with u ◦ v := uv is also called the free monoid generated by A\n(Concatenation of words u ◦ v = uv is associative with ε the neutral element.)",
      "[\ni∈N 0\nAi\n|u|: length of the word (as tuple) u ∈ A∗ with |u| ∈N 0 and |ε| = 0\nuv: concatenation of two words u, v∈ A∗\nD (A∗, ◦, ε) with u ◦ v := uv is also called the free monoid generated by A\n(Concatenation of words u ◦ v = uv is associative with ε the neutral element.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "cf. Phoenician alphabet (image source) and history of alphabets\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: (formal) languages 17 (30)\nD L ⊆ A∗ is called a language; if L, L′ ⊆ A∗ are languages, then\n▷ LL′ = {uv | u ∈ L, v∈ L′} concatenation of two languages L, L′ ⊆ A∗\n▷ Natural powers of a language:\nL0 := {ε} Lk+1 := LLk = {w(1) . . . w(k+1) | w(1), . . . , w(k+1) ∈ L}(k ∈ N 0)\nKleene star/closure (finite, but unbounded repetition of words in L)\nL∗ :=\n[\nk∈N 0\nLk = {ε, w(1) . . . w(k) | k ∈ N , w(1), . . . , w(k) ∈ L}\n▷ Example: Let L := {a, ba} and L′ := {b, ab}.\n▷ LL′ = {a b, a ab, ba b, ba b}\n▷ L′L = {b a, b ba, ab a, ab ba}\n▷ L∗ = {ε} ∪ {a, ba} ∪ {a a, a ba, ba a, ba ba} ∪. . .\n▷ (L′)∗ = {ε} ∪ {b, ab} ∪ {b b, b ab, ab b, ab ab} ∪. . .\n! ∅∗ = {ε}\n▷ Cf. expansion of (a + ba)(b + ab) w/o commutativity.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: (formal) languages 17 (31)\nD L ⊆ A∗ is called a language; if L, L′ ⊆ A∗ are languages, then\n▷ LL′ = {uv | u ∈ L, v∈ L′} concatenation of two languages L, L′ ⊆ A∗\n▷ Natural powers of a language:\nL0 := {ε} Lk+1 := LLk = {w(1) . . . w(k+1) | w(1), . . . , w(k+1) ∈ L}(k ∈ N 0)\nKleene star/closure (finite, but unbounded repetition of words in L)\nL∗ :=\n[\nk∈N 0\nLk = {ε, w(1) . . . w(k) | k ∈ N , w(1), . . . , w(k) ∈ L}\nD Let A be an alphabet: (2A∗\n, ∪, ◦, ∅, {ε}) with L ◦ L′ := LL′ is also called the\nlanguage semiring or free semiring generated by A:\n▷ Union of languages L ∪ L′ is associative, commutative with neutral element ∅.\nConcatenation of languages L ◦ L′ := LL′ is associative with {ε} the neutral\nelement; distributes over union; and ∅ “annihilates”:\nL(L′L′′) = (LL′)L′′, L{ε} = L = {ε}L, L(L′ ∪ L′′) = LL′ ∪ LL′′,\n(L′ ∪ L′′)L = L′L ∪ L′′L, L∅ = ∅ = ∅L.\n! In general: LL′ ̸= L′L, L(L′ ∩ L′′) ⊊ LL′ ∩ LL′′.",
      "Concatenation of languages L ◦ L′ := LL′ is associative with {ε} the neutral\nelement; distributes over union; and ∅ “annihilates”:\nL(L′L′′) = (LL′)L′′, L{ε} = L = {ε}L, L(L′ ∪ L′′) = LL′ ∪ LL′′,\n(L′ ∪ L′′)L = L′L ∪ L′′L, L∅ = ∅ = ∅L.\n! In general: LL′ ̸= L′L, L(L′ ∩ L′′) ⊊ LL′ ∩ LL′′.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Alphabets and encoding 18 (32)\n▷ Unary alphabets (e.g. {|}) can represent a single natural number (cf. tally):\n▷ Using G¨odel’s encoding any finite list of natural numbers can be encoded:\n(n1, . . . , nk) 7→ 2n1 3n2 5n3 ··· pnk\nk\nIdentifying Σ with [|Σ|], every finite word can thus be encoded in unary, e.g.:\nTHEO 7→ (20, 8, 5, 15) 7→ |2203855715\n≥ |1026\n(1 = 2030 . . .encodes ε.)\n▷ Alphabets Σ with at least two symbols ( |Σ| > 1) allow for exponentially\nsmaller representations (recall logb n = log2 b · log2 n):\n111111111111111111111111111111111111111111 in unary\nvs 42 in decimal (MSDF)\nvs 010101 in binary (LSBF)\nvs 0211 in ternary (LSDF)\nvs 2A in hexadecimal (MSDF).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Alphabets and encoding 18 (33)\n▷ Consider a binary alphabet like {0, 1} or {a, b}:\n▷ Given some alphabet Σ = {a1, . . . , ak} choose m s.t. k ≤ 2m.\nE.g. m = 1 + ⌊log2 k⌋\n▷ Fix some injective map c from Σ to {0, 1}m, e.g. use least-significant bit first:\nc(a1) := 000 . . .0 c(a2) := 100 . . .0 c(a3) := 010 . . .0 . . .\nGiven w = w1 . . . wl ∈ Σ∗ map (encode) it to c(w1) . . . c(wl).\n(I.e. c is extended to a homomorphism.)\nWe then have |c(w)| = m|w|, i.e. the length is only increased by a constant\nfactor; and we can recover w by splitting c(w) into blocks of m bits.\n▷ A typical example is ASCII.\n▷ For more efficient encodings see Base64 and UTF8.\n▷ At the end of the lecture we will discuss how to come up with (prefix-free)\nencodings that minimize the expected length.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: binary relations and reflexive-transitive closure (*) 19 (34)\n! The symbol “ ∗” is also used for denoting the reflexive-transitive closure of a\nbinary relation R ⊆ X × X as the underlying concept is the same:\n▷ Product/composition and powers for binary relations R, S⊆ X × X\nRS = {(x, z) | (x, y) ∈ R, (y, z) ∈ S}\nR0 := Id = {(x, x) | x ∈ X} Rk+1 := RRk (k ∈ N 0)\ni.e.: (x, z) ∈ Rk iff there exist y1, . . . , yk−1 ∈ X s.t. xRy1R . . . Ryk−1Rz.\nIf we visualize R as edge relation of the directed graph (X, R) with nodes X,\nwe have: (x, z) ∈ Rk iff there is an x-z-path of length exactly k wrt. R.\n▷ The reflexive-transitive closure resp. transitive closure of R is then\nR∗ :=\n[\nk∈N 0\nRk resp. R+ := RR∗ =\n[\nk∈N\nRk\ni.e.: (x, z) ∈ R∗ iff there is some finite x-z-path wrt. R iff x can reach z.\n! Typically: − →for “(binary) successor/step relation”, Σ, Γ for alphabets.\n▷ − →∗: reflexive-transitive closure (“reachable in finite, but unbounded time”)\n▷ Σ∗, Γ∗, L∗: Kleene closure (for L ⊆ Σ∗)",
      "[\nk∈N\nRk\ni.e.: (x, z) ∈ R∗ iff there is some finite x-z-path wrt. R iff x can reach z.\n! Typically: − →for “(binary) successor/step relation”, Σ, Γ for alphabets.\n▷ − →∗: reflexive-transitive closure (“reachable in finite, but unbounded time”)\n▷ Σ∗, Γ∗, L∗: Kleene closure (for L ⊆ Σ∗)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: Kozen’s characterization of R∗ (*) 20 (35)\n▷ For now just a reminder: see DS and also later regular expressions again.\nT For all relations R, S, X⊆ A × A (cf. DS, Kozen):\n▷ R∗ = IdA ∪ R R∗ = IdA ∪ R∗ R.\n▷ If S ∪ R X⊆ X, then R∗ S ⊆ X.\n▷ If S ∪ X R⊆ X, then S R∗ ⊆ X.\nC For all R, S⊆ A × A:\n! R∗S is the ⊆-least solution of S ∪ R X= X.\nS R∗ is the ⊆-least solution of S ∪ X R= X.\nD R+ := RR∗ is the transitive closure of R.\n▷ R∗ = R∗R∗ = (R∗)∗ = (R+)∗ = (R∗)+\n▷ If R ⊆ S, then R∗ ⊆ S∗ and R+ ⊆ S+.\n▷ (R ∪ S)∗ = R∗(SR∗)∗ = (R∗S)∗R∗\n▷ If RX = XS, then R∗X = XS∗.\n(Proof by induction, see DS.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From languages to relations and back again 21 (36)\n▷ The use of ∗ for both languages and relations can be motivated as follows:\nLet Σ be an alphabet, and define\nRε := IdΣ∗ = {(w, w) | w ∈ Σ∗} Ra := {(w, wa) | a ∈ wΣ∗}\nand extend this to words w, w′ ∈ Σ∗ and languages L ⊆ Σ∗ by\nRww′ := RwRw′ RL :=\n[\nw∈L\nRw R{w} = Rw\nWe can recover L from its representations as relation:\nL = {w | (ε, w) ∈ RL} = εRL\nWe now have that:\nRL∗ = R∗\nL\n(i.e. h(L) := RL is an injective homomorphism from the language semiring to the semiring\nof binary relations that is also compatible with ∗; cf. also C → R 2×2.)\n▷ Hence, all properties of ∗ for binary relations carry over to languages, e.g.:\n(L∗)∗ = L∗ L∗L∗ = L∗ (L ∪ L′)∗ = (L∗L′)∗L∗ = L∗(L′L∗)∗\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Alphabets as trees 22 (37)\n▷ Any finite alphabet Σ can be visualized as infinite |Σ|-ary tree (Σ∗, RΣ).\nFor example Σ = {a, b} leads to the infinite binary tree:\nε\na b\naa ab ba bb\naaa aab aba abb baa bab bba bbb\n(which is the Cayley graph of the free monoid Σ∗ wrt. Σ as generators.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "3 Turing machines, computable functions, semidecidable languages\nTuring machines\nComputable and non-computable functions\nSemidecidable/recognizable/recursively enumerable languages\nHalting problem, Rice’s theorem, deciding properties of computable functions\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Turing machines: informally 25 (40)\n▷ Proposed by Alan Turing as one model/definition of computation in 1936.\nIntentionally very simplistic in order to simplify proofs/arguments.\n▷ In terms of the von Neumann architecture (1945):\n▷ Memory unit, input and output device all captured by the notion ot tape.\nA tape is an infinite tape drive.\nA tape consists of countably infinitely many (atomic memory) cells.\nIn the simplest case just one single tape initially containing just the input.\nOnly the cell at the current position of the (tape) head can be read.\n▷ Control unit and arithemtic unit captured by states and transition rules.\n! In contrast to a real physical computer,\n▷ a transition rules may be nondeterministic, i.e. several rules might be\napplicable.\n▷ the tapes of a Turing machine are unbounded, i.e. a Turing machine has\nunbounded memory, but in finite time it can only access a finite amount of it.",
      "! In contrast to a real physical computer,\n▷ a transition rules may be nondeterministic, i.e. several rules might be\napplicable.\n▷ the tapes of a Turing machine are unbounded, i.e. a Turing machine has\nunbounded memory, but in finite time it can only access a finite amount of it.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Turing machines with a single tape 26 (41)\nD (one-tape) Turing machine (1TM) M = (Q, Σ, Γ, δ, q0, □, F)\n▷ Q: finite set of states\n▷ Σ: the finite input alphabet, e.g. Σ = {0, 1}\n▷ Γ: finite tape alphabet with Σ ⊆ Γ, e.g. Γ = {0, 1, □}\n▷ □: the blank symbol with □ ∈ Γ \\ Σ (“none”)\n▷ q0: the initial state with q0 ∈ Q\n▷ F: the final states with F ⊆ Q\n▷ δ: the transition rules with δ ⊆ (Q \\ F × Γ) × (Q × Γ × {−1, 0, +1})\nA transition rule ((q, a), (r, b, d)) ∈ δ stands for the instruction:\nif state == ’q’ and tape[pos] == ’a’:\nstate = ’r’ # update state\ntape[pos] = ’b’ # update tape\npos += d # move tape head\nA 1TM is deterministic (D1TM) if |(q, a)δ| ≤1 for every (q, a) ∈ Q × Γ,\notherwise it is nondeterministic.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A nondeterministic 1TM 27 (42)\nΣ = {a, b}\nΓ = Σ ∪ {□}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA □ A □ 0\nA a A a +1\nA a B a +1\nq a r b d\nB □ B □ 0\nB a B a 0\nB b H b 0\nAstart B H\n□: □/0\na: a/+ 1\na: a/+ 1\n□: □/0\na: a/+ 1\nb: b/0\nA\n. . .□ a a b □ . . .\nAaab\n▷ Accepted language: L(M) = {akabw | k ∈ N 0, w∈ {a, b}∗}\n▷ A nondeterministic 1TM can have multiple maximal runs for the same input.\nThe maximal runs can be illustrated as a directed (not necessarily finite) graph\non the configurations.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A nondeterministic 1TM 27 (43)\nΣ = {a, b}\nΓ = Σ ∪ {□}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA □ A □ 0\nA a A a +1\nA a B a +1\nq a r b d\nB □ B □ 0\nB a B a 0\nB b H b 0\nAstart B H\n□: □/0\na: a/+ 1\na: a/+ 1\n□: □/0\na: a/+ 1\nb: b/0\nA\n. . .□ a a b □ . . .\nAaab − →M aAab\n▷ Accepted language: L(M) = {akabw | k ∈ N 0, w∈ {a, b}∗}\n▷ A nondeterministic 1TM can have multiple maximal runs for the same input.\nThe maximal runs can be illustrated as a directed (not necessarily finite) graph\non the configurations.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A nondeterministic 1TM 27 (44)\nΣ = {a, b}\nΓ = Σ ∪ {□}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA □ A □ 0\nA a A a +1\nA a B a +1\nq a r b d\nB □ B □ 0\nB a B a 0\nB b H b 0\nAstart B H\n□: □/0\na: a/+ 1\na: a/+ 1\n□: □/0\na: a/+ 1\nb: b/0\nB\n. . .□ a a b □ . . .\nAaab − →M aAab − →M aaBb\n▷ Accepted language: L(M) = {akabw | k ∈ N 0, w∈ {a, b}∗}\n▷ A nondeterministic 1TM can have multiple maximal runs for the same input.\nThe maximal runs can be illustrated as a directed (not necessarily finite) graph\non the configurations.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A nondeterministic 1TM 27 (45)\nΣ = {a, b}\nΓ = Σ ∪ {□}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA □ A □ 0\nA a A a +1\nA a B a +1\nq a r b d\nB □ B □ 0\nB a B a 0\nB b H b 0\nAstart B H\n□: □/0\na: a/+ 1\na: a/+ 1\n□: □/0\na: a/+ 1\nb: b/0\nH\n. . .□ a a b □ . . .\nAaab − →M aAab − →M aaBb − →M aaHb\n▷ Accepted language: L(M) = {akabw | k ∈ N 0, w∈ {a, b}∗}\n▷ A nondeterministic 1TM can have multiple maximal runs for the same input.\nThe maximal runs can be illustrated as a directed (not necessarily finite) graph\non the configurations.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A nondeterministic 1TM 27 (46)\nΣ = {a, b}\nΓ = Σ ∪ {□}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA □ A □ 0\nA a A a +1\nA a B a +1\nq a r b d\nB □ B □ 0\nB a B a 0\nB b H b 0\nAstart B H\n□: □/0\na: a/+ 1\na: a/+ 1\n□: □/0\na: a/+ 1\nb: b/0\nAaab aAab\naBab\naaAb\naaBb aaHb\nA□\nAb\nAaa□ aAa□\naBa□\naaA□\naaB□\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM: configuration, run 28 (47)\nD Let M = (Q, Σ, Γ, δ, q0, □, F) be a 1TM. wlog Q ∩ Γ = ∅.\nA configuration\nαqβ := (α, q, β) ∈ Γ∗ × Q × Γ∗\nof a 1TM describes the current tape content and the position of the head:\nThe current tape content is . . .□□αβ□□. . .\nThe currently read symbol is the first symbol of β□□. . .\ni.e. αβ ∈ Γ∗ is implicitly extended in both directions by infinitely many blanks;\nthus, if β = ε, then the 1TM currently reads a blank □.\n! αqβ and □. . .□αqβ□. . .□ actually represent the same situation.\nUsually convention: drop trailing and leading blanks ( □).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM: configuration, run 28 (48)\nD Let M = (Q, Σ, Γ, δ, q0, □, F) be a 1TM. wlog Q ∩ Γ = ∅.\nThe rules δ generate the binary relation − →M on Γ∗ × Q × Γ∗:\n− →M := {(αqaβ, αbrβ) | ((q, a), (r, b,+1)) ∈ δ, α, β∈ Γ∗}\n∪ { (αzqaβ, αrzbβ) | ((q, a), (r, b,−1)) ∈ δ, α, β∈ Γ∗, z∈ Γ}\n∪ { (αqaβ, αrbβ) | ((q, a), (r, b,0)) ∈ δ, α, β∈ Γ∗}\nA run of the 1TM is a path α0q0β0 − →M α1q1β1 − →M . . .wrt. − →M .\n▷ “run” is just another word for “computation/calculation of the 1TM”.\n▷ A run on input x1 . . . xl ∈ Σl is a path wrt. − →M starting at q0x1 . . . xl\n▷ A run can be finite or infinite; a finite run is maximal if it cannot further be\nextended (dead end reached); an infinite run is always maximal.\n▷ δ is deterministic iff every configuration has at most one successor wrt. − →M\niff for every configuration there is exactly one maximal run.\n▷ Wrt. − →M we can think of Γ∗ × Q × Γ∗ as the infinite directed graph of all\ncalculations (semantics of the 1TM). − →∗\nM is the reflexive-transitive closure.",
      "iff for every configuration there is exactly one maximal run.\n▷ Wrt. − →M we can think of Γ∗ × Q × Γ∗ as the infinite directed graph of all\ncalculations (semantics of the 1TM). − →∗\nM is the reflexive-transitive closure.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM: configuration, run 28 (49)\nD Let M = (Q, Σ, Γ, δ, q0, □, F) be a 1TM. wlog Q ∩ Γ = ∅.\nThe 1TM terminates/halts on a given input w ∈ Σ∗ if all maximal runs\nstarting in q0w are finite.\nA run on input w ∈ Σ∗ is accepting if it ends in a configuration Γ∗ ×F ×Γ∗.\n▷ By definition, a run that visits a configuration Γ∗ × F × Γ∗ is always finite as\nwe require δ ⊆ (Q \\ F × Γ) × (Q × Γ × {−1, 0, +1}).\n▷ We can always let an 1TM empty its tape before entering a final state.\n▷ Analogously, we can turn a “dead end” into a “deadlock”:\nIf (q, a)δ = ∅ and q ̸∈ F, add ((q, a), (q, a,0)) to δ (loop forever).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM: configuration, run 28 (50)\nD Let M = (Q, Σ, Γ, δ, q0, □, F) be a 1TM. wlog Q ∩ Γ = ∅.\nThe 1TM accepts the input w ∈ Σ∗ if there is at least one accepting run.\nThe language accepted/recognized by the 1TM M is\nL(M) := {w ∈ Σ∗ | q0w − →∗\nM αqf β, qf ∈ F}\nAn accepting run on w can be thought of as a proof/certificate that\nw ∈ L(M).\nIf M is nondeterministic, then there might be multiple runs on input w: but\nfor w ∈ L(M) it suffices, if at least one of them is accepting. (Sometimes a\n“proof attempt” fails.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Busy beaver 29 (51)\nΣ = ∅\nΓ = Σ ∪ {□, x}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA □ B x +1\nA x B x −1\nB □ A x −1\nB x H x +1\nAstart B H□: x/+ 1; x: x/−1\n□: x/−1\nx: x/+ 1\nA\n. . .□ □ □ □ □ □ □ □ . . .\n□□A□□\n! δ is often read as labeled edges between the states Q.\n▷ A (standard) busy beaver is a deterministic 1TM that takes no input (only\nblanks), terminates eventually (only a finite maximal run), tries to write a\nmaximal number of xs (or any other symbol except □).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Busy beaver 29 (52)\nΣ = ∅\nΓ = Σ ∪ {□, x}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA □ B x +1\nA x B x −1\nB □ A x −1\nB x H x +1\nAstart B H□: x/+ 1; x: x/−1\n□: x/−1\nx: x/+ 1\nB\n. . .□ □ □ x □ □ □ □ . . .\n□□A□□ − →M □□xB□\n! δ is often read as labeled edges between the states Q.\n▷ A (standard) busy beaver is a deterministic 1TM that takes no input (only\nblanks), terminates eventually (only a finite maximal run), tries to write a\nmaximal number of xs (or any other symbol except □).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Busy beaver 29 (53)\nΣ = ∅\nΓ = Σ ∪ {□, x}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA □ B x +1\nA x B x −1\nB □ A x −1\nB x H x +1\nAstart B H□: x/+ 1; x: x/−1\n□: x/−1\nx: x/+ 1\nA\n. . .□ □ □ x x □ □ □ . . .\n□□A□□ − →M □□xB□ − →M □□Axx\n! δ is often read as labeled edges between the states Q.\n▷ A (standard) busy beaver is a deterministic 1TM that takes no input (only\nblanks), terminates eventually (only a finite maximal run), tries to write a\nmaximal number of xs (or any other symbol except □).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Busy beaver 29 (54)\nΣ = ∅\nΓ = Σ ∪ {□, x}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA □ B x +1\nA x B x −1\nB □ A x −1\nB x H x +1\nAstart B H□: x/+ 1; x: x/−1\n□: x/−1\nx: x/+ 1\nB\n. . .□ □ □ x x □ □ □ . . .\n□□A□□ − →M □□xB□ − →M □□Axx − →M □B□xx\n! δ is often read as labeled edges between the states Q.\n▷ A (standard) busy beaver is a deterministic 1TM that takes no input (only\nblanks), terminates eventually (only a finite maximal run), tries to write a\nmaximal number of xs (or any other symbol except □).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Busy beaver 29 (55)\nΣ = ∅\nΓ = Σ ∪ {□, x}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA □ B x +1\nA x B x −1\nB □ A x −1\nB x H x +1\nAstart B H□: x/+ 1; x: x/−1\n□: x/−1\nx: x/+ 1\nA\n. . .□ □ x x x □ □ □ . . .\n□□A□□ − →M □□xB□ − →M □□Axx − →M □B□xx − →M A□xxx\n! δ is often read as labeled edges between the states Q.\n▷ A (standard) busy beaver is a deterministic 1TM that takes no input (only\nblanks), terminates eventually (only a finite maximal run), tries to write a\nmaximal number of xs (or any other symbol except □).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Busy beaver 29 (56)\nΣ = ∅\nΓ = Σ ∪ {□, x}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA □ B x +1\nA x B x −1\nB □ A x −1\nB x H x +1\nAstart B H□: x/+ 1; x: x/−1\n□: x/−1\nx: x/+ 1\nB\n. . .□ x x x x □ □ □ . . .\n□□A□□ − →M □□xB□ − →M □□Axx − →M □B□xx − →M A□xxx − →M xBxxx\n! δ is often read as labeled edges between the states Q.\n▷ A (standard) busy beaver is a deterministic 1TM that takes no input (only\nblanks), terminates eventually (only a finite maximal run), tries to write a\nmaximal number of xs (or any other symbol except □).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Busy beaver 29 (57)\nΣ = ∅\nΓ = Σ ∪ {□, x}\nQ = {A, B, H}\nF = {H} q0 = A\nq a r b d\nA □ B x +1\nA x B x −1\nB □ A x −1\nB x H x +1\nAstart B H□: x/+ 1; x: x/−1\n□: x/−1\nx: x/+ 1\nH\n. . .□ x x x x □ □ □ . . .\n□□A□□ − →M □□xB□ − →M □□Axxx − →M □B□xx − →M A□xxx − →M xBxxx − →M xxHxx\n! δ is often read as labeled edges between the states Q.\n▷ A (standard) busy beaver is a deterministic 1TM that takes no input (only\nblanks), terminates eventually (only a finite maximal run), tries to write a\nmaximal number of xs (or any other symbol except □).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing “ +1” in LSBF 30 (58)\nΣ = {0, 1}\nΓ = Σ ∪ {□}\nQ = {inc, H}\nF = {H} q0 = inc\nq a r b d\ninc □ H □ 1\ninc 0 H 1 0\ninc 1 inc 0 +1\nincstart H\n1: 0/+ 1\n0: 1/0; □: □/1\ninc\n. . .□ 1 1 1 0 1 □ . . .\ninc11101\n▷ Usually, we are interested in deterministic TMs (programs/algorithms).\n▷ Using e.g. least-significant-bit-first representation, e.g. J11K10 = J1101K2 and\nJ12K10 = J0011K2, TMs can implement arithmetic operations. (Adapt the 1TM\nto MSBF.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing “ +1” in LSBF 30 (59)\nΣ = {0, 1}\nΓ = Σ ∪ {□}\nQ = {inc, H}\nF = {H} q0 = inc\nq a r b d\ninc □ H □ 1\ninc 0 H 1 0\ninc 1 inc 0 +1\nincstart H\n1: 0/+ 1\n0: 1/0; □: □/1\ninc\n. . .□ 0 1 1 0 1 □ . . .\ninc11101 − →M 0inc1101\n▷ Usually, we are interested in deterministic TMs (programs/algorithms).\n▷ Using e.g. least-significant-bit-first representation, e.g. J11K10 = J1101K2 and\nJ12K10 = J0011K2, TMs can implement arithmetic operations. (Adapt the 1TM\nto MSBF.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing “ +1” in LSBF 30 (60)\nΣ = {0, 1}\nΓ = Σ ∪ {□}\nQ = {inc, H}\nF = {H} q0 = inc\nq a r b d\ninc □ H □ 1\ninc 0 H 1 0\ninc 1 inc 0 +1\nincstart H\n1: 0/+ 1\n0: 1/0; □: □/1\ninc\n. . .□ 0 0 1 0 1 □ . . .\ninc11101 − →M 0inc1101 − →M 00inc101\n▷ Usually, we are interested in deterministic TMs (programs/algorithms).\n▷ Using e.g. least-significant-bit-first representation, e.g. J11K10 = J1101K2 and\nJ12K10 = J0011K2, TMs can implement arithmetic operations. (Adapt the 1TM\nto MSBF.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing “ +1” in LSBF 30 (61)\nΣ = {0, 1}\nΓ = Σ ∪ {□}\nQ = {inc, H}\nF = {H} q0 = inc\nq a r b d\ninc □ H □ 1\ninc 0 H 1 0\ninc 1 inc 0 +1\nincstart H\n1: 0/+ 1\n0: 1/0; □: □/1\ninc\n. . .□ 0 0 0 0 1 □ . . .\ninc11101 − →M 0inc1101 − →M 00inc101 − →M 000inc01\n▷ Usually, we are interested in deterministic TMs (programs/algorithms).\n▷ Using e.g. least-significant-bit-first representation, e.g. J11K10 = J1101K2 and\nJ12K10 = J0011K2, TMs can implement arithmetic operations. (Adapt the 1TM\nto MSBF.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing “ +1” in LSBF 30 (62)\nΣ = {0, 1}\nΓ = Σ ∪ {□}\nQ = {inc, H}\nF = {H} q0 = inc\nq a r b d\ninc □ H □ 1\ninc 0 H 1 0\ninc 1 inc 0 +1\nincstart H\n1: 0/+ 1\n0: 1/0; □: □/1\nH\n. . .□ 0 0 0 1 1 □ . . .\ninc11101 − →M 0inc1101 − →M 00inc101 − →M 000inc01 − →M 000H11\n▷ Usually, we are interested in deterministic TMs (programs/algorithms).\n▷ Using e.g. least-significant-bit-first representation, e.g. J11K10 = J1101K2 and\nJ12K10 = J0011K2, TMs can implement arithmetic operations. (Adapt the 1TM\nto MSBF.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing “ +” in LSBF 31 (63)\nΣ =\nn\n\u00140\n0\n\u0015\n,\n\u00141\n0\n\u0015\n,\n\u00140\n1\n\u0015\n,\n\u00141\n1\n\u0015\no\nΓ = Σ ∪ {□, 0, 1} Q = {add0, add1, H}F = {H}q0 = add0\nadd0start add1\nH\n\u00140\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 1/+ 1;\n\u00141\n0\n\u0015\n: 1/+ 1;\n\u00141\n1\n\u0015\n: 0/+ 1\n□: □/0\n\u00141\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 0/+ 1;\n\u00141\n1\n\u0015\n: 1/+ 1;\n\u00140\n0\n\u0015\n: 1/+ 1\n□: 1/0\nadd0\n. . .□\n\u00140\n1\n\u0015 \u00141\n1\n\u0015 \u00141\n0\n\u0015 \u00141\n1\n\u0015\n□ □ . . .\nadd0\n\u00140\n1\n\u0015\u00141\n1\n\u0015\u00141\n0\n\u0015\u00141\n1\n\u0015\n□\n! Using e.g. vectors/tuples (and at the cost of additional “book-keeping” and\nintermediate steps), a 1TM can also simulate multiple tapes (with independent\nheads). (cf. memory bus width, bit rate)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing “ +” in LSBF 31 (64)\nΣ =\nn\n\u00140\n0\n\u0015\n,\n\u00141\n0\n\u0015\n,\n\u00140\n1\n\u0015\n,\n\u00141\n1\n\u0015\no\nΓ = Σ ∪ {□, 0, 1} Q = {add0, add1, H}F = {H}q0 = add0\nadd0start add1\nH\n\u00140\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 1/+ 1;\n\u00141\n0\n\u0015\n: 1/+ 1;\n\u00141\n1\n\u0015\n: 0/+ 1\n□: □/0\n\u00141\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 0/+ 1;\n\u00141\n1\n\u0015\n: 1/+ 1;\n\u00140\n0\n\u0015\n: 1/+ 1\n□: 1/0\nadd0\n. . .□ 1\n\u00141\n1\n\u0015 \u00141\n0\n\u0015 \u00141\n1\n\u0015\n□ □ . . .\nadd0\n\u00140\n1\n\u0015\u00141\n1\n\u0015\u00141\n0\n\u0015\u00141\n1\n\u0015\n□ − →M 1add0\n\u00141\n1\n\u0015\u00141\n0\n\u0015\u00141\n1\n\u0015\n□\n! Using e.g. vectors/tuples (and at the cost of additional “book-keeping” and\nintermediate steps), a 1TM can also simulate multiple tapes (with independent\nheads). (cf. memory bus width, bit rate)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing “ +” in LSBF 31 (65)\nΣ =\nn\n\u00140\n0\n\u0015\n,\n\u00141\n0\n\u0015\n,\n\u00140\n1\n\u0015\n,\n\u00141\n1\n\u0015\no\nΓ = Σ ∪ {□, 0, 1} Q = {add0, add1, H}F = {H}q0 = add0\nadd0start add1\nH\n\u00140\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 1/+ 1;\n\u00141\n0\n\u0015\n: 1/+ 1;\n\u00141\n1\n\u0015\n: 0/+ 1\n□: □/0\n\u00141\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 0/+ 1;\n\u00141\n1\n\u0015\n: 1/+ 1;\n\u00140\n0\n\u0015\n: 1/+ 1\n□: 1/0\nadd1\n. . .□ 1 0\n\u00141\n0\n\u0015 \u00141\n1\n\u0015\n□ □ . . .\nadd0\n\u00140\n1\n\u0015\u00141\n1\n\u0015\u00141\n0\n\u0015\u00141\n1\n\u0015\n□ − →M 1add0\n\u00141\n1\n\u0015\u00141\n0\n\u0015\u00141\n1\n\u0015\n□ − →M 10add1\n\u00141\n0\n\u0015\u00141\n1\n\u0015\n□\n! Using e.g. vectors/tuples (and at the cost of additional “book-keeping” and\nintermediate steps), a 1TM can also simulate multiple tapes (with independent\nheads). (cf. memory bus width, bit rate)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing “ +” in LSBF 31 (66)\nΣ =\nn\n\u00140\n0\n\u0015\n,\n\u00141\n0\n\u0015\n,\n\u00140\n1\n\u0015\n,\n\u00141\n1\n\u0015\no\nΓ = Σ ∪ {□, 0, 1} Q = {add0, add1, H}F = {H}q0 = add0\nadd0start add1\nH\n\u00140\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 1/+ 1;\n\u00141\n0\n\u0015\n: 1/+ 1;\n\u00141\n1\n\u0015\n: 0/+ 1\n□: □/0\n\u00141\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 0/+ 1;\n\u00141\n1\n\u0015\n: 1/+ 1;\n\u00140\n0\n\u0015\n: 1/+ 1\n□: 1/0\nadd1\n. . .□ 1 0 0\n\u00141\n1\n\u0015\n□ □ . . .\nadd0\n\u00140\n1\n\u0015\u00141\n1\n\u0015\u00141\n0\n\u0015\u00141\n1\n\u0015\n□ − →M 1add0\n\u00141\n1\n\u0015\u00141\n0\n\u0015\u00141\n1\n\u0015\n□ − →M 10add1\n\u00141\n0\n\u0015\u00141\n1\n\u0015\n□ − →M 100add1\n\u00141\n1\n\u0015\n□\n! Using e.g. vectors/tuples (and at the cost of additional “book-keeping” and\nintermediate steps), a 1TM can also simulate multiple tapes (with independent\nheads). (cf. memory bus width, bit rate)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing “ +” in LSBF 31 (67)\nΣ =\nn\n\u00140\n0\n\u0015\n,\n\u00141\n0\n\u0015\n,\n\u00140\n1\n\u0015\n,\n\u00141\n1\n\u0015\no\nΓ = Σ ∪ {□, 0, 1} Q = {add0, add1, H}F = {H}q0 = add0\nadd0start add1\nH\n\u00140\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 1/+ 1;\n\u00141\n0\n\u0015\n: 1/+ 1;\n\u00141\n1\n\u0015\n: 0/+ 1\n□: □/0\n\u00141\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 0/+ 1;\n\u00141\n1\n\u0015\n: 1/+ 1;\n\u00140\n0\n\u0015\n: 1/+ 1\n□: 1/0\nadd1\n. . .□ 1 0 0 1 □ □ . . .\nadd0\n\u00140\n1\n\u0015\u00141\n1\n\u0015\u00141\n0\n\u0015\u00141\n1\n\u0015\n□ − →∗\nM 100add1\n\u00141\n1\n\u0015\n□ − →M 1001add1□\n! Using e.g. vectors/tuples (and at the cost of additional “book-keeping” and\nintermediate steps), a 1TM can also simulate multiple tapes (with independent\nheads). (cf. memory bus width, bit rate)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM for computing “ +” in LSBF 31 (68)\nΣ =\nn\n\u00140\n0\n\u0015\n,\n\u00141\n0\n\u0015\n,\n\u00140\n1\n\u0015\n,\n\u00141\n1\n\u0015\no\nΓ = Σ ∪ {□, 0, 1} Q = {add0, add1, H}F = {H}q0 = add0\nadd0start add1\nH\n\u00140\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 1/+ 1;\n\u00141\n0\n\u0015\n: 1/+ 1;\n\u00141\n1\n\u0015\n: 0/+ 1\n□: □/0\n\u00141\n0\n\u0015\n: 0/+ 1;\n\u00140\n1\n\u0015\n: 0/+ 1;\n\u00141\n1\n\u0015\n: 1/+ 1;\n\u00140\n0\n\u0015\n: 1/+ 1\n□: 1/0\nH\n. . .□ 1 0 0 1 1 □ . . .\nadd0\n\u00140\n1\n\u0015\u00141\n1\n\u0015\u00141\n0\n\u0015\u00141\n1\n\u0015\n□ − →∗\nM 1001add1□ − →M 1001H1\n! Using e.g. vectors/tuples (and at the cost of additional “book-keeping” and\nintermediate steps), a 1TM can also simulate multiple tapes (with independent\nheads). (cf. memory bus width, bit rate)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Multiple tapes 32 (69)\nq1\n. . .□ a b a b b □ . . .\nq2\n. . .□ b b b a b □ . . .\nqb,a\n. . .□\n\u0014a\nb\n\u0015\u0014q1b\nb\n\u0015\u0014a\nb\n\u0015\u0014 b\nq2a\n\u0015\u0014b\nb\n\u0015\n□ . . .\n▷ Multiple tapes and multiple 1TMs can be simulated by means of a\nvector/tuple alphabet, and e.g. simulating one step of each tape/1TM using\nround-robin scheduling (at a quadratic runtime increase) .\nThus, a 1TM can simulate a Petri net (e.g. one tape per counter) .\n▷ The central point is that, as Γ and Q can be any finite set, a 1TM can have\narbitrary finite “bit width”; see also the linear speedup theorem.\n▷ Further, a TM M can “call” a TM M′ e.g. by simulating M′ on an additional\ntape resp. an extended alphabet and waiting for M′ to halt in a final state.\n(Recall that by definition a TM cannot continue from a final state.)No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "“Simulating” machine code (fixed bit width) 33 (70)\nΣ = {00, 01, 10, 11} Γ = Σ ∪ {□, ADD, MUL, AND}\nQ = {bgn, add, addx, mul, mulx, and, andx, H | x ∈ Z 4} F = {H} q0 = bgn\nbgn\nadd\nmul\nand\nmul0\nmul1\nmul2\nmul3\nHMUL:□/+ 1ADD:□/+ 1\nAND:□/+ 1\n00:□/+ 1\n01:□/+ 1\n10:□/+ 1\n11:□/+ 1\n00: 00/+ 1, . . . ,11: 10/+ 1\nbgn\n. . .□ MUL 10 11 □ . . .\n▷ Here, we “emulate” a part of a very simple 2-bit ALU (this time MSBF) .\n! The states Q can be any finite set, e.g. any combination of values stored in\nthe program counter, registers, cache, RAM.\n! The tape can represent swap space, heap, stack, any kind of storage device\n(hard disks, floppy disks, tape drives, . . . ) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "“Simulating” machine code (fixed bit width) 33 (71)\nΣ = {00, 01, 10, 11} Γ = Σ ∪ {□, ADD, MUL, AND}\nQ = {bgn, add, addx, mul, mulx, and, andx, H | x ∈ Z 4} F = {H} q0 = bgn\nbgn\nadd\nmul\nand\nmul0\nmul1\nmul2\nmul3\nHMUL:□/+ 1ADD:□/+ 1\nAND:□/+ 1\n00:□/+ 1\n01:□/+ 1\n10:□/+ 1\n11:□/+ 1\n00: 00/+ 1, . . . ,11: 10/+ 1\nmul\n. . .□ □ 10 11 □ . . .\n▷ Here, we “emulate” a part of a very simple 2-bit ALU (this time MSBF) .\n! The states Q can be any finite set, e.g. any combination of values stored in\nthe program counter, registers, cache, RAM.\n! The tape can represent swap space, heap, stack, any kind of storage device\n(hard disks, floppy disks, tape drives, . . . ) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "“Simulating” machine code (fixed bit width) 33 (72)\nΣ = {00, 01, 10, 11} Γ = Σ ∪ {□, ADD, MUL, AND}\nQ = {bgn, add, addx, mul, mulx, and, andx, H | x ∈ Z 4} F = {H} q0 = bgn\nbgn\nadd\nmul\nand\nmul0\nmul1\nmul2\nmul3\nHMUL:□/+ 1ADD:□/+ 1\nAND:□/+ 1\n00:□/+ 1\n01:□/+ 1\n10:□/+ 1\n11:□/+ 1\n00: 00/+ 1, . . . ,11: 10/+ 1\nmul2\n. . .□ □ □ 11 □ . . .\n▷ Here, we “emulate” a part of a very simple 2-bit ALU (this time MSBF) .\n! The states Q can be any finite set, e.g. any combination of values stored in\nthe program counter, registers, cache, RAM.\n! The tape can represent swap space, heap, stack, any kind of storage device\n(hard disks, floppy disks, tape drives, . . . ) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "“Simulating” machine code (fixed bit width) 33 (73)\nΣ = {00, 01, 10, 11} Γ = Σ ∪ {□, ADD, MUL, AND}\nQ = {bgn, add, addx, mul, mulx, and, andx, H | x ∈ Z 4} F = {H} q0 = bgn\nbgn\nadd\nmul\nand\nmul0\nmul1\nmul2\nmul3\nHMUL:□/+ 1ADD:□/+ 1\nAND:□/+ 1\n00:□/+ 1\n01:□/+ 1\n10:□/+ 1\n11:□/+ 1\n00: 00/+ 1, . . . ,11: 10/+ 1\nH\n. . .□ □ □ 10 □ . . .\n▷ Here, we “emulate” a part of a very simple 2-bit ALU (this time MSBF) .\n! The states Q can be any finite set, e.g. any combination of values stored in\nthe program counter, registers, cache, RAM.\n! The tape can represent swap space, heap, stack, any kind of storage device\n(hard disks, floppy disks, tape drives, . . . ) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "“Simulating” machine code (fixed bit width) 33 (74)\nΣ = {00, 01, 10, 11} Γ = Σ ∪ {□, ADD, MUL, AND}\nQ = {bgn, add, addx, mul, mulx, and, andx, H | x ∈ Z 4} F = {H} q0 = bgn\nbgn\nadd\nmul\nand\nmul0\nmul1\nmul2\nmul3\nHMUL:□/+ 1ADD:□/+ 1\nAND:□/+ 1\n00:□/+ 1\n01:□/+ 1\n10:□/+ 1\n11:□/+ 1\n00: 00/+ 1, . . . ,11: 10/+ 1\nH\n. . .□ □ □ 10 □ . . .\n! For fixed bit width we can encode any standard CPU in δ.\n! I.e. an 1TM can emulate or even simulate any standard computer\nand interpret any program in machine code (very slowly).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "“Simulating” machine code (fixed bit width) 33 (75)\nΣ = {00, 01, 10, 11} Γ = Σ ∪ {□, ADD, MUL, AND}\nQ = {bgn, add, addx, mul, mulx, and, andx, H | x ∈ Z 4} F = {H} q0 = bgn\nbgn\nadd\nmul\nand\nmul0\nmul1\nmul2\nmul3\nHMUL:□/+ 1ADD:□/+ 1\nAND:□/+ 1\n00:□/+ 1\n01:□/+ 1\n10:□/+ 1\n11:□/+ 1\n00: 00/+ 1, . . . ,11: 10/+ 1\nH\n. . .□ □ □ 10 □ . . .\n▷ Informally, as a 1TM can run machine code for a given CPU, it can also\n▷ run C/C++/Java (Jave byte code)/Python code.\n▷ run numerical algorithms using floats.\n▷ implement arithmetics on N , Z , Q using big integers.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "“Simulating” machine code (fixed bit width) 33 (76)\nΣ = {00, 01, 10, 11} Γ = Σ ∪ {□, ADD, MUL, AND}\nQ = {bgn, add, addx, mul, mulx, and, andx, H | x ∈ Z 4} F = {H} q0 = bgn\nbgn\nadd\nmul\nand\nmul0\nmul1\nmul2\nmul3\nHMUL:□/+ 1ADD:□/+ 1\nAND:□/+ 1\n00:□/+ 1\n01:□/+ 1\n10:□/+ 1\n11:□/+ 1\n00: 00/+ 1, . . . ,11: 10/+ 1\nH\n. . .□ □ □ 10 □ . . .\n▷ The behavior of a 1TM can be defined precisely – in contrast to CPUs (cf.\nSMT) and programming languages (depends on e.g. compiler (version)) .\n! 1TMs serve as formal model of a computer (with unbounded memory).\nIf something is impossible for an 1TM, it is also impossible for any (standard)\ncomputer.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "“Simulating” machine code (fixed bit width) 33 (77)\nΣ = {00, 01, 10, 11} Γ = Σ ∪ {□, ADD, MUL, AND}\nQ = {bgn, add, addx, mul, mulx, and, andx, H | x ∈ Z 4} F = {H} q0 = bgn\nbgn\nadd\nmul\nand\nmul0\nmul1\nmul2\nmul3\nHMUL:□/+ 1ADD:□/+ 1\nAND:□/+ 1\n00:□/+ 1\n01:□/+ 1\n10:□/+ 1\n11:□/+ 1\n00: 00/+ 1, . . . ,11: 10/+ 1\nH\n. . .□ □ □ 10 □ . . .\n▷ A deteterministic 1TM can also (at the cost of an increased runtime)\n▷ simulate nondeterministic multi-tape TMs (including Petri nets),\n▷ simulate CPUs with SMT/multiple cores, or a parallel computer,\n▷ evaluate any neural network,\n▷ simulate a quantum computer.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Simulating a 1TM 34 (78)\ndef bfs(Q, Σ, Γ, δ, q0, □, F, w):\ncur = {(ε, q0, w)}\nwhile len(cur) > 0:\nnxt = {}\nfor (α, p, β) in cur:\nif β == ε: β = □\nfor ((q, a), (r, b, d)) in δ:\nif q == p and a == β[0]:\nif d == 0:\nnxt.add((α, r, bβ[1:]))\nif d == +1:\nnxt.add((αb, r, β[1:]))\nif d == −1:\nif α == ε: α = □\nnxt.add((α[:-1], r, α[-1]bβ[1:]))\ncur = nxt\n▷ We can simulate all possible runs of a (nondeterministic) 1TM using BFS:\nSimply compute all k-step successors q0w − →k\nM for increasing k.\n▷ In general, the number of k-step successors grows exponentially, e.g.\nδ = {(q0, x, q0, y,+1) | x ∈ {0, 1, □}, y∈ {0, 1}}\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Simulating a 1TM 34 (79)\ndef bfs(Q, Σ, Γ, δ, q0, □, F, w):\ncur = {(ε, q0, w)}\nwhile len(cur) > 0:\nnxt = {}\nfor (α, p, β) in cur:\nif β == ε: β = □\nfor ((q, a), (r, b, d)) in δ:\nif q == p and a == β[0]:\nif d == 0:\nnxt.add((α, r, bβ[1:]))\nif d == +1:\nnxt.add((αb, r, β[1:]))\nif d == −1:\nif α == ε: α = □\nnxt.add((α[:-1], r, α[-1]bβ[1:]))\ncur = nxt\n▷ If the 1TM is deterministic (D1TM), then the BFS reduces to following a\npath.\nI.e. the program only requires a single while-loop and two lists (or one cyclic\nlist) for representing the tape content.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Simulating a 1TM 34 (80)\ndef bfs(Q, Σ, Γ, δ, q0, □, F, w):\ncur = {(ε, q0, w)}\nwhile len(cur) > 0:\nnxt = {}\nfor (α, p, β) in cur:\nif β == ε: β = □\nfor ((q, a), (r, b, d)) in δ:\nif q == p and a == β[0]:\nif d == 0:\nnxt.add((α, r, bβ[1:]))\nif d == +1:\nnxt.add((αb, r, β[1:]))\nif d == −1:\nif α == ε: α = □\nnxt.add((α[:-1], r, α[-1]bβ[1:]))\ncur = nxt\n! As every program/algorithm (given in any known programming language,\nencoded in UTF8) can be “compiled” to a deterministic 1TM, this means\nthat every program can be rewritten as a deterministic program consisting\nonly of a single while-loop and a single list.\n(In the end, this is what a CPU is doing.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Simulating a 1TM 34 (81)\ndef bfs(Q, Σ, Γ, δ, q0, □, F, w):\ncur = {(ε, q0, w)}\nwhile len(cur) > 0:\nnxt = {}\nfor (α, p, β) in cur:\nif β == ε: β = □\nfor ((q, a), (r, b, d)) in δ:\nif q == p and a == β[0]:\nif d == 0:\nnxt.add((α, r, bβ[1:]))\nif d == +1:\nnxt.add((αb, r, β[1:]))\nif d == −1:\nif α == ε: α = □\nnxt.add((α[:-1], r, α[-1]bβ[1:]))\ncur = nxt\n▷ In particular, we can compile above program into a deterministic 1TM:\nT There is a universal D1TM (UTM) that takes the binary encoding of\nM = (Q, Σ, Γ, δ, q0, □, F) and an input w, and then simulates M on w.\n(An UTM is a very simple “CPU” that can even be simulated using Excel.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Simulating a 1TM 34 (82)\ndef bfs(Q, Σ, Γ, δ, q0, □, F, w):\ncur = {(ε, q0, w)}\nwhile len(cur) > 0:\nnxt = {}\nfor (α, p, β) in cur:\nif β == ε: β = □\nfor ((q, a), (r, b, d)) in δ:\nif q == p and a == β[0]:\nif d == 0:\nnxt.add((α, r, bβ[1:]))\nif d == +1:\nnxt.add((αb, r, β[1:]))\nif d == −1:\nif α == ε: α = □\nnxt.add((α[:-1], r, α[-1]bβ[1:]))\ncur = nxt\nT Thus every (nondeterministic) 1TM can be transformed into a D1TM\n▷ Nondeterminism will make a difference later when we consider the length of a\nrun (run time) resp. the space used by a run.\nIt is also useful for abstracting from user input.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "3 Turing machines, computable functions, semidecidable languages\nTuring machines\nComputable and non-computable functions\nSemidecidable/recognizable/recursively enumerable languages\nHalting problem, Rice’s theorem, deciding properties of computable functions\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Computable functions: preliminaries 36 (84)\n▷ We will use TMs to formally define “computable”.\nD A partial function f : A ,→ B is a binary relation f ⊆ A × B so that for all\na ∈ A we have |{b ∈ B | (a, b) ∈ f}| ≤1 (at most one image) .\n▷ i.e. f(a) might be undefined for some a ∈ A (e.g. f : R ,→ R , x7→ 1\nx ).\n▷ Every (total) function f : A → B is of course also a partial function.\nD lsbf(n) (msbf(n)) denotes the least (most) significant bit first representation\nwithout trailing (leading) zeros of n ∈ N .\n▷ e.g. 42 = 21 + 23 + 25 so lsbf(42) = 010101 and msbf(42) = 101010.\n! wlog we may always assume Σ = {0, 1}:\n▷ A simple approach is to fix some k ∈ N with |Σ| ≤2k and to injectively map\nΣ into {0, 1}k; then always first read k bits to decode the original symbol.\n▷ E.g. for Σ = {a, b, c} use a 7→ 00, b 7→ 10, c 7→ 01 (with 11 unsused).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Computable functions over words 37 (85)\nD A partial function f : Σ∗ ,→ Σ∗ is computable if there exists a\ndeterministic 1TM M = (Q, Σ, Γ, δ, q0, □, {qf }) so that for all w ∈ Σ∗\n▷ If f(w) = w′ is defined, then q0w − →∗\nM qf w′,\ni.e. M terminates on input w in a final state with w′ written as output on the\ntape.\n▷ If f(w) is undefined, then M runs forever or “crashes” in some state q ̸∈ F.\n! wlog we can always assume F = {qf }:\n▷ Simply add a new state qf and add transitions from each old final state to the\nnew final state qf .\n! wlog every maximal run either reaches qf (and thus is finite) or it is infinite.\n▷ If (q, a)δ = ∅, add ((q, a), (q, a,0)) to δ.\n▷ Hence we can always assume:\nIf f(x) is defined, then the call to M returns f(x); otherwise the call will not\nreturn/terminate.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Computable functions over words 37 (86)\nD A partial function f : Σ∗ ,→ Σ∗ is computable if there exists a\ndeterministic 1TM M = (Q, Σ, Γ, δ, q0, □, {qf }) so that for all w ∈ Σ∗\n▷ If f(w) = w′ is defined, then q0w − →∗\nM qf w′,\ni.e. M terminates on input w in a final state with w′ written as output on the\ntape.\n▷ If f(w) is undefined, then M runs forever or “crashes” in some state q ̸∈ F.\n! More precisely, above definition is the definition of Turing computable.\nUsing the Church-Turing conjecture, we identify “(intuitively) computable”\nwith Turing computable, and thus simply write “computable”.\n! All data on a computer is always a finite (binary) string, so computers only\ncompute (partial) functions on (binary) strings.\nIn case of programs, partial functions typically arise from programs that do\nnot terminate or fail silently/crash for a given input x; but error\nmessages/(caught) exception can be considered the image of x.",
      "compute (partial) functions on (binary) strings.\nIn case of programs, partial functions typically arise from programs that do\nnot terminate or fail silently/crash for a given input x; but error\nmessages/(caught) exception can be considered the image of x.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Computable functions over N 38(87)\nD A partial function f : N k ,→ N is computable if there exists a\ndeterministic 1TM M = (Q, Σ, Γ, δ, q0, □, {qf }) with Σ = {0, 1, #} so that\nfor all n1, . . . , nk ∈ N\n▷ If f(n1, . . . , nk) = m is defined, then\nq0lsbf(n1)# . . .#lsbf(nk) − →∗\nM qf lsbf(m)\n(# only serves as seperator; msbf can be used alternatively.)\n▷ If f(n1, . . . , nk) is undefined, then M runs forever or halts in a state q ̸∈ F.\n! This definition is only for convenience:\n▷ Using e.g. UTF8 encoding, we can encode “ (n1, . . . , nk)” as a binary string.\nThus every f : N k ,→ N defines a function F : {0, 1}∗ ,→ {0, 1}∗.\n(For an invalid/malformed encoding either return a default error value or leaf F\nundefined.)\n▷ Every f : {0, 1}∗ ,→ {0, 1}∗ defines a function F : N ,→ N .\n(Simply encode w ∈ {0, 1}∗ as lsbf−1(w1).)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cantor, noncomputable functions, (special) halting problem 39 (88)\n! There is a computable total surjective “decoding” that maps w ∈ {0, 1}∗ to\nthe description Mw = (Q, Σ, Γ, δ, q0, □, F) of a D1TM.\n▷ E.g. read w as (potentially malformed) UTF8 encoding of the description of a\nD1TM Mw (cf. source code of a program/algorithm) .\nIf w is malformed (i.e. does not describe a D1TM) , let Mw default the D1TM\nthat rejects every input immediately.\nF Not every function f : N → N is computable:\n▷ {0, 1}∗ → N , w7→ lsfb−1(w1) is a bijection, so |N | = |{0, 1}∗|.\n▷ By Cantor’s diagonalization argument: |N | < |{0, 1}N |\n▷ Trivially: |{0, 1}N | ≤ |N N |.\n▷ I.e. there are only countably many 1TMs, and thus computable functions,\nbut uncountably many characteristic functions (subsets, languages)\nf : {0, 1}∗ → {0, 1}, and thus also functions f : {0, 1}∗ → {0, 1}∗.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cantor, noncomputable functions, (special) halting problem 39 (89)\n! There is a computable total surjective “decoding” that maps w ∈ {0, 1}∗ to\nthe description Mw = (Q, Σ, Γ, δ, q0, □, F) of a D1TM.\n▷ Cantor’s diagonalization argument can also be adapted to obtain an example\nof an “noncomputable” function.\n▷ Reminder for |N | < |2N |:\n▷ Assume f : N\nbij\n− →2N (i.e. assume |N | = |2N |).\n▷ Define D := {k ∈ N | k ̸∈ f(k)} ⊆N .\n(If f is a function, then D is a well-defined set.)\n▷ Let d := f−1(D).\n(As f is bijective/surjective, D has to have a pre-image d.).\n▷ We obtain the contradiction (cf. Russell’s paradox) :\nd ∈ D iff d ̸∈ f(d) = D\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cantor, noncomputable functions, (special) halting problem 39 (90)\n! There is a computable total surjective “decoding” that maps w ∈ {0, 1}∗ to\nthe description Mw = (Q, Σ, Γ, δ, q0, □, F) of a D1TM.\nT Special halting problem: The total function h: {0, 1}∗ → {0, 1}\nh(w) :=\n(\n1 if Mw halts on input w\n0 else\nis not computable. (“halt”: maximal run is finite)\n▷ Assume M is a DTM that computes h.\n▷ Then the following is also DTM M′:\non input w run M to obtain y := h(w); while y = 1 pass;\n▷ Thus there is some w′ ∈ {0, 1}∗ s.t. M′ = Mw′.\n▷ M′ = Mw′ halts on w′ iff h(w′) = 0 iff Mw′ = M′ does not halt on w′\n! h is a (total) function for which no (finite) TM (algorithm, program) exists.\nBut the partial function h′ that is defined only on h−1(1) is computable:\nSimply run Mw on w; then return 1 (if the simulation terminates) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "3 Turing machines, computable functions, semidecidable languages\nTuring machines\nComputable and non-computable functions\nSemidecidable/recognizable/recursively enumerable languages\nHalting problem, Rice’s theorem, deciding properties of computable functions\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Decidable and semidecidable languages/sets 41 (92)\nD For a language (set) L ⊆ Σ∗ define\n▷ the total function (predicate) χL : Σ∗ → {0, 1} with χ−1\nL (1) = L.\nχL is called the characteristic/indicator function of L.\n▷ the partial function eχL : Σ∗ ,→ {1} with eχ−1\nL (1) = L\n! χL is total with χ(L) = 0 while eχL is undefined on L = Σ∗ \\ L.\nD A language L ⊆ Σ∗ is decidable if χL is computable.\nA language L ⊆ Σ∗ is semidecidable if eχL is computable.\n(also: “the word problem for language L is (semi)decidable if . . . ”)\nL L is decidable iff both L and L are semidecidable.\n▷ Obvious if L is decidable.\n▷ Let the D1TMs ML and ML compute eχL and eχL, respectively.\nInterleave the simulations of both ML and ML on input w.\n(Two threads on one core.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Decidable and semidecidable languages/sets 41 (93)\nD For a language (set) L ⊆ Σ∗ define\n▷ the total function (predicate) χL : Σ∗ → {0, 1} with χ−1\nL (1) = L.\nχL is called the characteristic/indicator function of L.\n▷ the partial function eχL : Σ∗ ,→ {1} with eχ−1\nL (1) = L\n! χL is total with χ(L) = 0 while eχL is undefined on L = Σ∗ \\ L.\nD A language L ⊆ Σ∗ is decidable if χL is computable.\nA language L ⊆ Σ∗ is semidecidable if eχL is computable.\n(also: “the word problem for language L is (semi)decidable if . . . ”)\nT The special halting problem as language\nLh = {w ∈ {0, 1}∗ | Mw halts on input w}\nis semidecidable, but not decidable (short: undecidable).\nC Lh = {w ∈ {0, 1}∗ | Mw does not halt on input w} is not semidecidable.\n! Computing the (total) characteristic function of a set resp. deciding a (unary)",
      "T The special halting problem as language\nLh = {w ∈ {0, 1}∗ | Mw halts on input w}\nis semidecidable, but not decidable (short: undecidable).\nC Lh = {w ∈ {0, 1}∗ | Mw does not halt on input w} is not semidecidable.\n! Computing the (total) characteristic function of a set resp. deciding a (unary)\npredicate resp. computing a single bit of information can be impossible.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Decidable and semidecidable languages/sets 41 (94)\nD For a language (set) L ⊆ Σ∗ define\n▷ the total function (predicate) χL : Σ∗ → {0, 1} with χ−1\nL (1) = L.\nχL is called the characteristic/indicator function of L.\n▷ the partial function eχL : Σ∗ ,→ {1} with eχ−1\nL (1) = L\n! χL is total with χ(L) = 0 while eχL is undefined on L = Σ∗ \\ L.\nD A language L ⊆ Σ∗ is decidable if χL is computable.\nA language L ⊆ Σ∗ is semidecidable if eχL is computable.\n(also: “the word problem for language L is (semi)decidable if . . . ”)\n▷ A language L can be understood as a unary predicate:\n▷ Given an encoding w ∈ {0, 1}∗ the object represented by w has a property iff\nw ∈ L, e.g.: Mw halts on ε iff w ∈ LH,ε.\n▷ E.g. in the simplest case, neural networks are used to “learn” (approximate) a\npredicate, i.e. a language (an n-bit output corresponds to n predicates).\n▷ If L is undecidable, any 1TM that tries to decide L has to give the wrong",
      "w ∈ L, e.g.: Mw halts on ε iff w ∈ LH,ε.\n▷ E.g. in the simplest case, neural networks are used to “learn” (approximate) a\npredicate, i.e. a language (an n-bit output corresponds to n predicates).\n▷ If L is undecidable, any 1TM that tries to decide L has to give the wrong\nanswer sometimes – and thus this has to be true also for neural networks.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Recognizable languages 42 (95)\nD (Reminder) The language accepted by the 1TM M = (Q, Σ, Γ, δ, q0, □, F) is\nL(M) := {w ∈ Σ∗ | q0w − →∗\nM αqf β for some α, β∈ Γ∗, qf ∈ F}\nA language L ⊆ Σ∗ is recognizable if\nthere is some 1TM M that accepts L, i.e. with L(M) = L.\n! “recognizable” allows M to be nondeterministic:\nM accepts a word w ∈ Σ∗ iff there is at least one run on input M that\neventually terminates in a final/accepting state.\nM rejects a word w ∈ Σ∗ iff every run on input M either is infinite or\neventually terminates in a non-final/rejecting state.\n(Recall: By definition of 1TM, δ is undefined for all final states.)\nBut as we already know that BFS can be used to turn every 1TM into a\nD1TM, it immediately follows that “recognizable” and “semidecidable”\ncoincide.\nFurther wlog we may assume that a D1TM halts iff it accepts.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Recognizable languages 42 (96)\nD (Reminder) The language accepted by the 1TM M = (Q, Σ, Γ, δ, q0, □, F) is\nL(M) := {w ∈ Σ∗ | q0w − →∗\nM αqf β for some α, β∈ Γ∗, qf ∈ F}\nA language L ⊆ Σ∗ is recognizable if\nthere is some 1TM M that accepts L, i.e. with L(M) = L.\nF L is semidecidable iff L is recognizable.\n▷ Assume L is semidecidable, i.e. eχL is computed by some 1TM M.\nThen by definition L(M) = L as M does not terminate on L.\n▷ Assume L is recognizable, i.e. L = L(M) for some 1TM M.\nAs M might be nondeterministic, use BFS to construct all reachable\nconfiguration for the given input w; only return 1 if some final state is\nreached, else return 0 or do not terminate.\n▷ If L is semidecidable, we can write a program/define a DTM that enumerates\nexactly the elements of L.\n(For every w ∈ L there is shortest accepting run; use BFS to simulate all runs on Σ∗ for\nincreasing time; cf. dovetailing/BFS on Σ∗)",
      "reached, else return 0 or do not terminate.\n▷ If L is semidecidable, we can write a program/define a DTM that enumerates\nexactly the elements of L.\n(For every w ∈ L there is shortest accepting run; use BFS to simulate all runs on Σ∗ for\nincreasing time; cf. dovetailing/BFS on Σ∗)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Semidecidable and computably/recursively enumerable sets 43 (97)\nL L ⊆ Σ∗ with L ̸= ∅ is semidecidable iff\nthere is a computable enumeration f : N →Σ∗ so that f(N ) = L.\nftable = [] # f only has to be surjective\nfor k = 1, 2, . . .in N : # max depth of BFS and time bound\nfor l = 0, 1, . . . , k: # in increasing order (depth of BFS)\nfor w ∈ Σl: # in lexicographic order (front of BFS)\nif simulate_for_k_steps(χL, w, k) == 1:\nftable.append(w) # happens infinitely often for every w ∈ L\nif len(ftable) == n: return w\nfor k = 1, 2, . . .in N :\nwk = f(k) # run M on k; has to terminate as f is total\nif w == wk: return 1\nD A semidecidable set (incl. ∅) is also called computably/recursively\nenumerable, i.e. we can construct/reach every w ∈ L in a finite time.\n(decidable: we can also construct/reach each w ∈ L in finite time.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Semidecidable, recognizable, enumerable, computable, . . . 44 (98)\n! So for L ⊆ Σ∗ we have:\nL is recognizable, i.e. accepted by some TM.\niff L is semidecidable.\niff L is computably/recursively enumerable.\niff The word problem for L is semidecidable.\niff There is a computable total function f : N → Σ∗ with f(N ) = L (if L ̸= ∅).\niff There is a computable partial function eχL : Σ∗ ,→ {1} with L = eχ−1\nL (1).\nand\nL is decidable\niff χL : Σ∗ → {0, 1} with χ−1\nL (1) = L is computable.\niff both L and L = Σ∗ \\ L are semidecidable/rec. enumerable/recognizable.\nand\nL is undecidable\niff at least L or L is not semidecidable/rec. enumerable/recognizable.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Languages vs. functions, semidecidable vs. computable 45 (99)\n▷ By definition, “semidecidable” is a special case of “computable”.\n! If f : Σ∗ ,→ Σ∗ is computable, then also f$ : Σ∗ ,→ Σ∗{$}, w7→ f(w)$ is\ncomputable where $ ̸∈ Σ is a “fresh” symbol for “end of word”.\n▷ “$” can also be encoded in binary, e.g.: 0 7→ 00, 1 7→ 01, $ 7→ 10\n▷ “$” is not required if we have a computable bound on |f(w)|.\nL Let f : Σ∗ ,→ Σ∗{$} and consider the language (“u prefix of f(w)?”)\nL = {u#w | w ∈ Σ∗, u, v∈ (Σ ∪ {$})∗, f(w) = uv}\nL is semidecidable iff f is computable.\n▷ See the next slide: treat Σ∗{$} as an infinite tree.\nGiven some input w we use eχL to guide a DFS from the root ε towards f(w).\nAs eχL does not terminate in general, we need to simulate eχL(ua) for all\na ∈ Σ ∪ {$} in parallel again.\n! In particular, if L is decidable, i.e. χL computable, then we can recover f(w)\nfrom L using at most |Σ ∪ {$}| · |f(w)| calls to χL.",
      "As eχL does not terminate in general, we need to simulate eχL(ua) for all\na ∈ Σ ∪ {$} in parallel again.\n! In particular, if L is decidable, i.e. χL computable, then we can recover f(w)\nfrom L using at most |Σ ∪ {$}| · |f(w)| calls to χL.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Semidecidability and computability 46 (100)\nL f : Σ∗ ,→ Σ∗{$} is computable ($ ̸∈ Σ) iff\nL = {u#w | w ∈ Σ∗, u, v∈ (Σ ∪ {$})∗, f(w) = uv} is semidecidable\n▷ Assume f : Σ∗ ,→ Σ∗{$} is computable:\nw′ = f(w) # direct call, might not terminate\nif w′.startswith(u):\nreturn 1\nwhile True:\npass\n▷ Assume L = {u#w | w ∈ Σ∗, u, v∈ (Σ ∪ {$})∗, f(w) = uv} is semidecidable,\ni.e. eχL is computed by some ML; use ML to as “oracle” to guide the search:\nu = ε # DFS path through Σ∗{$} from ε towards f(w)\nfor k = 1, 2, 3, . . .in N :\nfor a ∈ Σ ∪ {$}:\nif simulate_for_k_steps(ML, ua#w, k) == 1:\nif a == $:\nreturn u\nu = ua\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Deciding satisfiability vs. computing a satisfying assignment 47 (101)\n! If L is even decidable, then we can simply call ML and wait for the result,\nand thus remove the outer loop.\n▷ For instance, consider the satisfiability of propositional formulas ( wlog as\nclause set), e.g.\nF = {{¬p, q,¬r, s}, {¬q, ¬r, s}, {r}, {¬p, ¬s}, {¬p, r}}\nGiven a propositional formula F with variables x1, . . . , xn\n▷ the computation problem is: compute a satisfying assignment\nβ : {x1, . . . , xn} → {0, 1} of F, if there is one, else return false.\n▷ the decision problem is: decide (true / false) if F is satisfiable, i.e. has at least\none satisfying assignment.\nChecking all 2n (minimal) assignments allows to solve both problems. Note:\n▷ Any 1TM/algorithm that solves the computation problem also solves the\ndecision problem.\n▷ An 1TM/algorithm ML that solves the decision problem can be used to guide\nDPLL using at most 2n calls.",
      "one satisfying assignment.\nChecking all 2n (minimal) assignments allows to solve both problems. Note:\n▷ Any 1TM/algorithm that solves the computation problem also solves the\ndecision problem.\n▷ An 1TM/algorithm ML that solves the decision problem can be used to guide\nDPLL using at most 2n calls.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example DPLL basic version 48 (102)\n{{¬p, q,¬r, s}, {¬q, ¬r, s}, {r}, {¬p, ¬s}, {¬p, r}}\n{{q, ¬r, s}, {¬q, ¬r, s}, {r}, {¬s}} {{¬q, ¬r, s}, {r}}\n{{¬r, s}, {r}, {¬s}} {{¬r, s}, {r}, {¬s}}\n{{s}, {¬s}} {{}, {¬s}}\n{{}} {{}}\n{{s}, {¬s}} {{}, {¬s}}\n{{}} {{}}\n{{¬r, s}, {r}}\n{{s}}\n{}\np := true p := false\nq := true\nq := false\nr := true r := false\ns := true s := false\nr := true r := false\ns := true s := false\nq := true\nr := true\ns := true\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Decision vs. computation and approximation 49 (103)\n▷ Given a polynomial like\nX5 − X + 1\nwe can decide that it has a real root x0 ∈ [−1.1, −1.0] using e.g. Sturm’s\ntheorem:\nx\ny\nbut in general we can only approximate the (e.g.) binary representation of x0\ne.g. by means of the reduction to a decision problem (binary search/bisection\nmethod) (Newton’s method might still be the better approach) .\n! There exists (at least) one real number x s.t. there is no program/D1TM that\non input i ∈ N 0 outputs the first i digits of the binary representation of x.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "3 Turing machines, computable functions, semidecidable languages\nTuring machines\nComputable and non-computable functions\nSemidecidable/recognizable/recursively enumerable languages\nHalting problem, Rice’s theorem, deciding properties of computable functions\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem in general and Rice’s theorem 51 (105)\n▷ As the special halting problem is undecidable, any non-trivial question\nregarding computable functions is also undecidable:\n! As before let {0, 1}∗ ∋ w 7→ Mw be a computable total surjective “decoding”\nso that {Mw | w ∈ {0, 1}∗} is the set of all (descriptions of) D1TM.\nC Halting problem:\nLH := {w#x | w, x∈ {0, 1}∗, Mw halts on input x}\nis semideciable, but not decidable (i.e. LH is not semideciable).\n▷ “halts”: the maximal run q0w − →∗\nM is finite\n▷ semidecidable: simply simulate Mw on x.\n▷ undecidable: otherwise Lh would be decidable as: w ∈ Lh iff w#w ∈ LH\n! The argument used in the proof is called a (computable) reduction:\n▷ ρ(w) := w#w is a computable total function so that w ∈ Lh iff ρ(w) ∈ LH.\n▷ i.e. by deciding f(w) ∈ LH we can decide w ∈ Lh and thus “reduce” the\nspecial halting problem to the (general) halting problem.",
      "! The argument used in the proof is called a (computable) reduction:\n▷ ρ(w) := w#w is a computable total function so that w ∈ Lh iff ρ(w) ∈ LH.\n▷ i.e. by deciding f(w) ∈ LH we can decide w ∈ Lh and thus “reduce” the\nspecial halting problem to the (general) halting problem.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem in general and Rice’s theorem 51 (106)\n▷ As the special halting problem is undecidable, any non-trivial question\nregarding computable functions is also undecidable:\nC Halting problem on empty tape\nLH,ε := {w | w ∈ {0, 1}∗, Mw halts on input ε}\nis semideciable, but not decidable (i.e. LH,ε is not semideciable).\n▷ semidecidable: simply simulate Mw on ε.\n▷ undecidable: otherwise Lh would be decidable.\nWe define again a computable reduction ρ.\nGiven w construct from Mw the D1TM M′\nw as follows:\n▷ M′\nw initially replaces any input by w.\n▷ it then executes the original Mw on w.\nLet ρ(w) := w′ be the binary encoding of M′\nw.\nThen: w ∈ Lh iff ρ(w) = w′ ∈ LH,ε\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem in general and Rice’s theorem 51 (107)\n▷ As the special halting problem is undecidable, any non-trivial question\nregarding computable functions is also undecidable:\nC Halting problem for some input\nLH,? := {w | w ∈ {0, 1}∗, there ex. x ∈ {0, 1}∗ : Mw halts on input x}\nis semideciable, but not decidable (i.e. LH,? is not semideciable).\n▷ semidecidable: enumerate all x on which Mw halts (dove-tailing/BFS)\n▷ undecidable: just as for halting on empty tape\nGiven w construct from Mw the D1TM M′\nw as follows:\n▷ M′\nw initially replaces any input by w.\n▷ it then executes the original Mw on w.\nLet ρ(w) := w′ be the binary encoding of M′\nw.\nThen: w ∈ Lh iff ρ(w) = w′ ∈ LH,?\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem in general and Rice’s theorem 51 (108)\n▷ As the special halting problem is undecidable, any non-trivial question\nregarding computable functions is also undecidable:\nL Halting problem for all inputs/nontermination: Both\nLH,∗ := {w | w ∈ {0, 1}∗, for all x ∈ {0, 1}∗ : Mw halts on x}\nLH,∗ = {w | w ∈ {0, 1}∗, there ex. x ∈ {0, 1}∗ : Mw does not halt on x}\nare not semideciable.\n▷ LH,∗ is not semidecidable: As before, let ρ(w) be the encoding of the D1TM\nM′\nw that simply replaces every input by w and then runs Mw on w.\nThen: w ∈ Lh iff ρ(w) ∈ LH,∗\n▷ LH,∗ is not semidecidable: Given w ∈ {0, 1}∗ construct M′\nw as follows:\n▷ On input x simulate Mw on w for n := lsbf−1(x1) steps.\n▷ If Mw halts during the simulation, loop forever, otherwise halt.\nLet ˆρ(w) := w′ be the encoding of M′\nw.\nThen: w ∈ Lh iff ˆρ(w) ∈ LH,∗.\n(M′",
      "▷ LH,∗ is not semidecidable: Given w ∈ {0, 1}∗ construct M′\nw as follows:\n▷ On input x simulate Mw on w for n := lsbf−1(x1) steps.\n▷ If Mw halts during the simulation, loop forever, otherwise halt.\nLet ˆρ(w) := w′ be the encoding of M′\nw.\nThen: w ∈ Lh iff ˆρ(w) ∈ LH,∗.\n(M′\nw rejects all x with |x| larger than the running time of Mw on input w.)No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem in general and Rice’s theorem 51 (109)\n▷ As the special halting problem is undecidable, any non-trivial question\nregarding computable functions is also undecidable:\nC Language (in)equivalence\nL= := {w#w′ | w, w′ ∈ {0, 1}∗, L(Mw) = L(Mw′)}\nL̸= := {w#w′ | w, w′ ∈ {0, 1}∗, L(Mw) ̸= L(Mw′)}\nBoth L= and L̸= are undecidable:\n▷ wlog we can assume that a D1TM either halts in a unique final state or does\nnot halt at all, i.e. wlog “accepting” and “halting” coincide. Then:\nLH,ε = {w ∈ Σ∗ | ε ∈ L(Mw)} is semidecidable,\nbut not LH,ε = {w ∈ Σ∗ | ε ̸∈ L(Mw)}.\nLH,? = {w ∈ Σ∗ | L(Mw) ̸= ∅} is semidecidable,\nbut not LH,? = {w ∈ Σ∗ | L(Mw) = ∅}.\nLH,∗ = {w ∈ Σ∗ | L(Mw) = Σ∗} and\nLH,∗ = {w ∈ Σ∗ | L(Mw) ̸= Σ∗} are both undecidable.\n▷ Thus simply choose some D1TM Mw′ with L(Mw′) = Σ∗ to reduce L= to\nLH,∗.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem in general and Rice’s theorem 51 (110)\n▷ As the special halting problem is undecidable, any non-trivial question\nregarding computable functions is also undecidable:\n! The main ideas used in the preceeding proofs can be generalized to Rice’s\ntheorem and the Rice-Shapiro theorem.\n! Informally, both theorems say that as soon as we try to write a “code\nanalyzer” for any non-trivial property/predicate regarding the function\ncomputed by a TM/program (semantics)\n▷ like termination/nontermination, total/partial, finitely many outputs/infinitely\nmany outputs, prototype vs. optimized code, . . .\nthis code analyzer will not terminate for some programs (or it has to give\nsometimes the wrong answer).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem in general and Rice’s theorem 51 (111)\n▷ As the special halting problem is undecidable, any non-trivial question\nregarding computable functions is also undecidable:\n! The important point to realize is that such a “code analyzer” has to\nterminate and give the correct answer for all TMs/programs.\nTwo solutions:\n▷ Only consider sufficiently restricted classes of TMs/programs.\nE.g. if we restrict the tape of 1TM to the length of the original input, then for\na given input x ∈ {0, 1}∗, there are at most (Γ ∪ Q)|x|+2 configurations.\n▷ Allow that the “code analyzer” returns “don’t know” or “false negatives” or\n“false positives” depending on the application.\nE.g. software tests actually yield “fail” or “couldn’t find an error/don’t\nknow/might be correct”, and thus potentially false positives.\n▷ Combination of both are used in software/hardware verifcation in practice,\ne.g. for device drivers.\n(Hardware is the physical realization of TMs (see e.g. VHDL).)",
      "know/might be correct”, and thus potentially false positives.\n▷ Combination of both are used in software/hardware verifcation in practice,\ne.g. for device drivers.\n(Hardware is the physical realization of TMs (see e.g. VHDL).)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem, busy beavers, and Kolmogorov complexity 52 (112)\nD Busy beaver: Let En be the set of all D1TMs with Γ = {0, 1}, □ = 0,\n|F| = 1, |Q| = n + 1 and terminate on input ε.\nΣB(n) is the maximal number of 1s output by any M ∈ En.\nSB(n) is the maximal number of steps made by any M ∈ En.\n▷ By definition ΣB(n) and SB(n) are always finite with\nn ≤ ΣB(n) ≤ SB(n)\nIt can be shown that SB(n) < ΣB(3n + 6) (cf. here).\n▷ wlog every M ∈ En always moves its head by ±1 until entering qf .\nAs δ ∈ (Q × Γ × {±1})Q\\{qf }×Γ we have |En| ≤((n + 1)4)2n.\n▷ Currently, the precise values are only known for n <5:\nΣB(1) = 1 Σ B(2) = 4 Σ B(3) = 6 Σ B(4) = 13\nSB(1) = 1 SB(2) = 6 SB(3) = 21 SB(4) = 107\n! w ̸∈ LH,ε iff Mw makes at least S(|Mw|) steps on input ε.\nHence: ΣB is not computable. In fact ΣB “outgrows” every computable\nfunction f : N → N . (cf. Rado).No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem, busy beavers, and Kolmogorov complexity 52 (113)\nD Busy beaver: Let En be the set of all D1TMs with Γ = {0, 1}, □ = 0,\n|F| = 1, |Q| = n + 1 and terminate on input ε.\nΣB(n) is the maximal number of 1s output by any M ∈ En.\nSB(n) is the maximal number of steps made by any M ∈ En.\n▷ An important consequence is:\n▷ Assume we have some open mathematical problem that can be disproved by\nmeans of a counter-example, e.g.:\nGoldbach’s conjecture: For all even n ∈ N with n >2 there exists two primes\np, qwith n = p + q.\n▷ Assume further we can write a program that implements an exhaustive search\nfor a counter-example (and thus does not require any input).\n▷ Then we can translate this program into a D1TM M with n states.\n▷ Thus: the conjecture is valid iff M makes at least SB(n) + 1 stesp.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem, busy beavers, and Kolmogorov complexity 52 (114)\nD Busy beaver: Let En be the set of all D1TMs with Γ = {0, 1}, □ = 0,\n|F| = 1, |Q| = n + 1 and terminate on input ε.\nΣB(n) is the maximal number of 1s output by any M ∈ En.\nSB(n) is the maximal number of steps made by any M ∈ En.\n▷ A bit simplified:\nKolmogorov complexity asks, given some word w ∈ Σ∗:\n“What is the shortest DTM that outputs w?”\nwhere “shortest” is e.g. the bit-length of the string describing the DTM.\nHence, the Kolmogorov complexity of a given word w is computable,\nbut as function in general not computable resp. as decision problem\nundedicable.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "4 String rewriting, grammars, Chomsky hierarchy\nString rewriting and grammars: abstraction of “mathematical calculations”\nChomsky hierarchy and normal forms\nGrammars vs. Turing machines (recognizable languages)\nUndecidable problems and Post correspondence problem\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "String rewriting systems 55 (117)\nD (Effective) string rewriting system (SRS, also: semi-Thue system) (Σ, R):\n▷ Σ: finite alphabet\n▷ R: finite set of rewrite rules with R ⊆ Σ∗ × Σ∗\n▷ The rewrite rules generate the rewriting relation − →R on Σ∗:\n− →R:= {(xuy, xvy) | (u, v) ∈ R, x, y∈ Σ∗}\nIts reflexive-transitive closure is denoted by − →∗\nR.\n▷ w can be rewritten to w′ wrt. R if w − →∗\nR w′.\n▷ SRS are a simple model of the concept of “calculating by hand”.\n▷ Consider e.g. for n = 5\nΣ := Z n ∪ {[, ], ·, +}\nR := {([x · y], z) | x, y, z∈ Z n, x· y ≡n z}\n∪ { ([x + y], z) | x, y, z∈ Z n, x+ y ≡n z}\n[[3 + 1]· [2 · 4]] − →R [4 · [2 · 4]] − →R [4 · 3] − →R 2\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "String rewriting systems 55 (118)\nD (Effective) string rewriting system (SRS, also: semi-Thue system) (Σ, R):\n▷ Σ: finite alphabet\n▷ R: finite set of rewrite rules with R ⊆ Σ∗ × Σ∗\n▷ The rewrite rules generate the rewriting relation − →R on Σ∗:\n− →R:= {(xuy, xvy) | (u, v) ∈ R, x, y∈ Σ∗}\nIts reflexive-transitive closure is denoted by − →∗\nR.\n▷ w can be rewritten to w′ wrt. R if w − →∗\nR w′.\n▷ SRS are a simple model of the concept of “calculating by hand”.\n▷ Consider e.g. for n = 2\nΣ := {0, 1, [, ], ∧, ¬}\nR := {(¬x, z) | x, z∈ {0, 1}, z= 1 − x}\n∪ { ([x ∧ y], z) | x, y, z∈ {0, 1}, z= min(x, y)}\n[¬0 ∧ [1 ∧ ¬0]] − →R [¬0 ∧ [1 ∧ 1]] − →R [¬0 ∧ 1] − →R [1 ∧ 1] − →R 1\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "String rewriting systems 55 (119)\nD (Effective) string rewriting system (SRS, also: semi-Thue system) (Σ, R):\n▷ Σ: finite alphabet\n▷ R: finite set of rewrite rules with R ⊆ Σ∗ × Σ∗\n▷ The rewrite rules generate the rewriting relation − →R on Σ∗:\n− →R:= {(xuy, xvy) | (u, v) ∈ R, x, y∈ Σ∗}\nIts reflexive-transitive closure is denoted by − →∗\nR.\n▷ w can be rewritten to w′ wrt. R if w − →∗\nR w′.\n▷ SRS are a simple model of the concept of “calculating by hand”.\n▷ For 1TM M the relation − →M is an SRS on the configurations Γ∗ × Q × Γ∗\n− →M = {(αqaβ, αbrβ) | ((q, a), (r, b,+1)) ∈ δ, α, β∈ Γ∗}\n∪ {(αzqaβ, αrzbβ) | ((q, a), (r, b,−1)) ∈ δ, α, β∈ Γ∗, z∈ Γ}\n∪ {(αqaβ, αrbβ) | ((q, a), (r, b,0)) ∈ δ, α, β∈ Γ∗}\nIn the end, the CPU just rewrites binary strings.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "String rewriting systems 55 (120)\nD (Effective) string rewriting system (SRS, also: semi-Thue system) (Σ, R):\n▷ Σ: finite alphabet\n▷ R: finite set of rewrite rules with R ⊆ Σ∗ × Σ∗\n▷ The rewrite rules generate the rewriting relation − →R on Σ∗:\n− →R:= {(xuy, xvy) | (u, v) ∈ R, x, y∈ Σ∗}\nIts reflexive-transitive closure is denoted by − →∗\nR.\n▷ w can be rewritten to w′ wrt. R if w − →∗\nR w′.\n▷ SRS are a simple model of the concept of “calculating by hand”.\n▷ Consider {a, b}∗ modulo the identities a2 ≡ ε ≡ b2 and ab ≡ ba where each\nidentity w ≡ w′ gives rise to the rewrite rules (w, w′), (w′, w) ∈ R s.t.:\nam1 bn1 . . . amlbnl − →∗\nR a(m1+...+ml) mod 2b(n1+...+nl) mod 2 ∈ {ε, a, b, ab}\ni.e. this is actually a group that is isomorphic with Z 2 × Z 2.\n▷ The word problem for groups is undecidable in general, too:\nCan w be rewritten to w′ wrt. the defining identities?No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "String rewriting systems 55 (121)\nD (Effective) string rewriting system (SRS, also: semi-Thue system) (Σ, R):\n▷ Σ: finite alphabet\n▷ R: finite set of rewrite rules with R ⊆ Σ∗ × Σ∗\n▷ The rewrite rules generate the rewriting relation − →R on Σ∗:\n− →R:= {(xuy, xvy) | (u, v) ∈ R, x, y∈ Σ∗}\nIts reflexive-transitive closure is denoted by − →∗\nR.\n▷ w can be rewritten to w′ wrt. R if w − →∗\nR w′.\n▷ SRS are a simple model of the concept of “calculating by hand”.\n▷ Proofs based on natural deduction, e.g.:\n⊢ A ∧ B\n⊢ A\n⊢ A\n⊢ A ∨ B\n⊢ A ⊢ A → B\n⊢ B\nA ⊢ B A ⊢ ¬B\n⊢ ¬A\ncan be enumerated by a TM and can thus be rephrased as SRS.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "MU puzzle 56 (122)\n▷ The MU puzzle consists of the rewriting relation on Σ∗ = {M, I, U}∗:\n− →R := {(xI, xIU) | x ∈ Σ∗}\n∪ { (My, Myy) | y ∈ Σ∗}\n∪ { (xIIIy, xUy) | x, y∈ Σ∗}\n∪ { (xUUy, xy) | x, y∈ Σ∗}\ne.g.\nMI − →R MII − →R MIIII − →R MUI − →R MUIU\n− →R MUIUUIU − →R MUIIU\nThe question is to decide whether MI − →∗\nR MU .\n(e.g. see the DS slides for the solution.)\n▷ Note that formally − →R is not an SRS.\nBut a 1TM can nondeterministically apply the rules to rewrite MI step by\nstep into any word w with MI − →∗\nR w.\nAs we will see, any such 1TM can be translated to an SRS resp. grammar.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Formal grammars 57 (123)\nD (unrestricted formal) grammar G = (V, Σ, P, S)\n▷ V: finite alphabet of variable/nonterminal symbols\n▷ Σ: finite alphabet of terminal symbols with Σ ∩ V = ∅\n▷ P: finite set of production rules P ⊆ V + × (Σ ∪ V )∗ (with V + := V V∗)\n▷ S: the start symbol (axiom) with S ∈ V\n▷ The production rules generate the rewriting relation on (V ∪ Σ)∗\n− →G:= {(uvw, uv′w) | (v, v′) ∈ P, u, w∈ (Σ ∪ V )∗}\nIts reflexive-transitive closure is denoted by − →∗\nG.\n▷ A path S − →G α1 − →G . . .− →G αl − →G w is called a derivation of w.\n(A word w ∈ Σ∗ is also called a sentence, a word αi ∈ (V ∪ Σ)∗ a sentential form.)\nThe language generated by G is L(G) := {w ∈ Σ∗ | S − →∗\nG w}.\n▷ A grammar is a restricted from of SRS specifically for generating/producing a\nlanguage. (But this distinction is only of technical nature) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (124)\nV = {E, N, D, X} S = E\nΣ = {[, ], +, ∗, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,′ }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E ∗ E])}\n∪ {(N, DN), (N, ε)} ∪ {(D, i) | i ∈ Z 10} ∪ {(X, X′), (X, x)}\nE − →G [E ∗ E]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (125)\nV = {E, N, D, X} S = E\nΣ = {[, ], +, ∗, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,′ }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E ∗ E])}\n∪ {(N, DN), (N, ε)} ∪ {(D, i) | i ∈ Z 10} ∪ {(X, X′), (X, x)}\nE − →G [E ∗ E]\n− →G [[E + E] ∗ E]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (126)\nV = {E, N, D, X} S = E\nΣ = {[, ], +, ∗, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,′ }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E ∗ E])}\n∪ {(N, DN), (N, ε)} ∪ {(D, i) | i ∈ Z 10} ∪ {(X, X′), (X, x)}\nE − →G [E ∗ E]\n− →G [[E + E] ∗ E]\n− →G [[DN + E] ∗ E]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (127)\nV = {E, N, D, X} S = E\nΣ = {[, ], +, ∗, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,′ }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E ∗ E])}\n∪ {(N, DN), (N, ε)} ∪ {(D, i) | i ∈ Z 10} ∪ {(X, X′), (X, x)}\nE − →G [E ∗ E]\n− →G [[E + E] ∗ E]\n− →G [[DN + E] ∗ E]\n− →G [[DDN + X] ∗ X]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (128)\nV = {E, N, D, X} S = E\nΣ = {[, ], +, ∗, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,′ }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E ∗ E])}\n∪ {(N, DN), (N, ε)} ∪ {(D, i) | i ∈ Z 10} ∪ {(X, X′), (X, x)}\nE − →G [E ∗ E]\n− →G [[E + E] ∗ E]\n− →G [[DN + E] ∗ E]\n− →G [[DDN + X] ∗ X]\n− →G [[DD + X] ∗ X]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (129)\nV = {E, N, D, X} S = E\nΣ = {[, ], +, ∗, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,′ }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E ∗ E])}\n∪ {(N, DN), (N, ε)} ∪ {(D, i) | i ∈ Z 10} ∪ {(X, X′), (X, x)}\nE − →G [E ∗ E]\n− →G [[E + E] ∗ E]\n− →G [[DN + E] ∗ E]\n− →G [[DDN + X] ∗ X]\n− →G [[DD + X] ∗ X]\n− →G [[D3 + X] ∗ X]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (130)\nV = {E, N, D, X} S = E\nΣ = {[, ], +, ∗, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,′ }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E ∗ E])}\n∪ {(N, DN), (N, ε)} ∪ {(D, i) | i ∈ Z 10} ∪ {(X, X′), (X, x)}\nE − →G [E ∗ E]\n− →G [[E + E] ∗ E]\n− →G [[DN + E] ∗ E]\n− →G [[DDN + X] ∗ X]\n− →G [[DD + X] ∗ X]\n− →G [[D3 + X] ∗ X]\n− →G [[23 + X] ∗ X]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (131)\nV = {E, N, D, X} S = E\nΣ = {[, ], +, ∗, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,′ }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E ∗ E])}\n∪ {(N, DN), (N, ε)} ∪ {(D, i) | i ∈ Z 10} ∪ {(X, X′), (X, x)}\nE − →G [E ∗ E]\n− →G [[E + E] ∗ E]\n− →G [[DN + E] ∗ E]\n− →G [[DDN + X] ∗ X]\n− →G [[DD + X] ∗ X]\n− →G [[D3 + X] ∗ X]\n− →G [[23 + X] ∗ X]\n− →G [[23 + X′] ∗ X]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (132)\nV = {E, N, D, X} S = E\nΣ = {[, ], +, ∗, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,′ }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E ∗ E])}\n∪ {(N, DN), (N, ε)} ∪ {(D, i) | i ∈ Z 10} ∪ {(X, X′), (X, x)}\nE − →G [E ∗ E]\n− →G [[E + E] ∗ E]\n− →G [[DN + E] ∗ E]\n− →G [[DDN + X] ∗ X]\n− →G [[DD + X] ∗ X]\n− →G [[D3 + X] ∗ X]\n− →G [[23 + X] ∗ X]\n− →G [[23 + X′] ∗ X]\n− →G [[23 + x′] ∗ X]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (133)\nV = {E, N, D, X} S = E\nΣ = {[, ], +, ∗, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,′ }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E ∗ E])}\n∪ {(N, DN), (N, ε)} ∪ {(D, i) | i ∈ Z 10} ∪ {(X, X′), (X, x)}\nE − →G [E ∗ E]\n− →G [[E + E] ∗ E]\n− →G [[DN + E] ∗ E]\n− →G [[DDN + X] ∗ X]\n− →G [[DD + X] ∗ X]\n− →G [[D3 + X] ∗ X]\n− →G [[23 + X] ∗ X]\n− →G [[23 + X′] ∗ X]\n− →G [[23 + x′] ∗ X]\n− →G [[23 + x′] ∗ X]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (134)\nV = {E, N, D, X} S = E\nΣ = {[, ], +, ∗, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,′ }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E ∗ E])}\n∪ {(N, DN), (N, ε)} ∪ {(D, i) | i ∈ Z 10} ∪ {(X, X′), (X, x)}\nE − →G [E ∗ E]\n− →G [[E + E] ∗ E]\n− →G [[DN + E] ∗ E]\n− →G [[DDN + X] ∗ X]\n− →G [[DD + X] ∗ X]\n− →G [[D3 + X] ∗ X]\n− →G [[23 + X] ∗ X]\n− →G [[23 + X′] ∗ X]\n− →G [[23 + x′] ∗ X]\n− →G [[23 + x′] ∗ X]\n− →G [[23 + x′] ∗ x\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (135)\nV = {E, N, D, X} S = E\nΣ = {[, ], +, ∗, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,′ }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E ∗ E])}\n∪ {(N, DN), (N, ε)} ∪ {(D, i) | i ∈ Z 10} ∪ {(X, X′), (X, x)}\nE − →G [E ∗ E]\n− →G [[E + E] ∗ E]\n− →G [[DN + E] ∗ E]\n− →G [[DDN + X] ∗ X]\n− →G [[DD + X] ∗ X]\n− →G [[D3 + X] ∗ X]\n− →G [[23 + X] ∗ X]\n− →G [[23 + X′] ∗ X]\n− →G [[23 + x′] ∗ X]\n− →G [[23 + x′] ∗ X]\n− →G [[23 + x′] ∗ x\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for arithmetic expressions 58 (136)\nV = {E, N, D, X} S = E\nΣ = {[, ], +, ∗, 0, 1, 2, 3, 4, 5, 6, 7, 9, x,′ }\nP = {(E, DN), (E, X), (E, [E + E]), (E, [E ∗ E])}\n∪ {(N, DN), (N, ε)} ∪ {(D, i) | i ∈ Z 10} ∪ {(X, X′), (X, x)}\n! For convenience:\n▷ “− →G” is commonly used also for denoting/defining the actual rules P.\nThis makes it also easier to use “ (” and “ )” as terminal symbols.\n▷ “|” is used for grouping rules with the same left-hand side.\nBut this assumes/requires that | ̸∈V ∪ Σ.\nAbove grammar is thus typically defined by simply writing\nE − →G DN | X | [E + E] | [E ∗ E]\nN − →G DN | ε\nX − →G X′ | x\nD − →G 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (137)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (138)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (139)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (140)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (141)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (142)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\n− →G ((¬X op X ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (143)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\n− →G ((¬X op X ) op F)\n− →G ((¬X op x) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (144)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\n− →G ((¬X op X ) op F)\n− →G ((¬X op x) op F)\n− →G ((¬X′ op x) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (145)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\n− →G ((¬X op X ) op F)\n− →G ((¬X op x) op F)\n− →G ((¬X′ op x) op F)\n− →G ((¬X′ op x) op X)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (146)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\n− →G ((¬X op X ) op F)\n− →G ((¬X op x) op F)\n− →G ((¬X′ op x) op F)\n− →G ((¬X′ op x) op X)\n− →G ((¬X′ op x) op X′)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (147)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\n− →G ((¬X op X ) op F)\n− →G ((¬X op x) op F)\n− →G ((¬X′ op x) op F)\n− →G ((¬X′ op x) op X)\n− →G ((¬X′ op x) op X′)\n− →G ((¬x′ op x) op X′)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (148)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\n− →G ((¬X op X ) op F)\n− →G ((¬X op x) op F)\n− →G ((¬X′ op x) op F)\n− →G ((¬X′ op x) op X)\n− →G ((¬X′ op x) op X′)\n− →G ((¬x′ op x) op X′)\n− →G ((¬x′ op x) op x′)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (149)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\n− →G ((¬X op X ) op F)\n− →G ((¬X op x) op F)\n− →G ((¬X′ op x) op F)\n− →G ((¬X′ op x) op X)\n− →G ((¬X′ op x) op X′)\n− →G ((¬x′ op x) op X′)\n− →G ((¬x′ op x) op x′)\n− →G ((¬x′ op x) ∧ x′)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (150)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\n− →G ((¬X op X ) op F)\n− →G ((¬X op x) op F)\n− →G ((¬X′ op x) op F)\n− →G ((¬X′ op x) op X)\n− →G ((¬X′ op x) op X′)\n− →G ((¬x′ op x) op X′)\n− →G ((¬x′ op x) op x′)\n− →G ((¬x′ op x) ∧ x′)\n− →G ((¬x′ ∨ x) ∧ x′)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A grammar for propositional formulas 59 (151)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\n▷ The order in which we apply the production rules is not predetermined,\ni.e. rewriting is usually nondeterministic.\n▷ The grammars in this and the preceding examples are called context-free as\nwe/the computer can simply replace a single nonterminal/variable without\nkeeping track of the surrounding symbols (context).\n▷ The main use of context-free grammars is to define the (syntax) trees\nunderlying well-formed expressions/formulas/terms/programs:\nRecall from DS: every binary tree can be encoded into a Dyck word, every\nDyck word encodes a binary tree.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Unary-binary trees and Dyck languages 60 (152)\nV = {S} Σ = {[, ]}\nS − →G [] | [S] | [SS]\n\"\u0014h\u0002\n[ ]\n\u0003ih\n[ ]\ni\u0015\u0014 hi \u0015#\n▷ Dyck language wrt. a single type of brackets:\nEvery node is encoded by means of a matching pair of brackets,\n▷ Often, a leaf is also encoded as the empty word.\nchildren are encoded by means of nesting.\n▷ E.g. using the CYK algorithm (later) we can recover the tree from the word.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars for Python variable identifiers and keywords 61 (153)\nV = {K} S = K Σ = {A, . . . ,Z, a, . . . ,z}\nK − →G True | False | None | and | or | not | if | else | elif\nK − →G . . .\nV ′ = {I, J, D, L} S′ = I Σ′ = {A, . . . ,Z, a, . . . ,z, , 0, . . . ,9}\nI − →G′ LJ\nJ − →G′ LJ | DJ | ε\nD − →G′ 0 | . . .| 9\nL − →G′ a | . . .| z | A | . . .| Z |\n▷ Actually, an identifier must not be a keyword, i.e. we rather should give a\ngrammar for L(G′) \\ L(G).\n▷ For these simple grammars (called regular) we know how to compute from G\nand G′ a grammar for L(G′) \\ L(G).\n▷ But in general, this is not possible\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n ∈ N } 62(154)\nV = {S, A, B, C, L, R} Σ = {a, b, c}\nS − →G ASBC | R\nRB − →G BR RCB − →G BCR RC − →G CR | Lc\nCL − →G Lc BL − →G Lb AL − →G La | a\nS − →G A S BC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n ∈ N } 62(155)\nV = {S, A, B, C, L, R} Σ = {a, b, c}\nS − →G ASBC | R\nRB − →G BR RCB − →G BCR RC − →G CR | Lc\nCL − →G Lc BL − →G Lb AL − →G La | a\nS − →G A S BC\n− →G AA S BCBC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n ∈ N } 62(156)\nV = {S, A, B, C, L, R} Σ = {a, b, c}\nS − →G ASBC | R\nRB − →G BR RCB − →G BCR RC − →G CR | Lc\nCL − →G Lc BL − →G Lb AL − →G La | a\nS − →G A S BC\n− →G AA S BCBC\n− →G AA R BCBC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n ∈ N } 62(157)\nV = {S, A, B, C, L, R} Σ = {a, b, c}\nS − →G ASBC | R\nRB − →G BR RCB − →G BCR RC − →G CR | Lc\nCL − →G Lc BL − →G Lb AL − →G La | a\nS − →G A S BC\n− →G AA S BCBC\n− →G AA R BCBC\n− →G AAB R CBC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n ∈ N } 62(158)\nV = {S, A, B, C, L, R} Σ = {a, b, c}\nS − →G ASBC | R\nRB − →G BR RCB − →G BCR RC − →G CR | Lc\nCL − →G Lc BL − →G Lb AL − →G La | a\nS − →G A S BC\n− →G AA S BCBC\n− →G AA R BCBC\n− →G AAB R CBC\n− →G AABBC R C\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n ∈ N } 62(159)\nV = {S, A, B, C, L, R} Σ = {a, b, c}\nS − →G ASBC | R\nRB − →G BR RCB − →G BCR RC − →G CR | Lc\nCL − →G Lc BL − →G Lb AL − →G La | a\nS − →G A S BC\n− →G AA S BCBC\n− →G AA R BCBC\n− →G AAB R CBC\n− →G AABBC R C\n− →G AABBC L c\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n ∈ N } 62(160)\nV = {S, A, B, C, L, R} Σ = {a, b, c}\nS − →G ASBC | R\nRB − →G BR RCB − →G BCR RC − →G CR | Lc\nCL − →G Lc BL − →G Lb AL − →G La | a\nS − →G A S BC\n− →G AA S BCBC\n− →G AA R BCBC\n− →G AAB R CBC\n− →G AABBC R C\n− →G AABBC L c\n− →G AABB L cc\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n ∈ N } 62(161)\nV = {S, A, B, C, L, R} Σ = {a, b, c}\nS − →G ASBC | R\nRB − →G BR RCB − →G BCR RC − →G CR | Lc\nCL − →G Lc BL − →G Lb AL − →G La | a\nS − →G A S BC\n− →G AA S BCBC\n− →G AA R BCBC\n− →G AAB R CBC\n− →G AABBC R C\n− →G AABBC L c\n− →G AABB L cc\n− →G AAB L bcc\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n ∈ N } 62(162)\nV = {S, A, B, C, L, R} Σ = {a, b, c}\nS − →G ASBC | R\nRB − →G BR RCB − →G BCR RC − →G CR | Lc\nCL − →G Lc BL − →G Lb AL − →G La | a\nS − →G A S BC\n− →G AA S BCBC\n− →G AA R BCBC\n− →G AAB R CBC\n− →G AABBC R C\n− →G AABBC L c\n− →G AABB L cc\n− →G AAB L bcc\n− →G AA L bbcc\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n ∈ N } 62(163)\nV = {S, A, B, C, L, R} Σ = {a, b, c}\nS − →G ASBC | R\nRB − →G BR RCB − →G BCR RC − →G CR | Lc\nCL − →G Lc BL − →G Lb AL − →G La | a\nS − →G A S BC\n− →G AA S BCBC\n− →G AA R BCBC\n− →G AAB R CBC\n− →G AABBC R C\n− →G AABBC L c\n− →G AABB L cc\n− →G AAB L bcc\n− →G AA L bbcc\n− →G A L abbcc\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "An incorrect grammar for {anbncn | n ∈ N } 62(164)\nV = {S, A, B, C, L, R} Σ = {a, b, c}\nS − →G ASBC | R\nRB − →G BR RCB − →G BCR RC − →G CR | Lc\nCL − →G Lc BL − →G Lb AL − →G La | a\nS − →G A S BC\n− →G AA S BCBC\n− →G AA R BCBC\n− →G AAB R CBC\n− →G AABBC R C\n− →G AABBC L c\n− →G AABB L cc\n− →G AAB L bcc\n− →G AA L bbcc\n− →G A L abbcc\n− →G aabbcc\nConsider S − →∗\nG AAA S BCBCBCand fix the error.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “non-decreasing” grammar for {anbncn | n ∈ N } 63(165)\nV = {S, A, B, C, R, L} Σ = {a, b, c}\nS − →G abc | ARBC RB − →G BR RC − →G LBCC\nBL − →G LB AL − →G AA | AAR\nA − →G a B − →G b C − →G c\nS − →G A R BC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “non-decreasing” grammar for {anbncn | n ∈ N } 63(166)\nV = {S, A, B, C, R, L} Σ = {a, b, c}\nS − →G abc | ARBC RB − →G BR RC − →G LBCC\nBL − →G LB AL − →G AA | AAR\nA − →G a B − →G b C − →G c\nS − →G A R BC\n− →G AB R C\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “non-decreasing” grammar for {anbncn | n ∈ N } 63(167)\nV = {S, A, B, C, R, L} Σ = {a, b, c}\nS − →G abc | ARBC RB − →G BR RC − →G LBCC\nBL − →G LB AL − →G AA | AAR\nA − →G a B − →G b C − →G c\nS − →G A R BC\n− →G AB R C\n− →G AB L BCC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “non-decreasing” grammar for {anbncn | n ∈ N } 63(168)\nV = {S, A, B, C, R, L} Σ = {a, b, c}\nS − →G abc | ARBC RB − →G BR RC − →G LBCC\nBL − →G LB AL − →G AA | AAR\nA − →G a B − →G b C − →G c\nS − →G A R BC\n− →G AB R C\n− →G AB L BCC\n− →G A L BBCC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “non-decreasing” grammar for {anbncn | n ∈ N } 63(169)\nV = {S, A, B, C, R, L} Σ = {a, b, c}\nS − →G abc | ARBC RB − →G BR RC − →G LBCC\nBL − →G LB AL − →G AA | AAR\nA − →G a B − →G b C − →G c\nS − →G A R BC\n− →G AB R C\n− →G AB L BCC\n− →G A L BBCC\n− →G AA R BBCC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “non-decreasing” grammar for {anbncn | n ∈ N } 63(170)\nV = {S, A, B, C, R, L} Σ = {a, b, c}\nS − →G abc | ARBC RB − →G BR RC − →G LBCC\nBL − →G LB AL − →G AA | AAR\nA − →G a B − →G b C − →G c\nS − →G A R BC\n− →G AB R C\n− →G AB L BCC\n− →G A L BBCC\n− →G AA R BBCC\n− →G AAB R BCC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “non-decreasing” grammar for {anbncn | n ∈ N } 63(171)\nV = {S, A, B, C, R, L} Σ = {a, b, c}\nS − →G abc | ARBC RB − →G BR RC − →G LBCC\nBL − →G LB AL − →G AA | AAR\nA − →G a B − →G b C − →G c\nS − →G A R BC\n− →G AB R C\n− →G AB L BCC\n− →G A L BBCC\n− →G AA R BBCC\n− →G AAB R BCC\n− →G AABB R CC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “non-decreasing” grammar for {anbncn | n ∈ N } 63(172)\nV = {S, A, B, C, R, L} Σ = {a, b, c}\nS − →G abc | ARBC RB − →G BR RC − →G LBCC\nBL − →G LB AL − →G AA | AAR\nA − →G a B − →G b C − →G c\nS − →G A R BC\n− →G AB R C\n− →G AB L BCC\n− →G A L BBCC\n− →G AA R BBCC\n− →G AAB R BCC\n− →G AABB R CC\n− →G AABB L BBCCC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “non-decreasing” grammar for {anbncn | n ∈ N } 63(173)\nV = {S, A, B, C, R, L} Σ = {a, b, c}\nS − →G abc | ARBC RB − →G BR RC − →G LBCC\nBL − →G LB AL − →G AA | AAR\nA − →G a B − →G b C − →G c\nS − →G A R BC\n− →G AB R C\n− →G AB L BCC\n− →G A L BBCC\n− →G AA R BBCC\n− →G AAB R BCC\n− →G AABB R CC\n− →G AABB L BBCCC\n− →G AAB L BBBCCC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “non-decreasing” grammar for {anbncn | n ∈ N } 63(174)\nV = {S, A, B, C, R, L} Σ = {a, b, c}\nS − →G abc | ARBC RB − →G BR RC − →G LBCC\nBL − →G LB AL − →G AA | AAR\nA − →G a B − →G b C − →G c\nS − →G A R BC\n− →G AB R C\n− →G AB L BCC\n− →G A L BBCC\n− →G AA R BBCC\n− →G AAB R BCC\n− →G AABB R CC\n− →G AABB L BBCCC\n− →G AAB L BBBCCC\n− →G AA L BBBBCCC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “non-decreasing” grammar for {anbncn | n ∈ N } 63(175)\nV = {S, A, B, C, R, L} Σ = {a, b, c}\nS − →G abc | ARBC RB − →G BR RC − →G LBCC\nBL − →G LB AL − →G AA | AAR\nA − →G a B − →G b C − →G c\nS − →G A R BC\n− →G AB R C\n− →G AB L BCC\n− →G A L BBCC\n− →G AA R BBCC\n− →G AAB R BCC\n− →G AABB R CC\n− →G AABB L BBCCC\n− →G AAB L BBBCCC\n− →G AA L BBBBCCC\n− →G AAABBBBCCC\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “non-decreasing” grammar for {anbncn | n ∈ N } 63(176)\nV = {S, A, B, C, R, L} Σ = {a, b, c}\nS − →G abc | ARBC RB − →G BR RC − →G LBCC\nBL − →G LB AL − →G AA | AAR\nA − →G a B − →G b C − →G c\nS − →G A R BC\n− →G AB R C\n− →G AB L BCC\n− →G A L BBCC\n− →G AA R BBCC\n− →G AAB R BCC\n− →G AABB R CC\n− →G AABB L BBCCC\n− →G AAB L BBBCCC\n− →G AA L BBBBCCC\n− →G AAABBBBCCC\n− →∗\nG aaabbbccc\nIn every rule: |LHS| ≤ |RHS|; such grammars are called context-sensitive.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “linear” grammar for {anbn | n ∈ N } 64(177)\nV = {S} Σ = {a, b}\nS − →G aSb | ab\nS − →G aSb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “linear” grammar for {anbn | n ∈ N } 64(178)\nV = {S} Σ = {a, b}\nS − →G aSb | ab\nS − →G aSb\n− →G aaSbb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “linear” grammar for {anbn | n ∈ N } 64(179)\nV = {S} Σ = {a, b}\nS − →G aSb | ab\nS − →G aSb\n− →G aaSbb\n− →G aaaSbbb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “linear” grammar for {anbn | n ∈ N } 64(180)\nV = {S} Σ = {a, b}\nS − →G aSb | ab\nS − →G aSb\n− →G aaSbb\n− →G aaaSbbb\n− →G aaaabbbb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “right-linear” grammar for {an | n ∈ N } 65(181)\nV = {S} Σ = {a, b}\nS − →G aS | a\nS − →G aS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “right-linear” grammar for {an | n ∈ N } 65(182)\nV = {S} Σ = {a, b}\nS − →G aS | a\nS − →G aS\n− →G aaS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “right-linear” grammar for {an | n ∈ N } 65(183)\nV = {S} Σ = {a, b}\nS − →G aS | a\nS − →G aS\n− →G aaS\n− →G aaaS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A “right-linear” grammar for {an | n ∈ N } 65(184)\nV = {S} Σ = {a, b}\nS − →G aS | a\nS − →G aS\n− →G aaS\n− →G aaaS\n− →G aaaa\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Graphs and right-linear grammars 66 (185)\na b\nc d\nV = {S, Xa, Xb, Xc, Xd} Σ = {a, b, c, d}\nS − →G aXa | bXb | cXc | dXd\nXa − →G bXb | cXc | ε X b − →G dXd | bXb | ε X c − →G bXb | dXd | ε X d − →G cXc | ε\n▷ Every finite directed graph G gives rise to a right-linear (regular) grammar G\nso that L(G) is the set of finite paths of G.\n▷ This is one reason why ∗ is used both for languages and the\nreflexive-transitive closure.\n▷ Similarly, every regular grammar G gives rise to a graph (which can be read\nas a 1TM that “accepts” L(G)).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "SRS vs. grammars 67 (186)\nF Every grammar G = (V, Σ, P, S) is an SRS on the alphabet Γ = V ∪ Σ.\nL For every SRS (Σ, R) there exists a grammar G so that\nL(G) = {w#w′ | w − →∗\nR w′}\ni.e. L(G) generates all pairs of words so that the first word can be rewritten\nto the second word wrt. R. (“#” serves as delimiter again.)\n▷ For each a ∈ Σ introduce new variables Xa, Za and rules Xa − →G a, Za − →G a.\nFor each rule (α, β) ∈ R introduce the rule (α′, β′) where α′, β′ are obtained\nfrom α, βby replacing every a ∈ Σ by Za.\nIntroduce a new variable S as start symbol and add the rules\nS − →G XaYa# | XaYaS Y aXb − →G XbYa Ya# − →G #Za\ni.e. S generates Xa1 . . . Xal#Za1 . . . Zal\nS − →∗\nG Xa1 Ya1 . . . XalYal# − →∗\nG Xa1 . . . Xal#Za1 . . . Zal − →∗\nG . . .\nfor some nondeterministically chosen w = a1 . . . al ∈ Σ∗ where the modified\nrules of R can only be applied to Za1 . . . Zal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "4 String rewriting, grammars, Chomsky hierarchy\nString rewriting and grammars: abstraction of “mathematical calculations”\nChomsky hierarchy and normal forms\nGrammars vs. Turing machines (recognizable languages)\nUndecidable problems and Post correspondence problem\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Chomsky hierarchy 69 (188)\nD Chomsky hierarchy\n▷ Type 0 or general phrase structure grammar:\nno restrictions, i.e. P ⊆ V + × (Σ ∪ V )∗ with |P| < ∞.\n▷ Type 1 or context-sensitive grammar:\n|v| ≤ |v′| for all (v, v′) ∈ P (so v′ ̸= ε as v ∈ V +)\n▷ Type 2 or context-free grammar:\n|v| ≤ |v′| for all (v, v′) ∈ P and v ∈ V\n▷ Type 3 or (right) regular grammar:\n|v| ≤ |v′| for all (v, v′) ∈ P and v ∈ V and v′ ∈ Σ ∪ ΣV\n! For grammars of type 1, 2, 3 the usual convention is to allow (S, ε) ∈ P but\nto require that S does not appear on the right of any rule.\n▷ A context-free rule (v, v′) ∈ V × (Σ ∪ V )+ can be applied without taking the\ncontext into consideration, i.e. it is conceptually simpler for a human.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Chomsky hierarchy 69 (189)\nD Chomsky hierarchy\n▷ Type 0 or general phrase structure grammar:\nno restrictions, i.e. P ⊆ V + × (Σ ∪ V )∗ with |P| < ∞.\n▷ Type 1 or context-sensitive grammar:\n|v| ≤ |v′| for all (v, v′) ∈ P (so v′ ̸= ε as v ∈ V +)\n▷ Type 2 or context-free grammar:\n|v| ≤ |v′| for all (v, v′) ∈ P and v ∈ V\n▷ Type 3 or (right) regular grammar:\n|v| ≤ |v′| for all (v, v′) ∈ P and v ∈ V and v′ ∈ Σ ∪ ΣV\nD A language L ⊆ Σ∗ is of type x if L = L(G) for some grammar G of type x.\n▷ Examples:\n▷ {an | n ∈ N } is of type 0, 1, 2, 3 (regular).\n▷ {anbn | n ∈ N } is of type 0, 1, 2 (context-free).\n▷ {anbncn . . . zn | n ∈ N } is of type 0, 1 (context-sensitive).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Chomsky hierarchy and programming languages 70 (190)\n▷ Grammars are used for defining and parsing languages.\n▷ E.g. programming languages typically use grammars of both type 2 and 3:\n▷ type 3 (regular) grammars are used for defining valid/correct lexemes:\n▷ E.g. reserved words like class, def or variable identifiers.\n▷ the lexemes form the alphabet from which the program is formed.\n▷ lexemes usually only have a very simple structure, the grammar serves as\ncompressed infinite dictionary.\n▷ type 2 (context-free) grammars are used for defining\nvalid/correct/well-structured/well-formed programs (= words over lexemes).\n▷ As most programming languages support arithemtic expressions and\npropositional formulas (for assertions), i.e. more complex versions of Dyck\nlanguages, we actually need to use at least context-free grammars here.\n▷ See lex and yacc\nAs we will see: for both types 2, 3 we can decide the word problem efficiently.",
      "propositional formulas (for assertions), i.e. more complex versions of Dyck\nlanguages, we actually need to use at least context-free grammars here.\n▷ See lex and yacc\nAs we will see: for both types 2, 3 we can decide the word problem efficiently.\n▷ Some natural languages use context-sensitive rules in their grammar/syntax\n(cf. here).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Normal forms for grammars 71 (191)\nT Kuroda normal form for type 0: For every grammar G = (V, Σ, P, S) we can\ncompute a grammar G′ = (V ′, Σ, P′, S′) with L(G) = L(G′) and\nP′ ⊆ V V× V V∪ V × ({ε} ∪Σ ∪ V ∪ V V)\n▷ First, introduce for every a ∈ Σ a variable Xa.\nNext, replace a by Xa in all rules P.\nFinally, add the rules (Xa, a) for all a ∈ Σ.\nNow all rules are of the form P ⊆ V + × V ∗ ∪ V × Σ.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Normal forms for grammars 71 (192)\nT Kuroda normal form for type 0: For every grammar G = (V, Σ, P, S) we can\ncompute a grammar G′ = (V ′, Σ, P′, S′) with L(G) = L(G′) and\nP′ ⊆ V V× V V∪ V × ({ε} ∪Σ ∪ V ∪ V V)\n▷ If there is at least one rule (α, β) ∈ P with |α| > |β|:\nFirst, introduce a variable Xε and the rule (Xε, ε).\nNext replace every rule (α, γ) ∈ V + × V ∗ with |α| > |γ| by\nα − →G γ Xε . . . Xε| {z }\n|α|−|γ| copies\ns.t. G is now almost of type 1: the only exception is the rule (Xε, ε).\n! Can only happen, if G is of type 0.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Normal forms for grammars 71 (193)\nT Kuroda normal form for type 0: For every grammar G = (V, Σ, P, S) we can\ncompute a grammar G′ = (V ′, Σ, P′, S′) with L(G) = L(G′) and\nP′ ⊆ V V× V V∪ V × ({ε} ∪Σ ∪ V ∪ V V)\n▷ Replace all rules r = (A, C1C2C3 . . . Cl) ∈ P with l ≥ 3 by\nA − →G Rr;1\nRr;1 − →G C1Rr;2\n...\nRr;l − →G Cl\nThe newly introduced auxiliary variables Rr;i are used to write the sentential\nform Ci . . . Cl from left to right.\nBy construction Rr;1 can only produce C1 . . . Cl.\nFrequently, instead of Rr;i simply [CiCi+1 . . . Cl] is used so that\n[CiCi+1 . . . Cl] − →G Ci[Ci+1 . . . Cl]\nbut this requires that [ and ] are not already used.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Normal forms for grammars 71 (194)\nT Kuroda normal form for type 0: For every grammar G = (V, Σ, P, S) we can\ncompute a grammar G′ = (V ′, Σ, P′, S′) with L(G) = L(G′) and\nP′ ⊆ V V× V V∪ V × ({ε} ∪Σ ∪ V ∪ V V)\n▷ Replace all rules r = (A1A2A3 . . . Ak, C1 . . . Cl) ∈ P with k ≥ 3 by\nA1 − →G Rr;1\nRr;1A2 − →G C1Rr;2\n...\nRr;k−1Ak − →G Ck−1Ck (if k = l)\nRr;k−1Ak − →G Ck−1Rr;k (if k < l)\nRr;k − →G CkRr;k+1 (k ≤ l by construction)\n...\nRr;l − →G Cl\nThe newly introduced auxiliary variables Rr;i apply the again the rule r by\nreading A1 . . . Ak and replacing it by C1 . . . CkCk+1 . . . Cl.\n(Using the bracket convention, you can e.g. also write [Ai+1 . . . Ak; Ci . . . Cl] for\nRr;i if i < k; and simply [Ci . . . Cl] if i ≥ l; recall after the first step we have k ≤ l.)\n! Can only happen, if G is of type 0 or 1.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Normal forms for grammars 71 (195)\nT Kuroda normal form for type 0: For every grammar G = (V, Σ, P, S) we can\ncompute a grammar G′ = (V ′, Σ, P′, S′) with L(G) = L(G′) and\nP′ ⊆ V V× V V∪ V × ({ε} ∪Σ ∪ V ∪ V V)\n▷ Every derivation wrt. the original grammar can be simulated by the\nconstructed grammar\nWe skip the proof that the new grammar cannot generate any additional\nwords.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Normal forms for grammars 71 (196)\nT Kuroda normal form for type 0: For every grammar G = (V, Σ, P, S) we can\ncompute a grammar G′ = (V ′, Σ, P′, S′) with L(G) = L(G′) and\nP′ ⊆ V V× V V∪ V × ({ε} ∪Σ ∪ V ∪ V V)\nL Rules of the form (A, B) ∈ V × V can be contracted:\nConsider the the finite graph (V, {(A, C) | (A, C) ∈ P}).\nIf A can reach A′ in this graph and\n▷ (αA′β, C′D′) ∈ P with αβ ∈ V ∪ {ε}:\nadd the rule (αAβ, C′D′) as αAβ − →∗\nG αA′β − →G C′D′\n▷ (A′, a′) ∈ P: add the rule (A, a′) as A − →∗\nG A′ − →G a′ (a′ ∈ Σ).\n▷ (A′, ε) ∈ P: add the rule (A, ε) as A − →∗\nG A′ − →G ε (only for type 0).\nFinally, remove all rules (A, B) ∈ V × V .\n▷ These rules are sometimes called unary/unit/chain rules; the new rules simply\njump to the end of all possible “chains” A0 − →G A1 − →G A2 − →G . . .− →G Al.\n▷ Any such chain cannot produce a terminal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Normal forms for grammars 71 (197)\nT Kuroda normal form for type 0: For every grammar G = (V, Σ, P, S) we can\ncompute a grammar G′ = (V ′, Σ, P′, S′) with L(G) = L(G′) and\nP′ ⊆ V V× V V∪ V × ({ε} ∪Σ ∪ V ∪ V V)\nC Kuroda normal form for type 1: For every grammar G of type 1 we can\ncompute a grammar G′ = (V ′, Σ, P′, S′) with L(G) = L(G′) and\nP′ ⊆ V V× V V∪ V × (Σ ∪ V ∪ V 2)\nC Kuroda normal form for type 2: For every grammar G of type 2 we can\ncompute a grammar G′ = (V ′, Σ, P′, S′) with L(G) = L(G′) and\nP′ ⊆ V × (Σ ∪ V ∪ V 2)\nC Chomsky normal form for type 2: For every grammar G of type 2 we can\ncompute a grammar G′ = (V ′, Σ, P′, S′) with L(G) = L(G′) and\nP′ ⊆ V × (Σ ∪ V 2)\n(contract chain/unit/unary rules)No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Normal forms for grammars 71 (198)\nT Kuroda normal form for type 0: For every grammar G = (V, Σ, P, S) we can\ncompute a grammar G′ = (V ′, Σ, P′, S′) with L(G) = L(G′) and\nP′ ⊆ V V× V V∪ V × ({ε} ∪Σ ∪ V ∪ V V)\n▷ Remarks:\n▷ By the Kuroda normal form the only difference between type 1 and type 0 is\nthe use of (arbitrary) ε-rules A − →G ε.\n▷ And the difference between type 2 and 1 is the use of the context-sensitive\nrules AB − →G CD.\n▷ Using the Chomsky normal form, one can also show that\nGreibach normal form for type 1: For every grammar G of type 2 we can\ncompute a grammar G′ = (V ′, Σ, P′, S′) with L(G) = L(G′) and\nP′ ⊆ V × ΣV ∗\nRecall: G is of type 3 (regular, right-linear) if P ⊆ V × (Σ ∪ ΣV ).\nSo, the main difference is that in type 2 we may use also nonlinear rules of the\nform X − →G aX1 . . . Xk (k ≥ 2) (tail recursion vs. general recursion).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kuroda and Chomsky normal forms 72 (199)\n▷ Consider the grammar with V = {S, A, B, C, R, L}, Σ = {a, b, c} and\nS − →G aSBC | aR R − →G Lc L − →G b\nRB − →G BR RCB − →G BCR RC − →G CR\nCL − →G Lc BL − →G Lb\n▷ First introduce Xa − →G a, Xb − →G b, Xc − →G c,\nXa − →G a X b − →G b X c − →G c\nS − →G XaSBC | XaR R − →G LXc L − →G Xb\nRB − →G BR RCB − →G BCR RC − →G CR\nCL − →G LXc BL − →G LXb\n(Rather annoying.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kuroda and Chomsky normal forms 72 (200)\n▷ Consider the grammar with V = {S, A, B, C, R, L}, Σ = {a, b, c} and\nS − →G aSBC | aR R − →G Lc L − →G b\nRB − →G BR RCB − →G BCR RC − →G CR\nCL − →G Lc BL − →G Lb\n! Very often, (square) brackets are used to denote auxiliary variables:\nE.g. instead of Xa also [a] is used assuming that “ [, ]” are not used as symbols.\n[a] − →G a [b] − →G b [c] − →G c\nS − →G [a]SBC | [a]R R − →G L[c] L − →G [b]\nRB − →G BR RCB − →G BCR RC − →G CR\nCL − →G L[c] BL − →G L[b]\nwhere [a], [b], [c] have to be read as three nonterminals.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kuroda and Chomsky normal forms 72 (201)\n▷ Consider the grammar with V = {S, A, B, C, R, L}, Σ = {a, b, c} and\nS − →G aSBC | aR R − →G Lc L − →G b\nRB − →G BR RCB − →G BCR RC − →G CR\nCL − →G Lc BL − →G Lb\n▷ Next we need to replace S − →G [a]SBC :\nUsing the bracket-convention, we can actually simply unravel this rule into\nS − →G [a][SBC ] [ SBC ] − →G S[BC] [ BC] − →G BC\nyielding\n[a] − →G a [b] − →G b [c] − →G c\n[SBC ] − →G S[BC] [ BC] − →G BC\nS − →G [a][SBC ] | [a]R R − →G L[c] L − →G [b]\nRB − →G BR RCB − →G BCR RC − →G CR\nCL − →G L[c] BL − →G L[b]\n(cf. replacing z = x3 by z = xy, y= x2; replacing x′′ = λx by x′ = λy, y′ = x; the\nnew rules “simulate” the old rules.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kuroda and Chomsky normal forms 72 (202)\n▷ Consider the grammar with V = {S, A, B, C, R, L}, Σ = {a, b, c} and\nS − →G aSBC | aR R − →G Lc L − →G b\nRB − →G BR RCB − →G BCR RC − →G CR\nCL − →G Lc BL − →G Lb\n▷ Finally, we need to replace RCB − →G BCR e.g. by means to\nR − →G [CB; BCR] [ CB; BCR]C − →G B[B; CR] [ B; CR]B − →G CR\n(read [CG; BCR] as “replace CG by BCR”.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kuroda and Chomsky normal forms 72 (203)\n▷ Consider the grammar with V = {S, A, B, C, R, L}, Σ = {a, b, c} and\nS − →G aSBC | aR R − →G Lc L − →G b\nRB − →G BR RCB − →G BCR RC − →G CR\nCL − →G Lc BL − →G Lb\n▷ Kuroda normal form:\n[a] − →G a [b] − →G b [c] − →G c\n[SBC ] − →G S[BC] [ BC] − →G BC\nR − →G [CB; BCR]\n[CB; BCR] C − →G B[B; CR] [ B; CR]B − →G CR\nS − →G [a][SBC ] | [a]R R − →G L[c] L − →G [b]\nRB − →G BR RC − →G CR\nCL − →G L[c] BL − →G L[b]\nThe main point is that the new rules for [CB; BCR] and [B; CR] will\n“deadlock” if they cannot read the required symbols, and thus the derivation\ncannot reach a terminal word.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kuroda and Chomsky normal forms 72 (204)\n▷ Consider the grammar with V = {S, A, B, C, R, L}, Σ = {a, b, c} and\nS − →G aSBC | aR R − →G Lc L − →G b\nRB − →G BR RCB − →G BCR RC − →G CR\nCL − →G Lc BL − →G Lb\n▷ Just as a reminder “ [CB; BCR]” has to be read as a single\nnonteriminal/variable (identifier). Alternatively, we could write XCB;BCR , e.g.\nXa − →G a X b − →G b X c − →G c\nXSBC − →G SXBC XBC − →G BC\nR − →G XCB;BCR\nXCB;BCR C − →G BXB;CR XB;CRB − →G CR\nS − →G XaXSBC | XaR R − →G LXc L − →G Xb\nRB − →G BR RC − →G CR\nCL − →G LXc BL − →G LXb\nWe could further replace the linear rule R − →G XCB;BCR by\nRC − →G BXB;CR\nwhich makes the nonterminal XCB;BCR unreachable so that we can remove th\nXCB;BCR and all rules in which it is used.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "ε-rules in context-free grammars 73 (205)\n▷ Formally, the Chomsky hierarchy does not allow ε-rules (X − →G ε) for all\ngrammars of type greater than 0.\n! By the Kuroda normal form, ε-rules are also the only difference between\ngrammars of type 0 and type 1; as we will see, the word problem is\nundecidable for type 0, but decidable for type 1.\n▷ The usual convention for type 1, 2, 3 grammars is that the start symbol S\nmay be rewritten to ε with the restriction that S must not be used on\nright-hand side of any rule, i.e. the grammar looks like this:\nS − →G S′ | ε S ′ − →G . . .\nwith S′ the start symbol of a grammar of type 1, 2, 3 in the strict sense (no\nε-rules). (The word problem stays decidable.)\n▷ Still, allowing arbitrary ε-rules in grammars of type 2, 3 (not type 1) does not\nchange the type of the language.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Removing ε-rules in context-free grammars 74 (206)\nL Let G = (V, Σ, P, S) be a grammar with P ⊆ V × ({ε} ∪Σ ∪ V ∪ V V).\nCompute inductively E := E|V | by means of\nE0 := {X | (X, ε) ∈ P}\nEk+1 := Ek ∪ {X | (X, α) ∈ P, α∈ E∗\nk }| {z }\nX− →Gα− →∗\nGε\nDefine G′ = (V, Σ, P′, S) by (all combinations; Kuroda reduces this to two cases)\nP′ := P \\ {(X, ε) ∈ P}\n∪ { (X, Y) | (X, Y Z) ∈ P, Z∈ E}| {z }\nX− →GY Z− →∗\nGY\n∪{(X, Z) | (X, Y Z), Y∈ E}| {z }\nX− →GY Z− →∗\nGZ\nThen L(G) \\ {ε} = L(G′) and ε ∈ L(G) iff S ∈ E.\n▷ Consider the grammar G′′ with P′′ = P′ ∪ {(X, ε) | X ∈ E}.\n▷ Obviously, L(G) ⊆ L(G′′); we show by induction/well-ordering of N that the\ncontracted ε-rules {(X, ε) | X ∈ E}do not add any additional word s.t. the\nrules P′ already suffice.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Removing ε-rules in context-free grammars 74 (207)\nL Let G = (V, Σ, P, S) be a grammar with P ⊆ V × ({ε} ∪Σ ∪ V ∪ V V).\nCompute inductively E := E|V | by means of\nE0 := {X | (X, ε) ∈ P}\nEk+1 := Ek ∪ {X | (X, α) ∈ P, α∈ E∗\nk }| {z }\nX− →Gα− →∗\nGε\nDefine G′ = (V, Σ, P′, S) by (all combinations; Kuroda reduces this to two cases)\nP′ := P \\ {(X, ε) ∈ P}\n∪ { (X, Y) | (X, Y Z) ∈ P, Z∈ E}| {z }\nX− →GY Z− →∗\nGY\n∪{(X, Z) | (X, Y Z), Y∈ E}| {z }\nX− →GY Z− →∗\nGZ\nThen L(G) \\ {ε} = L(G′) and ε ∈ L(G) iff S ∈ E.\n▷ Consider a shortest derivation S − →∗\nG′′ w of a word w ̸∈ L(G′) ∪ {ε}\nS − →∗\nG′′ α′X′β′ − →G′′ α′αXββ ′ − →G′′ α′αββ′ − →∗\nG′′ w\nwith X′ − →G ε. Then X′ is introduced by some rule (X′, αXβ) so that we can\nshorten the derivation either by (X′, ε) ∈ P′′ (if αβ = ε) or (X′, X) ∈ P′ (if\nαβ = Y ).No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Removing ε-rules in context-free grammars 74 (208)\nL Let G = (V, Σ, P, S) be a grammar with P ⊆ V × ({ε} ∪Σ ∪ V ∪ V V).\nCompute inductively E := E|V | by means of\nE0 := {X | (X, ε) ∈ P}\nEk+1 := Ek ∪ {X | (X, α) ∈ P, α∈ E∗\nk }| {z }\nX− →Gα− →∗\nGε\nDefine G′ = (V, Σ, P′, S) by (all combinations; Kuroda reduces this to two cases)\nP′ := P \\ {(X, ε) ∈ P}\n∪ { (X, Y) | (X, Y Z) ∈ P, Z∈ E}| {z }\nX− →GY Z− →∗\nGY\n∪{(X, Z) | (X, Y Z), Y∈ E}| {z }\nX− →GY Z− →∗\nGZ\nThen L(G) \\ {ε} = L(G′) and ε ∈ L(G) iff S ∈ E.\nC ε ∈ L(G) is decidable for context-free grammars.\nC L(G) = ∅ is decidable for context-free grammars:\n▷ Replace every rule (X, a) ∈ P by (X, ε); then L(G) ̸= ∅ iff S ∈ E.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Removing ε-rules in context-free grammars: example 75 (209)\nV = {S} Σ = {a, b}\nS − →G T T T − →G ε T − →G aSb\n▷ First transform the grammar into Kuroda normal form:\nIntroduce variables for nonterimals and ε:\nS − →G T T T − →G Xε T − →G XaSXb\nXa − →G a X b − →G b X ε − →G ε\nThen “make right-hand sides at most quadractic”:\nS − →G T T T − →G Xε T − →G Xa[SXb] [ SXb] − →G SXb\nXa − →G a X b − →G b X ε − →G ε\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Removing ε-rules in context-free grammars: example 75 (210)\nV = {S} Σ = {a, b}\nS − →G T T T − →G Xε T − →G Xa[SXb] [ SXb] − →G SXb\nXa − →G a X b − →G b X ε − →G ε\n▷ Compute (“color”) the nonterminals that can produce ε:\n▷ E0 = {Xε}\n▷ E1 = {Xε, T} as T − →G Xε − →G ε.\n▷ E2 = {Xε, T, S} as S − →G T T− →G XεT − →G T − →G Xε − →G ε.\nRemove the ε-rule Xε − →G ε and adapt the rules\nS − →G T T T − →G Xε T − →G Xa[SXb] [ SXb] − →G SXb\nS − →G T T − →G Xa[SXb] [ SXb] − →G Xb\nXa − →G a X b − →G b\nS′ − →G S S ′ − →G ε\nwith S′ the new start symbol.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Removing ε-rules in context-free grammars: example 75 (211)\nV = {S} Σ = {a, b}\nS − →G T T T − →G Xε T − →G Xa[SXb] [ SXb] − →G SXb\nS − →G T T − →G Xa[SXb] [ SXb] − →G Xb\nXa − →G a X b − →G b S ′ − →G S S ′ − →G ε\n! In general, the resulting grammar will contain “nonproductive” nonterminals:\nFor a context-free grammar these can be computed by replacing (conceptually)\nall terminals by ε and then determining the nonterminals that can produce ε,\ni.e.:\nI.B. Every terminal is productive (P0 = Σ).\nI.S. A nonterminal X is productive iff (X, γ) ∈ P with γ consisting only of\nproductive symbols (Pk+1 = Pk ∪ {X ∈ V | (X, γ) ∈ P, γ∈ P∗\nk}).\nAll nonproductive nonterminals (including all rules that use an unproductive\nnonterminal) can be discarded.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Removing ε-rules in context-free grammars: example 75 (212)\nV = {S} Σ = {a, b}\nS − →G T T T − →G Xε T − →G Xa[SXb] [ SXb] − →G SXb\nS − →G T T − →G Xa[SXb] [ SXb] − →G Xb\nXa − →G a X b − →G b S ′ − →G S S ′ − →G ε\n! Finally, the context-free grammar might contain variables that are\nunreachable from the start symbol, i.e. cannot occur in any derivation.\nThese can be computed by considering the “call graph”\n(V, {(X, Y) | (X, Y) ∈ P} ∪ {(X, Y), (X, Z) | (X, Y Z) ∈ P})\nI.B. The axiom S is reachable (R0 = {S}).\nI.S. A nonterminal Y is reachable iff (X, αY γ) ∈ P for some α, γ∈ (V ∪ Σ)∗\n(Rk+1 = Rk ∪ {Y ∈ V | (X, αY γ) ∈ P, X∈ R∗\nk}).\nAny nonterminal that is not reachable starting from S in this graph can be\nremoved (including all rules in which it occurs) .\n(Note: For ε-rules and productive nonterminals we move backwards along the rules, for\nreachable nonterminals we move forwards; but all are variants of reachability/refl.-transitive",
      "removed (including all rules in which it occurs) .\n(Note: For ε-rules and productive nonterminals we move backwards along the rules, for\nreachable nonterminals we move forwards; but all are variants of reachability/refl.-transitive\nclosure, see also Horn clause resolution/exercises.)No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Remving ε-rules in context-free grammars: Dyck grammar 76 (213)\n▷ The usual definition of the Dyck grammar wrt. one pair of brackets is:\nS − →G ε | [S] | SS\n(The Kuroda normal makes it much simpler in general to remove ε-rules (e.g. consider a\nrule X − →G Y Y . . . Y) but is actually not required.)\n▷ As S − →G ε, we add the rules\n▷ S − →G [] as shortcut for S − →G [S] − →G []\n▷ S − →G S as shortcut for S − →G SS − →G S\nDiscarding S − →G ε yields\nS − →G [S] | SS | [] | S\nThe chain rule (cycle) S − →G S can trivally be discarded:\nS − →G [S] | SS | []\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Simplifying context-free grammars 77 (214)\n1 Introduce auxiliary variables s.t. P ⊆ V × ({ε} ∪Σ ∪ V ∪ V V)\n▷ This includes of course regular grammars.\n2 Contract ε-rules:\n▷ This might introduce additional chain rules.\n▷ If ε ∈ L(G), add a new start symbol S′ and the new rule S′ − →G ε.\n3 For Chomsky normal form: contract all chain/unary/unit rules.\n4 Discard all unproductive nonterminals:\n▷ This only removes rules and nonterminals, but might make some nonterminals\nunreachable.\n5 Discard all unreachable nonterminals:\n▷ This only removes rules and nonterminals, and cannot re-introduce\nunproductive nonterminals, chain rules or ε-rules.\n! Removing ε-rules before transforming the grammar into Kuroda normal form\ncan lead to an exponential blow up.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "4 String rewriting, grammars, Chomsky hierarchy\nString rewriting and grammars: abstraction of “mathematical calculations”\nChomsky hierarchy and normal forms\nGrammars vs. Turing machines (recognizable languages)\nUndecidable problems and Post correspondence problem\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Formal grammars and recognizable languages 79 (216)\nT A language is recognizable iff it can be generated by a type- 0 grammar.\n▷ Assume L = L(G) for some grammar G:\nBy using a vector alphabet we can assume the TM has two tapes.\nThe first tape remembers the input; the second tape is used to\nnondeterministically simulate every derivation wrt. G.\nThe TM accepts iff the second tape contains also the input.\n! If G is of type 1, then every simulation that tries to access tape cells not\ncontaining the original input can terminated.\nThat is: a context-sensitive language can be accepted by a 1TM that only\nmakes use of the cells that initially stored the input.\n▷ For the other direction, we treat the transition rules of the TM as rewrite rules\non the configurations (next).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Formal grammars and recognizable languages 79 (217)\nT A language is recognizable iff it can be generated by a type- 0 grammar.\n▷ Assume L = L(M), i.e. there is a deterministic 1TM\nM = (Q, Σ, Γ, δ, q0, □, {qf }) that accepts L.\nFor the grammar, we use the nonterminals {\n\u0014qx\ny\n\u0015\n,\n\u0014x\ny\n\u0015\n| q ∈ Q, x∈ Γ, y∈ Σ}.\nThe lower component remembers the original word, the upper component\nsimulates the tape of the 1TM.\nThe start symbol first nondeterministically a word w ∈ L(M) with the\nmaximal space required by a run of M on w:\nS − →G BIB |\n\u0014q0□\n□\n\u0015\nB − →G\n\u0014□\n□\n\u0015\nB |\n\u0014□\n□\n\u0015\nI − →G I\n\u0014a\na\n\u0015\n|\n\u0014q0a\na\n\u0015\n(a ∈ Σ)\nFor each transition rule ((q, a), (r, b, d)) ∈ δ introduce the production rules:\n\u0014qa\nx\n\u0015\u0014z\ny\n\u0015\n− →G\n\u0014b\nx\n\u0015\u0014rz\ny\n\u0015\n(d = +1)\n\u0014qa\nx\n\u0015\n− →G\n\u0014rb\nx\n\u0015\n(d = 0)\n\u0014z\ny\n\u0015\u0014qa\nx\n\u0015\n− →G\n\u0014rz\ny\n\u0015\u0014b\nx\n\u0015\n(d = −1)\nFinally, we can always terminate the simulation by means of the rules\n\u0014z\nx\n\u0015\n− →G x\n\u0014qf z\nx\n\u0015\n− →G x\n\u0014z\n□\n\u0015\n− →G ε\n\u0014qf z\n□\n\u0015\n− →G ε (z ∈ Γ, x∈ Σ)\nNote that we only allow to “forget” the final state.",
      "\u0015\n− →G\n\u0014b\nx\n\u0015\u0014rz\ny\n\u0015\n(d = +1)\n\u0014qa\nx\n\u0015\n− →G\n\u0014rb\nx\n\u0015\n(d = 0)\n\u0014z\ny\n\u0015\u0014qa\nx\n\u0015\n− →G\n\u0014rz\ny\n\u0015\u0014b\nx\n\u0015\n(d = −1)\nFinally, we can always terminate the simulation by means of the rules\n\u0014z\nx\n\u0015\n− →G x\n\u0014qf z\nx\n\u0015\n− →G x\n\u0014z\n□\n\u0015\n− →G ε\n\u0014qf z\n□\n\u0015\n− →G ε (z ∈ Γ, x∈ Σ)\nNote that we only allow to “forget” the final state.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Formal grammars and recognizable languages 79 (218)\nT A language is recognizable iff it can be generated by a type- 0 grammar.\nC There is no type- 0 grammar for Lε.\nC The word problem for a grammar G = (V, Σ, P, S) asks to decide whether\nS − →∗\nG w for a given word w. It is thus only semideciable (for type 0).\nC G is of type 1 iff it can be recognized by a 1TM that only accesses the tape\non cells originally containing input symbols.\n▷ Follows from the sketched constructions:\nIf G is type 1, we only need to nondeterministically simulate derivations\nS − →G α1 − →G α2 − →G . . .− →G αl\nas long as |s| ≤ |α0| ≤ |α1| ≤. . .≤ |αl| ≤ |w|\nIf M only uses the space given by the original input, we do not introduce or\nremove\n\u0014□\n□\n\u0015\nand the grammar is immediately of type 1.\nD A 1TM that only uses the space originally occupied by the input\n▷ i.e. if q0w − →∗\nM αqβ, then |αβ| ≤ |w|.",
      "as long as |s| ≤ |α0| ≤ |α1| ≤. . .≤ |αl| ≤ |w|\nIf M only uses the space given by the original input, we do not introduce or\nremove\n\u0014□\n□\n\u0015\nand the grammar is immediately of type 1.\nD A 1TM that only uses the space originally occupied by the input\n▷ i.e. if q0w − →∗\nM αqβ, then |αβ| ≤ |w|.\nis called an linear bounded automaton (LBA).No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-sensitive (type 1) languages 80 (219)\nT L(G) is decidable if G is at least of type 1 (resp. accepted by an LBA).\n▷ Again, use BFS to enumerate the words α ∈ (V ∪ Σ)∗ with S − →k\nG α for\nincreasing k. Along any path (derivation)\nS − →G α1 − →G α2 − →G . . .\nwe have |αi| ≤ |αi+1| by definition of type 1.\nGiven some word w ∈ Σ∗ thus terminate the BFS for all |αi| with |w| < |αi|.\n▷ Using BFS can again lead to an exponential runtime and also space/memory\nconsumption as we might traverse the perfect tree Σ≤|w|.\n▷ As we will see, for type 2 and 3 much more efficient decision procedures exists,\ni.e. we can parse such grammars efficiently.\n▷ Note that this is a purely syntactic criterion.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-sensitive (type 1) languages 80 (220)\nT L(G) is decidable if G is at least of type 1 (resp. accepted by an LBA).\n▷ Again, use BFS to enumerate the words α ∈ (V ∪ Σ)∗ with S − →k\nG α for\nincreasing k. Along any path (derivation)\nS − →G α1 − →G α2 − →G . . .\nwe have |αi| ≤ |αi+1| by definition of type 1.\nGiven some word w ∈ Σ∗ thus terminate the BFS for all |αi| with |w| < |αi|.\n! Some important results on LBAs/context-sensitive languages (w/o proofs):\n▷ Linear speedup theorem: A vector/tuple alphabet allows an LBA to actually\nuse O(|w|) space and thus also to simulate a constant number of tapes.\n▷ Savitch’s theorem: An LBA can be simulated by a deterministic 1TM that uses\nat most O(|w|2) space; it is still open if deterministic LBAs suffice.\nSo, it might be the case the deterministic LBAs can do less than\n(nondeterministic) LBAs.\n▷ Immerman–Szelepcs´ enyi theorem: for a given LBAM we can construct an\nLBA M′ with L(M′) = L(M), i.e. languages of type 1 are closed wrt.",
      "at most O(|w|2) space; it is still open if deterministic LBAs suffice.\nSo, it might be the case the deterministic LBAs can do less than\n(nondeterministic) LBAs.\n▷ Immerman–Szelepcs´ enyi theorem: for a given LBAM we can construct an\nLBA M′ with L(M′) = L(M), i.e. languages of type 1 are closed wrt.\ncomplement in contrast to languages of type 0.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-sensitive (type 1) languages 80 (221)\nT L(G) is decidable if G is at least of type 1 (resp. accepted by an LBA).\n▷ Again, use BFS to enumerate the words α ∈ (V ∪ Σ)∗ with S − →k\nG α for\nincreasing k. Along any path (derivation)\nS − →G α1 − →G α2 − →G . . .\nwe have |αi| ≤ |αi+1| by definition of type 1.\nGiven some word w ∈ Σ∗ thus terminate the BFS for all |αi| with |w| < |αi|.\n! There are infinitely many decidable languages of type 0 but not of type 1:\n▷ Informally: L ∈ NSPACE(nk) if there is some (nondeterministic) 1TM M\nwith L = L(M) using at most O(nk) space for every input w (with k ∈ N 0).\nNSPACE(nk) ⊊ NSPACE(nk+1) for every k ∈ N 0 (cf. complexity zoo).\nIf L ∈ NSPACE(nk), we can “inflate” the input L′ = {w10|w|k\n| w ∈ L} s.t.\nL′ ∈ NSPACE(n), i.e. these languages are “polynomially-bounded expansive”.\n▷ We can also bound the length of any run on input w by f(|w|): as a 1TM can\nonly write a single symbol in one step, this also bounds the space by f(|w|).",
      "| w ∈ L} s.t.\nL′ ∈ NSPACE(n), i.e. these languages are “polynomially-bounded expansive”.\n▷ We can also bound the length of any run on input w by f(|w|): as a 1TM can\nonly write a single symbol in one step, this also bounds the space by f(|w|).\n▷ Reachability in Petri nets (via TMs a special case of SRS) is decidable but\nonly in non-elementary time and space (Mayr, 1981).No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammar vs. machine 81 (222)\n! Some things are easier to do with a grammar than with a 1TM and v.v.:\nL If L1, L2 are both of type τ ∈ {0, 1, 2, 3}, then so is L1 ∪ L2.\nIf L1, L2 are both of type τ ∈ {0, 1}, then so is L1 ∩ L2.\nIf L1, L2 are both of type 1, then so is L1 \\ L2.\n▷ L1 ∪ L2:\nFor i = 1, 2 let Gi = (Vi, Σ, Pi, Si) be a grammar with Li = L(Gi).\nwlog V1 ∩ V2 = ∅ (simply tag variables with i) and S ̸∈ V1 ∪ V2 ∪ Σ.\nThen (V1 ∪ V2 ∪ {S}, Σ, P1 ∪ P2 ∪ {(S, S1), (S, S2)}, S) produces L1 ∪ L2.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammar vs. machine 81 (223)\n! Some things are easier to do with a grammar than with a 1TM and v.v.:\nL If L1, L2 are both of type τ ∈ {0, 1, 2, 3}, then so is L1 ∪ L2.\nIf L1, L2 are both of type τ ∈ {0, 1}, then so is L1 ∩ L2.\nIf L1, L2 are both of type 1, then so is L1 \\ L2.\n▷ L1 ∩ L2:\nFor i = 1, 2 let Mi be an 1TM (LBA) with Li = L(Mi).\nLet M be the following TM:\n▷ Run M1 on input w; if M1 does not accept w, loop forever.\n▷ Run M2 on input w; if M2 does not accept w, loop forever.\n▷ Accept w.\nIf M1, M2 are both LBAs, then also M will be linearly bounded.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammar vs. machine 81 (224)\n! Some things are easier to do with a grammar than with a 1TM and v.v.:\nL If L1, L2 are both of type τ ∈ {0, 1, 2, 3}, then so is L1 ∪ L2.\nIf L1, L2 are both of type τ ∈ {0, 1}, then so is L1 ∩ L2.\nIf L1, L2 are both of type 1, then so is L1 \\ L2.\n▷ L1 \\ L2:\nFor i = 1, 2 let Mi be an LBA with Li = L(Mi).\nBy the Immerman–Szelepcs´ enyi theorem we can compute an LBA M′\n2 from\nM2 with L(M′\n2) = L2, and thus “ M1; M′\n2” yields an LBA M with\nL(M) = L1 ∩ L2 = L1 \\ L.\n▷ Recall: if L and L are both recognizable/semidecidable/type 0/rec.\nenumerable, then L is decidable: but L = Σ∗ \\ L.\n▷ Moving from grammar to “machine” to grammar is conceptually a simple\napproach for computing a grammar for L(G1) \\ L(G2).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammar vs. machine 81 (225)\n! Some things are easier to do with a grammar than with a 1TM and v.v.:\nL If L1, L2 are both of type τ ∈ {0, 1, 2, 3}, then so is L1 ∪ L2.\nIf L1, L2 are both of type τ ∈ {0, 1}, then so is L1 ∩ L2.\nIf L1, L2 are both of type 1, then so is L1 \\ L2.\n▷ As we will see:\n▷ The intersection of two languages of type 2 is in general only of type 1.\n▷ Languages of type 3 are also closed wrt. boolean (set) operations.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "4 String rewriting, grammars, Chomsky hierarchy\nString rewriting and grammars: abstraction of “mathematical calculations”\nChomsky hierarchy and normal forms\nGrammars vs. Turing machines (recognizable languages)\nUndecidable problems and Post correspondence problem\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Undecidable problems for grammars of type 0 83(227)\nF As unrestricted grammars (type 0) can simulate 1TMs, the following\nproblems are undecidable for unrestricted grammars G, G1, G2 in general:\n▷ w ∈ L(G), specifically ε ∈ L(G) (see LH and LH,ε)\n▷ L(G) = Σ∗ (see LH,∗)\n▷ L(G) = ∅ (see LH,?)\n▷ L(G1) = L(G2) (see L=)\n▷ L(G1) ⊆ L(G2) (e.g. as we may choose L(G1) = Σ∗)\n▷ |L(G)| = ∞ (see LH,∗)\n▷ there exists a grammar G′ of type 1 with L(G) = L(G′). (Rice)\n▷ |L(G1) ∩ L(G2)| = ∞ (as we may choose L(G1) = Σ∗)\n▷ |L(G1) ∩ L(G2)| = 0 (as we may choose L(G1) = Σ∗)\nRemarks:\n▷ Type 3: all of these problems are decidable (later).\n▷ Type 2: ε ∈ L(G) and L(G) = ∅ are decidable (as seen); also |L(G)| = ∞.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Undecidable problems for grammars of type 0 83(228)\nF As unrestricted grammars (type 0) can simulate 1TMs, the following\nproblems are undecidable for unrestricted grammars G, G1, G2 in general:\n▷ w ∈ L(G), specifically ε ∈ L(G) (see LH and LH,ε)\n▷ L(G) = Σ∗ (see LH,∗)\n▷ L(G) = ∅ (see LH,?)\n▷ L(G1) = L(G2) (see L=)\n▷ L(G1) ⊆ L(G2) (e.g. as we may choose L(G1) = Σ∗)\n▷ |L(G)| = ∞ (see LH,∗)\n▷ there exists a grammar G′ of type 1 with L(G) = L(G′). (Rice)\n▷ |L(G1) ∩ L(G2)| = ∞ (as we may choose L(G1) = Σ∗)\n▷ |L(G1) ∩ L(G2)| = 0 (as we may choose L(G1) = Σ∗)\nRemarks:\n▷ Type 1: L(G) = ∅ and |L(G)| = ∞ are undecidable.\n▷ We cannot remove ε-rules, unproductive or unreachable nonterminals for type\n0, 1 in general.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Post correspondence problem 84 (229)\nD An instance of the Post correspondence problem (PCP) (cf. Emil Post)\nconsists of a finite number of word pairs\n(u(1), v(1)), . . . ,(u(n), v(n)) ∈ Σ∗ × Σ∗\nand asks whether there is a nonempty sequence i1, i2 . . . , il ∈ [n] (l >1) s.t.\nu(i1)u(i2) . . . u(il) = v(i1)v(i2) . . . v(il)\n! A PCP instance has either no or infinitely many solutions.\n▷ E.g. consider the PCP instance\n(u(1), v(1)) := (1, 101) ( u(2), v(2)) := (10, 00) ( u(3), v(3)) := (011, 11)\nA solution is 1, 3, 2, 3 as:\nu(1)u(3)u(2)u(3) = 1 011 10 011 = 101110011\nv(1)v(3)v(2)v(3) = 101 11 00 11 = 101110011\n▷ Try to solve the following using a computer (or cf. wikipedia (de))\n(001, 0) (01 , 011) (01 , 101) (10 , 001)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Post correspondence problem and grammars 85 (230)\nT For every (deterministic) 1TM M we can compute a PCP instance\n(u(1), v(1)), . . . ,(u(n), v(n)) ∈ Σ∗ × Σ∗\nso that ε ∈ L(M) iff the PCP instance has a solution.\n▷ The main idea is that we use both components to simulate the same run\nq0ε − →G α1q1β1 − →G . . .− →G αlqlβl − →G qf ε\nwhere the run is encoded as: (“#” as delimiter; wlog the TM empties its tape)\n#q0#α1q1β1# . . .#αlqlβl#qf #\nThe central idea is to let the first component trail behind by 1 step:\n\u0014 #\n#q0#\n\u0015\u0014 q0#\nα1q1β1#\n\u0015\n. . .\n\u0014 αiqiβi#\nαi+1qi+1βi+1#\n\u0015\n=\n\u0014 #q0# . . .#αiqiβi#\n#q0# . . .#αiqiβi#αi+1qi+1βi+1#\n\u0015\nwhere the word pairs are represented as vectors.\nAs the first component trails behind, we can choose the PCP word pairs so\nthat the upper (first) component “reads qia from below” in order to correctly\nencode the successor in the lower component again.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Post correspondence problem and grammars 85 (231)\nT For every (deterministic) 1TM M we can compute a PCP instance\n(u(1), v(1)), . . . ,(u(n), v(n)) ∈ Σ∗ × Σ∗\nso that ε ∈ L(M) iff the PCP instance has a solution.\n▷ This leads to the following PCP instance (wlog M empties its tape, F = {qf }):\n{(#, #q0#), (qf ##, #), (aqf , qf ), (qf a, qf ) | a ∈ Γ}\n∪ {(x, x) | x ∈ Γ ∪ {#}}\n∪ {(qa, q′a′) | ((q, a), (q′, a′, 0)) ∈ δ}\n∪ {(q#, q′a′#) | ((q, □), (q′, a′, 0)) ∈ δ}\n∪ {(qa, a′q′) | ((q, a), (q′, a′, +1)) ∈ δ}\n∪ {(q#, a′q′#) | ((q, □), (q′, a′, +1)) ∈ δ}\n∪ {(bqa, q′ba′) | ((q, a), (q′, a′, −1)) ∈ δ, b∈ Γ}\n∪ {(bq#, q′ba′#) | ((q, □), (q′, a′, −1)) ∈ δ, b∈ Γ}\n∪ {(#qa, #q′□a′) | ((q, a), (q′, a′, −1)) ∈ δ}\nWe still need to enforce that a solution starts with (#, #q0#):\nThe main idea is to replace each pair (u(i), v(i)) = (a1 . . . ak, b1 . . . , bl) by\n(a1|a2|. . .|ak|, |b1|b2|. . .|bl) except for (#, #q0#) which becomes",
      "∪ {(#qa, #q′□a′) | ((q, a), (q′, a′, −1)) ∈ δ}\nWe still need to enforce that a solution starts with (#, #q0#):\nThe main idea is to replace each pair (u(i), v(i)) = (a1 . . . ak, b1 . . . , bl) by\n(a1|a2|. . .|ak|, |b1|b2|. . .|bl) except for (#, #q0#) which becomes\n(|#|, |#|q0|#); and add the additional pair ($, |$) as “terminator” (cf. Sipser).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Post correspondence problem and grammars 85 (232)\nT For every (deterministic) 1TM M we can compute a PCP instance\n(u(1), v(1)), . . . ,(u(n), v(n)) ∈ Σ∗ × Σ∗\nso that ε ∈ L(M) iff the PCP instance has a solution.\nC The PCP is only semidecidable, but not decidable.\n! An PCP instance can be described as the intersection of two cf. languages:\n▷ Assume that a1, . . . , an, # ̸∈ Σ and add them to Σ.\nDefine the two grammars Gu, Gv as follows ( i = 1, 2 . . . , l)\nSu − →G u(i)Suai | u(i)#ai resp. Sv − →G v(i)Svai | v(i)#ai\nL(Gu) ∩ L(Gv) = {u(i1) . . . u(il)#ail . . . ai1 | i1, . . . , il is a solution }\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Post correspondence problem and grammars 85 (233)\nT For every (deterministic) 1TM M we can compute a PCP instance\n(u(1), v(1)), . . . ,(u(n), v(n)) ∈ Σ∗ × Σ∗\nso that ε ∈ L(M) iff the PCP instance has a solution.\n▷ Recall: a PCP instance has one solution iff it has infinitely many solutions.\nC Already for (linear) context-free grammars G1, G2 it is undecidable if\n▷ L(G1) ∩ L(G2) = ∅ resp. |L(G1) ∩ L(G2)| = 0\n▷ |L(G1) ∩ L(G2)| < ∞ resp. |L(G1) ∩ L(G2)| = ∞\nC For a context-sensitive grammar G it is undecidable if\n▷ L(G) = ∅ resp. |L(G)| = 0\n▷ |L(G)| < ∞ resp. |L(G)| = ∞\nas the intersection L(G1) ∩ L(G2) of grammars of type 2 is still of type 1.\n▷ These problems are still decidable for a single context-free grammar (using\ngraph reachability).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Post correspondence problem and grammars 85 (234)\nT For every (deterministic) 1TM M we can compute a PCP instance\n(u(1), v(1)), . . . ,(u(n), v(n)) ∈ Σ∗ × Σ∗\nso that ε ∈ L(M) iff the PCP instance has a solution.\nC Let g, h: Γ∗ → Σ∗ be two monoid homomorphisms with |Γ| < ∞.\n▷ i.e. h(uv) = h(u)h(v) and h(ε) = ε and analogously for g.\nThen it is undecidable if g(w) = h(w) for some w ∈ Γ∗.\n! Some remarks:\n▷ If |Σ| = 1, then every PCP can be decided.\n▷ Every PCP instance with |Σ| > 2 can be “inflated” to a PCP instance with\n|Σ| = 2 using unary encoding (e.g. {a1, . . . , al} → {0, 1}, ai 7→ 01i).\n▷ Every PCP instance with only n = 2 word pairs is decidable.\n▷ PCP instances with at least n ≥ 5 word pairs are undecidable in general.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "5 Regular languages\nRegular grammars and finite automata\nMoving between regular grammars and finite automata\nClosure properties\nRegular expressions\nMinimizing DFAs and Myhill-Nerode\nBasics of probabilistic finite automata (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular languages and regular grammars (reminder) 88 (237)\nD A grammar G = (V, Σ, P, S) is regular if P ⊆ V × ({ε} ∪Σ ∪ V ∪ ΣV ).\n▷ Recall: the Chomsky hierarchy requires P ⊆ V × (Σ ∪ ΣV ), but ε-rules only\nadd ε to L(G) and both ε-rules and chain rules can be removed.\nA language L ⊆ Σ∗ is regular if L = L(G) for some regular grammar G.\n▷ Regular grammars can be conveniently represented as labeled directed graphs\n(formally: nondeterministic finite automata with ε-transitions), e.g.\nV = {S, T} S − →G aS | T T − →G bT | S | ε\nas\nS T\na\nε\nb\nε\nor S T qf\na\nε\nb\nε\nε\nwhich can be understood as the succinct representation of a 1TM that reads\nthe input completely from left to right till the first blank □ is read.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular languages and regular grammars (reminder) 88 (238)\nD A grammar G = (V, Σ, P, S) is regular if P ⊆ V × ({ε} ∪Σ ∪ V ∪ ΣV ).\n▷ Recall: the Chomsky hierarchy requires P ⊆ V × (Σ ∪ ΣV ), but ε-rules only\nadd ε to L(G) and both ε-rules and chain rules can be removed.\nA language L ⊆ Σ∗ is regular if L = L(G) for some regular grammar G.\n▷ Regular grammars can be conveniently represented as labeled directed graphs\n(formally: nondeterministic finite automata with ε-transitions), e.g.\nV = {S, T} S − →G aS | T T − →G bT | S | ε\nas\nS T qf\na: a/+ 1\na: a/0; b: b/0\nb: b/+ 1\na: a/0; b: b/0\n□: □/0\nwhich can be understood as the succinct representation of a 1TM that reads\nthe input completely from left to right till the first blank □ is read.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Finite automata 89 (239)\nD Finite automata (FA) (also: finite-state machine (FSM)): (Q, Σ, δ, q0, F)\n▷ Q: finite set of states\n▷ Σ: the finite input alphabet, e.g. Σ = {0, 1}\n▷ q0: the initial state with q0 ∈ Q\n▷ F: the final states with F ⊆ Q\n▷ δ: the transition rules with δ ⊆ (Q × (Σ ∪ {ε})) × Q.\nA transition rule ((q, a), r) ∈ δ with a ∈ Σ stands for the instruction:\nif state == ’q’ and tape[pos] == ’a’:\nstate = ’r’; pos += 1 # head moves to the right\nAn ε-transition rule ((q, ε), r) ∈ δ stands for the instruction:\nif state == ’q’:\nstate = ’r’ # head does not move\n(A 1TM does not have ε-transitions.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "FA: configuration, run, accepted language 90 (240)\nD Let M = (Q, Σ, δ, q0, F) be an FA.\nAn FA is almost a 1TM except that\n▷ it must not write to the tape,\n▷ it must read/consume its input completely from left to right,\n▷ it may continue from a final state.\n(If we introduce an “end of input” marker, then the definitions for 1TMs can be directly\nre-used.)\nA configuration\nqβ with q ∈ Q, β∈ Σ∗\nof an FA consists simply of the current state q and the remaining input β.\n▷ Because of its restrictions, the future behavior of an FA can only depend on\nthe current state q and the remaining input β.\n▷ This is sometimes called “memoryless” (or “markovian”).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "FA: configuration, run, accepted language 90 (241)\nD Let M = (Q, Σ, δ, q0, F) be an FA.\nThe rules δ generate the binary relation − →M on Q × Σ∗:\n− →M := {(qaβ, rεβ) | ((q, a\n∈Σ∪{ε}\n), r) ∈ δ, β∈ Σ∗}\nA run of the FA is a path q0β0 − →M q1β1 − →M . . .wrt. − →M .\n▷ A run on input x1 . . . xl ∈ Σl is a path wrt. − →M starting at q0x1 . . . xl\n! Infinite runs can only arise from ε-transitions in case of FAs.\n▷ δ is deterministic iff every configuration has at most one successor wrt. − →M\niff for every configuration there is exactly one maximal run.\nThe language accepted/recognized by M is\nL(M) = {w ∈ Σ∗ | q0w − →∗\nM qf ε for some qf ∈ F}\ni.e. as before it suffices it there is at least one run that is accepting, but now\nthe input has to be read completely from left to right; note that the run is\nnot required to be maximal which only mattes in case of ε-transitions.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "FA: configuration, run, accepted language 90 (242)\nD Let M = (Q, Σ, δ, q0, F) be an FA.\nThe rules δ generate the binary relation − →M on Q × Σ∗:\n− →M := {(qaβ, rεβ) | ((q, a\n∈Σ∪{ε}\n), r) ∈ δ, β∈ Σ∗}\nA run of the FA is a path q0β0 − →M q1β1 − →M . . .wrt. − →M .\nAs ε-transitions can also lead to nondeterminism, the following abbreviations\nfor restricted sublasses of FAs are used:\n▷ ε-NFA: an FA with ε-transitions, i.e. a general FA.\n▷ NFA: an FA without ε-transitions.\n▷ DFA: an FA without ε-transitions and |(q, a)δ| ≤1 for all (q, a) ∈ Q × Σ,\ni.e. for every state q there is at most one r with qa − →M r (a-successor).\n▷ complete DFA: a DFA with |(q, a)δ| = 1 for all (q, a) ∈ Q × Σ,\ni.e. for every state q there is exactly one r with qa − →M r.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: “ends with bc” 91 (243)\nΣ = {a, b} S − →G aS | bS | cS | bT T − →G cU U − →G ε\nSstart T U\na, b, c\nb c\nS\n. . .□ a b b c □ . . .\nSabbc − →M Sbbc − →M T bc (wrong guess)\n▷ FA “=” no-write-no-going-back-1TM\n▷ Behavior depends only on state and remaining input\n▷ Nondeterminism can “guess” when to read the last two letters, but might\nguess wrong.\n▷ Regular grammar writes letters (S − →G aS), FA reads letters (Sa − →M S)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: “ends with bc” 91 (244)\nΣ = {a, b} S − →G aS | bS | cS | bT T − →G cU U − →G ε\nSstart T U\na, b, c\nb c\nS\n. . .□ a b b c □ . . .\nSabbc − →M Sbbc − →M T bc (wrong guess)\n▷ FA “=” no-write-no-going-back-1TM\n▷ Behavior depends only on state and remaining input\n▷ Nondeterminism can “guess” when to read the last two letters, but might\nguess wrong.\n▷ Regular grammar writes letters (S − →G aS), FA reads letters (Sa − →M S)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: “ends with bc” 91 (245)\nΣ = {a, b} S − →G aS | bS | cS | bT T − →G cU U − →G ε\nSstart T U\na, b, c\nb c\nT\n. . .□ a b b c □ . . .\nSabbc − →M Sbbc − →M T bc (wrong guess)\n▷ FA “=” no-write-no-going-back-1TM\n▷ Behavior depends only on state and remaining input\n▷ Nondeterminism can “guess” when to read the last two letters, but might\nguess wrong.\n▷ Regular grammar writes letters (S − →G aS), FA reads letters (Sa − →M S)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: “ends with bc” 91 (246)\nΣ = {a, b} S − →G aS | bS | cS | bT T − →G cU U − →G ε\nSstart T U\na, b, c\nb c\nS\n. . .□ a b b c □ . . .\nSabbc − →M Sbbc − →M Sbc − →M T c− →M U (correct guess)\n▷ FA “=” no-write-no-going-back-1TM\n▷ Behavior depends only on state and remaining input\n▷ Nondeterminism can “guess” when to read the last two letters, but might\nguess wrong.\n▷ Regular grammar writes letters (S − →G aS), FA reads letters (Sa − →M S)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: “ends with bc” 91 (247)\nΣ = {a, b} S − →G aS | bS | cS | bT T − →G cU U − →G ε\nSstart T U\na, b, c\nb c\nS\n. . .□ a b b c □ . . .\nSabbc − →M Sbbc − →M Sbc − →M T c− →M U (correct guess)\n▷ FA “=” no-write-no-going-back-1TM\n▷ Behavior depends only on state and remaining input\n▷ Nondeterminism can “guess” when to read the last two letters, but might\nguess wrong.\n▷ Regular grammar writes letters (S − →G aS), FA reads letters (Sa − →M S)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: “ends with bc” 91 (248)\nΣ = {a, b} S − →G aS | bS | cS | bT T − →G cU U − →G ε\nSstart T U\na, b, c\nb c\nS\n. . .□ a b b c □ . . .\nSabbc − →M Sbbc − →M Sbc − →M T c− →M U (correct guess)\n▷ FA “=” no-write-no-going-back-1TM\n▷ Behavior depends only on state and remaining input\n▷ Nondeterminism can “guess” when to read the last two letters, but might\nguess wrong.\n▷ Regular grammar writes letters (S − →G aS), FA reads letters (Sa − →M S)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: “ends with bc” 91 (249)\nΣ = {a, b} S − →G aS | bS | cS | bT T − →G cU U − →G ε\nSstart T U\na, b, c\nb c\nT\n. . .□ a b b c □ . . .\nSabbc − →M Sbbc − →M Sbc − →M T c− →M U (correct guess)\n▷ FA “=” no-write-no-going-back-1TM\n▷ Behavior depends only on state and remaining input\n▷ Nondeterminism can “guess” when to read the last two letters, but might\nguess wrong.\n▷ Regular grammar writes letters (S − →G aS), FA reads letters (Sa − →M S)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: “ends with bc” 91 (250)\nΣ = {a, b} S − →G aS | bS | cS | bT T − →G cU U − →G ε\nSstart T U\na, b, c\nb c\nU\n. . .□ a b b c □ . . .\nSabbc − →M Sbbc − →M Sbc − →M T c− →M U (correct guess)\n▷ FA “=” no-write-no-going-back-1TM\n▷ Behavior depends only on state and remaining input\n▷ Nondeterminism can “guess” when to read the last two letters, but might\nguess wrong.\n▷ Regular grammar writes letters (S − →G aS), FA reads letters (Sa − →M S)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: “ends with bc” (DFA) 92 (251)\nSstart T U\na, b, c\nb c\npstart q r\na, c\nb c\na, c\nba\nb\ndef p(w):\nx = w[0]\nif x in [’a’,’c’]:\nreturn p(w[1:])\nif x == ’b’:\nreturn q(w[1:])\nreturn False\ndef q(w):\nx = w[0]\nif x == ’a’:\nreturn p(w[1:])\nif x == ’b’:\nreturn q(w[1:])\nif x == ’c’:\nreturn r(w[1:])\nreturn False\ndef r(w):\nif w == ’’:\nreturn True\nx = w[0]\nif x == ’b’:\nreturn q(w[1:])\nif x in [’a’,’c’]:\nreturn p(w[1:])\nreturn False\n▷ Nondeterminism can lead to much simpler/smaller FAs, whereas DFAs can be\neasily translated e.g. into tail-recursive programs.\n▷ The DFA is essentially the call graph; recall that tail-recursion is directly\nequivalent to a while-loop.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "FA vs. 1TM 93 (252)\n▷ As in the case of 1TMs any kind of finite information can be encoded in the\nstates, e.g. also a “work tape” of fixed length L, roughly:\n▷ Every possible content of this work tape is described by ΓL.\nAs this is a finite set, we can use it as states for an FA.\n(We still need an unbounded “input tape” which is read-only, and has to be read\nfrom left to right.)\n▷ For instance, controllers/drivers/RNNs can be described by FAs.\n▷ Output can e.g. be encoded in the states or modeled by means of interleaving\ninput and output ( i1o1i2o2i3o3 . . . ilol).\n▷ FAs with output are called finite-state transducers. Special cases: Mealy\nmachines, Moore machines, recurrent NNs (finite Σ & Q due to quantization) .\nIn practice, a central question is the synthesis problem, e.g.:\n▷ Given: some definition of a regular language L describing/specifying the\nbehavior of the system.\n▷ Goal: compute a minimal DFA M with L(M) = L, i.e. M is an\nimplementation/realization of the specification.",
      "In practice, a central question is the synthesis problem, e.g.:\n▷ Given: some definition of a regular language L describing/specifying the\nbehavior of the system.\n▷ Goal: compute a minimal DFA M with L(M) = L, i.e. M is an\nimplementation/realization of the specification.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Turing on finite memory 94 (253)\n[...] Consider first the more accurate form of the question. I believe that\nin about fifty years’ time it will be possible to programme computers,\nwith a storage capacity of about 109, to make them play the imitation\ngame so well that an average interrogator will not have more than 70\nper cent, chance of making the right identification after five minutes of\nquestioning. The original question, ‘Can machines think!’ I believe to be\ntoo meaningless to deserve discussion. Nevertheless I believe that at the\nend of the century the use of words and general educated opinion will\nhave altered so much that one will be able to speak of machines thinking\nwithout expecting to be contradicted. I believe further that no useful\npurpose is served by concealing these beliefs. [...] (A. Turing, “COMPUTING\nMACHINERY AND INTELLIGENCE”, Mind, 1950)\n(Cf. ChatGPT based on GPT-3 which requires 800GB of storage.)",
      "without expecting to be contradicted. I believe further that no useful\npurpose is served by concealing these beliefs. [...] (A. Turing, “COMPUTING\nMACHINERY AND INTELLIGENCE”, Mind, 1950)\n(Cf. ChatGPT based on GPT-3 which requires 800GB of storage.)\n(Related interactive polynomial time: “PSPACE is almighty for PTIME”)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "FAs and rational numbers 95 (254)\ndef binrep(x: float):\nassert 0 <= x < 1\nyield ’.’\nwhile True:\nx = 2 * x\nif x >= 1:\nx -= 1\nyield ’1’\nelse:\nyield ’0’\nJ.1001K2 = P∞\ni=0(2−1+i·4 + 2−4+i·4)\n= (2 −1 + 2−4) · P∞\ni=0(2−4)i\n= (2 −1 + 2−4) 1\n1−2−4\n= 23+20\n24−1 = 9\n15 = 3\n5 = J.6K10\n0.6start 1.2 0.4 0.8 1.6. 1 0 0\n1\n▷ For a fixed rational number x ∈ [0, 1) the program becomes an FA that\naccepts the prefixes of the binary representation of x. (Similarly, for other bases.)\n▷ Regular languages are closely related to rational numbers Q .\n! As for TMs: we can always add a sink/trap/rejecting state to complete a\nDFA.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "NDDs: FAs for compression of infinite sets 96 (255)\nJx0x1 . . . xlK2 − Jy0y1 . . . ylK2 ≤ c\niff (x0 + 2Jx1 . . . xlK2) − (y0 + 2Jy1 . . . ylK2) ≤ c\niff Jx1 . . . xlK2 − Jy1 . . . ylK2 ≤ ⌊c−x0+y0\n2 ⌋\n1start 0 −100,10,11\n01\n10\n00,01,11\n01\n00,10,11\n▷ FAs can represent solutions of linear inequalities like ax + by ≤ c over Z for\nfixed coefficients a, b, c∈ Z using LSBF or MSBF encoding and bit\ntuples/vectors as alphabet.\n▷ States remember the “slack”.\n▷ I.e. FAs can be used as finite representation of certain infinite sets of integers.\n▷ Using the closure properties of regular languages, this is the basis for a\ndecision procedure for Presburger arithmetic.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "BBDs: FAs for compression of finite sets 97 (256)\nw ∧ (¬x ∨ (¬y ∧ z)) {10}{0, 1}2 ∪ {11}{01}\nstart 1\n0\n0,1\n0,1\n1\n0\n1\n▷ FAs are also used to compress e.g. finite sets of bit vectors (cf. BDD).\n▷ E.g. the exponential set {0, 1}n can be represented by a DFA with n states.\n▷ The closure properties of regular languages allow to efficiently work with the\ncompressed representation which is e.g. used in the verification of software\nand hardware.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: Suffix trees 98 (257)\nstart\n5 3 1\n4 2\n0\na n a n a\n$ $ $\nn\na n a\n$ $\nb\na n a n a\n$\n▷ Suffix trees are finite state transducers that recognize the suffixes of a given\nword w and encode in their final states the position at which a suffix starts.\n▷ “$” is used as “end of string” marker again.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: Markov language models 99 (258)\npstart q r\na: 0.2, c: 0.3\nb: 0.5 c: 0.1\na: 0.6\nb: 0.3\na: 0.8,c: 0.1\nb: 0.1\n▷ DFAs can be used to assign probabilities to words resp. to randomly generate\nwords from the accepted language:\n▷ The “random surfer” starts in the initial state q0 and generates a “random\nwalk” q0q1q2 . . .by choosing in state qi a letter a ∈ Σ with the defined\nconditional probability.\n▷ An explicit “end of word”/“terminator” symbol like $ can be used to explicitly\nterminate the generating process.\n▷ This allows e.g. to encode that a blank □ is more likely to follow after “the”\nthen e.g. the letter l.\n▷ The stationary distribution (or more generally, the Cesaro limit) yield the\naverage distribution of letters Σ.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Examples: Bots (Controllers, Drivers, . . . ) 100 (259)\n▷ Animation and behavior of “non player characters” and “bots” is often\ndescribed using finite automatas (=finite state machines).\n▷ E.g. in Unity3d and in Unreal (image source).\n▷ In general, FAs are used to describe any kind of systems with only finitely\nmany states like vending machines, clocks, controllers/robots, protocols, . . . .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "5 Regular languages\nRegular grammars and finite automata\nMoving between regular grammars and finite automata\nClosure properties\nRegular expressions\nMinimizing DFAs and Myhill-Nerode\nBasics of probabilistic finite automata (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Of regular grammars and ε-NFA 102 (261)\nF From every regular grammar G (without ε-rules and chain rules) we can\ncompute an ε-NFA (resp. NFA) M with L(G) = L(M), and vice versa.\n▷ Informally:\nA regular grammar describes how to construct a “regularly structured” word,\nwhereas the FA describes how to check if a word is “regularly structured”.\nWe thus simply need to turn “read a” into “produce a” and v.v.:\n▷ A production rule X − →G aY becomes the transition rule Xa − →M Y\nA production rule X − →G a becomes the transition rule Xa − →M qf\nwith qf the only final state of M.\n▷ A transition rule pa − →M q becomes the production rule p − →G aq;\nif q ∈ F is final, then also add the production rule p − →G a.\n▷ I.e. states and nonterminals essentially coincide (except for the introduction of\nqf ).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Of regular grammars and ε-NFA 102 (262)\nF From every regular grammar G (without ε-rules and chain rules) we can\ncompute an ε-NFA (resp. NFA) M with L(G) = L(M), and vice versa.\n▷ Let G = (V, Σ, P, S) be a regular grammar (with ε-rules).\nThe ε-NFA is M = (V ∪ {qf }, Σ, δ, S, F) with\nF := {qf } ∪ {X | (X, ε) ∈ P} (X − →G ε and stop)\nδ := {((X, a), Y) | (X, aY) ∈ P} (X − →G aY )\n∪ { ((X, a), qf ) | (X, a) ∈ P} (X − →G a and stop)\n∪ { ((X, ε), Y) | (X, Y) ∈ P, a∈ Σ} (X − →G Y )\nwhere we assume that qf ̸∈ V .\nBy construction: if G has no chain rules, then M has no ε-transitions,\n▷ At the cost of additional ε-transitions, a single final state suffices:\nF := {qf }\nδ := {((X, a), Y) | (X, aY) ∈ P} (X − →G aY )\n∪ { ((X, a), qf ) | (X, a) ∈ P} (X − →G a and stop)\n∪ { ((X, ε), Y) | (X, Y) ∈ P, a∈ Σ} (X − →G Y )\n∪ { ((X, ε), qf ) | (X, ε) ∈ P} (X − →G ε and stop)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Of regular grammars and ε-NFA 102 (263)\nF From every regular grammar G (without ε-rules and chain rules) we can\ncompute an ε-NFA (resp. NFA) M with L(G) = L(M), and vice versa.\n▷ Let M = (Q, Σ, δ, q0, F) be an ε-NFA (with ε-rules).\nThe regular grammar is G = (Q, Σ, P, q0) with\nP := {(q, ar) | ((q, a), r) ∈ δ} (qa − →M r)\n∪ { (q, a) | ((q, a), qf ) ∈ δ, qf ∈ F} (qa − →M qf with qf ∈ F)\n∪ { (q, r) | ((q, ε), r) ∈ δ} (q − →M r)\n∪ { (q, ε) | ((q, ε), qf ) ∈ δ, qf ∈ F} (q − →M qf with qf ∈ F)\n∪ { (q, ε) | ((q, ε), qf ) ∈ δ, qf ∈ F} (q − →M qf with qf ∈ F)\n∪ { (q0, ε) | q0 ∈ F} (only if q0 ∈ F)\nBy construction: if M has no ε-transitions, then q0 − →G ε is the only potential\nε-rule in G.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Of regular grammars and ε-NFA 102 (264)\nF From every regular grammar G (without ε-rules and chain rules) we can\ncompute an ε-NFA (resp. NFA) M with L(G) = L(M), and vice versa.\nC For every ε-NFA we can construct an NFA which accepts the same language.\n▷ E.g. (i) represent the ε-NFA M as a regular grammar G, (ii) remove chain\nrules in G to obtain G′, (iii) represent G′ as NFA M′.\n▷ Exercise: combine these steps into a single algorithm.\n(The main advantage of ε-NFA is that they are less restricted and thus usually more\nconvenient to “draw”.)\n▷ Example: from regular grammar with chain rules to ε-NFA.\nG: S → aS | T | ε T − →G aS | bT | b\nSstart T\nqf\na b\nε\na\nb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Of regular grammars and ε-NFA 102 (265)\nF From every regular grammar G (without ε-rules and chain rules) we can\ncompute an ε-NFA (resp. NFA) M with L(G) = L(M), and vice versa.\nC For every ε-NFA we can construct an NFA which accepts the same language.\n▷ E.g. (i) represent the ε-NFA M as a regular grammar G, (ii) remove chain\nrules in G to obtain G′, (iii) represent G′ as NFA M′.\n▷ Exercise: combine these steps into a single algorithm.\n(The main advantage of ε-NFA is that they are less restricted and thus usually more\nconvenient to “draw”.)\n▷ Example: Removing chain rules in\nG: S → aS | T | ε T − →G aS | bT | b\nyields (replace S − →G T by S − →G aS | bT | b)\nG′ : S → aS | bT | b | ε T − →G aS | bT | b\n(Removing ε-rules in G is the same as contracting ε-transitions in the FA (=call graph)) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Of regular grammars and ε-NFA 102 (266)\nF From every regular grammar G (without ε-rules and chain rules) we can\ncompute an ε-NFA (resp. NFA) M with L(G) = L(M), and vice versa.\nC For every ε-NFA we can construct an NFA which accepts the same language.\n▷ E.g. (i) represent the ε-NFA M as a regular grammar G, (ii) remove chain\nrules in G to obtain G′, (iii) represent G′ as NFA M′.\n▷ Exercise: combine these steps into a single algorithm.\n(The main advantage of ε-NFA is that they are less restricted and thus usually more\nconvenient to “draw”.)\n▷ Example: Without chain rules, we obtain an NFA from\nG′ : S → aS | bT | b | ε T − →G aS | bT | b\nSstart T\nqf\na b\nε\na\nb\nbecomes Sstart T\nqf\na b\nb\na\nb b\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From NFA to DFA 103 (267)\nT From an NFA M = (Q, Σ, δ, q0, F) we can compute a DFA\nM′ = (Q′, Σ, δ′, q′\n0, F′) with L(M) = L(M′).\n▷ As M is an NFA, it reads its input strictly one letter at a time from left to\nright, and every run on w = a1 . . . al has the form\nq0a1a2 . . . al − →M q1a2 . . . al − →M . . .\nand it is accepting iff ql ∈ F. (A run might also deadlock.)\nWe can thus superpose all possible runs by tracking the states we have\nreached after reading a1 . . . ai e.g. using sets of states or, more efficiently,\nusing bit vectors.\nq0start q1 q2\na, b, c\nb c\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From NFA to DFA 103 (268)\nT From an NFA M = (Q, Σ, δ, q0, F) we can compute a DFA\nM′ = (Q′, Σ, δ′, q′\n0, F′) with L(M) = L(M′).\n▷ As M is an NFA, it reads its input strictly one letter at a time from left to\nright, and every run on w = a1 . . . al has the form\nq0a1a2 . . . al − →M q1a2 . . . al − →M . . .\nand it is accepting iff ql ∈ F. (A run might also deadlock.)\nWe can thus superpose all possible runs by tracking the states we have\nreached after reading a1 . . . ai e.g. using sets of states or, more efficiently,\nusing bit vectors.\nq0start q1 q2\na, b, c\nb c\nq0babc − →M q0abc − →M q0bc − →M q0c − →M q0\nq0babc − →M q1abc\nq0babc − →M q0abc − →M q0bc − →M q1c\nq0babc − →M q0abc − →M q0bc − →M q1c − →M q2\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From NFA to DFA 103 (269)\nT From an NFA M = (Q, Σ, δ, q0, F) we can compute a DFA\nM′ = (Q′, Σ, δ′, q′\n0, F′) with L(M) = L(M′).\n▷ As M is an NFA, it reads its input strictly one letter at a time from left to\nright, and every run on w = a1 . . . al has the form\nq0a1a2 . . . al − →M q1a2 . . . al − →M . . .\nand it is accepting iff ql ∈ F. (A run might also deadlock.)\nWe can thus superpose all possible runs by tracking the states we have\nreached after reading a1 . . . ai e.g. using sets of states or, more efficiently,\nusing bit vectors.\nq0start q1 q2\na, b, c\nb c\n\"1\n0\n0\n#\nbabc − →M\n\"1\n1\n0\n#\nabc − →M\n\"1\n0\n0\n#\nbc − →M\n\"1\n1\n0\n#\nc − →M\n\"1\n0\n1\n#\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From NFA to DFA 103 (270)\nT From an NFA M = (Q, Σ, δ, q0, F) we can compute a DFA\nM′ = (Q′, Σ, δ′, q′\n0, F′) with L(M) = L(M′).\n▷ As M is an NFA, it reads its input strictly one letter at a time from left to\nright, and every run on w = a1 . . . al has the form\nq0a1a2 . . . al − →M q1a2 . . . al − →M . . .\nand it is accepting iff ql ∈ F. (A run might also deadlock.)\nWe can thus superpose all possible runs by tracking the states we have\nreached after reading a1 . . . ai e.g. using sets of states or, more efficiently,\nusing bit vectors.\nq0start q1 q2\na, b, c\nb c\nq100start q110 q101\na, c\nb c\na, c\nba\nb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From NFA to DFA 103 (271)\nT From an NFA M = (Q, Σ, δ, q0, F) we can compute a DFA\nM′ = (Q′, Σ, δ′, q′\n0, F′) with L(M) = L(M′).\n▷ As M is an NFA, it reads its input strictly one letter at a time from left to\nright, and every run on w = a1 . . . al has the form\nq0a1a2 . . . al − →M q1a2 . . . al − →M . . .\nand it is accepting iff ql ∈ F. (A run might also deadlock.)\nWe can thus superpose all possible runs by tracking the states we have\nreached after reading a1 . . . ai e.g. using sets of states or, more efficiently,\nusing bit vectors.\nThis leads to the “brute-force” definition called power-set construction:\nQ′ := 2 Q = {C ⊆ Q} ∼= {0, 1}|Q|\nδ′ := {((C, a), {r | ((q, a), r) ∈ δ, q∈ C, C∈ 2Q})}\nq′\n0 := {q0}\nF′ := {C ⊆ Q | C ∩ F ̸= ∅}\n(Do not take this literally, use BFS or DFS instead!)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From NFA to DFA 104 (272)\ndef bfs_power_set_construction(Q,Σ,δ,q0, F):\nC = {q0} # current states we might be in\ntodo = {C}\nQ′ = todo # copy\nδ′ = ∅\nwhile todo ̸= ∅:\nnext_todo = ∅\nfor C ∈ todo:\nfor a ∈ Σ:\nN = ∅ # superposition of all possible successors wrt. a\nfor q ∈ C:\nfor r ∈ Q:\nif ((q, a), r) ∈ δ:\nN = N ∪ {r}.\nδ′ = δ′ ∪ {((C, a), N)}\nif N ̸∈ Q′:\nQ′ = Q′ ∪ {N}\nnext_todo = next_todo ∪ {C}\ntodo = next_todo\nreturn (Q′, Σ, δ′, {q0}, {C ∈ Q′ | C ∩ F ̸= ∅})\n! This extends to NFA with a set of I ⊆ Q of initial states: simply start the\nBFS from I instead of {q0}.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "5 Regular languages\nRegular grammars and finite automata\nMoving between regular grammars and finite automata\nClosure properties\nRegular expressions\nMinimizing DFAs and Myhill-Nerode\nBasics of probabilistic finite automata (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Closure properties 106 (274)\nC Regular languages are closed wrt. complement, union, intersection, difference,\nconcatenation and Kleene star:\n▷ Complement: Let M = (Q, Σ, δ, q0, F) be a DFA with L = L(M).\nAs δ might not be defined for all a ∈ Σ, add a sink/trap state:\nQ := Q ∪ {q⊥} δ := δ ∪ {((q, a), q⊥): |(q, a)δ| = 0, q∈ Q, a∈ Σ}\nIn case of a deadend, we now move to q⊥ and loop there forever.\nNow, define M′ by simply making final states nonfinal and v.v.:\nM′ = (Q, Σ, δ, q0, Q\\ F)\nAs M is deterministic, so is M′, and thus there is for every word exactly one\nrun. By construction: a run in M is accepting iff it is rejecting in M′.\nstart a start ⊥a a\na\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Closure properties 106 (275)\nC Regular languages are closed wrt. complement, union, intersection, difference,\nconcatenation and Kleene star:\n▷ Union: let Mi = (Qi, Σ, δi, q0,i, Fi) be FA with Li = L(Mi) for i = 1, 2.\n(Assume Q1 ∩ Q2 = ∅.)\nAdd a new initial state that can nondeterministically choose to run M1 or M2:\n(Q1 ∪ Q2 ∪ {q0}, Σ, δ1 ∪ δ2 ∪ {((q0, ε), q0,1), ((q0, ε), q0,2)}, q0, F1 ∪ F2)\n(The same approach as for grammars.)\nstart a\nstart b\na, b a, b\na, b a, b start\na\nb\nε\nε\na, b a, b\na, b a, b\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Closure properties 106 (276)\nC Regular languages are closed wrt. complement, union, intersection, difference,\nconcatenation and Kleene star:\n▷ All other Boolean operations can be reduced to union and complement, e.g.:\nL1 ∩ L2 = L1 ∪ L2 L1 \\ L2 = L1 ∩ (Σ∗ \\ L2)\nThe complement requires to move to a DFA in general.\nIf M1, M2 are both DFAs, then their product can be used:\n(Q1 × Q2, Σ, {((q1, q2, a), (r1, r2)) | ((qi, a), ri) ∈ δi, i= 1, 2}, (q0,1, q0,2), F×)\ni.e. we let the two DFAs run in parallel/“lockstep”. The definition of F× then\ndepends on the Boolean operation (see the tutorials) .\npstart qa\nsstart tb\nb a, b\na a, b psstart\nqs\npt\nqt\na\nb\na\nb\nb\na\na, b\n(Again, use BFS/DFS to only construct the required states.)No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Closure properties 106 (277)\nC Regular languages are closed wrt. complement, union, intersection, difference,\nconcatenation and Kleene star:\n▷ Concatenation: Let Mi = (Qi, Σ, δi, q0,i, Fi) be FA with Li = L(Mi) for\ni = 1, 2. (Assume Q1 ∩ Q2 = ∅.)\nSimply connect the NFAs in series using ε-transitions:\n(Q1 ∪ Q2, Σ, δ1 ∪ δ2 ∪ {((qf,1, ε), q0,2) | qf,1 ∈ F1}, q0,1, F2)\nstart a\nstart b\na, b a, b\na, b a, b\nstart a\nb\nε\na, b a, b\na, b a, b\n(For comparison, try to prove this using regular grammars, and try to understand why\nthe final state is helpful here.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Closure properties 106 (278)\nC Regular languages are closed wrt. complement, union, intersection, difference,\nconcatenation and Kleene star:\n▷ Kleene star: Let M = (Q, Σ, δ, q0, F) be an FA with L = L(M).\nAdd a “feedback loop” to M1, and a new initial state so that ε can be\naccepted, too:\n(Q ∪ {q0,∗}, Σ, δ∪ {((qf , ε), q0), ((q0,∗, ε), q0) | qf ∈ F}, q0,∗, F∪ {q0,∗})\nstart a\na, b a, b\nstart a\nε\nε\na, b a, b\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "5 Regular languages\nRegular grammars and finite automata\nMoving between regular grammars and finite automata\nClosure properties\nRegular expressions\nMinimizing DFAs and Myhill-Nerode\nBasics of probabilistic finite automata (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions 108 (280)\nD Regular expression (RegEx): For a given alphabet Σ, a regular expression is a\nword produced by the following context-free grammar:\nS − →G 0 | 1 | a (a ∈ Σ) S − →G (SS) S − →G (S + S) S − →G S∗\nThe language L(ρ) described by a regular expression ρ is defined inductively:\nL(1) := {ε} L((ρ + ρ′)) := L(ρ) ∪ L(ρ′)\nL(a) := {a} L((ρρ′)) := L(ρ)L(ρ′)\nL(0) := ∅ L(ρ∗) := L(ρ)∗\nThe usual convention to use the operator precedence\nstar/repetition before product/concatenation before addition/choice\nin order to reduce the number of parentheses, e.g.:\nL(ab∗c) = {a}{b}∗{c} L(a∗bc) = {a}∗{bc} L((ab)∗c) = {ab}∗{c}\nL((a + b)∗c) = {a, b}∗{c} L(a + b∗c) = {a} ∪ {b}∗{c}\n(L(ρ) is the interpretation/meaning of the expression ρ as a language; but we can also give\nρ the meaning/semantics e.g. of an arithmetic expression with A(ρ∗) := (1 − A(ρ))−1.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions 108 (281)\nD Regular expression (RegEx): For a given alphabet Σ, a regular expression is a\nword produced by the following context-free grammar:\nS − →G 0 | 1 | a (a ∈ Σ) S − →G (SS) S − →G (S + S) S − →G S∗\nThe language L(ρ) described by a regular expression ρ is defined inductively:\nL(1) := {ε} L((ρ + ρ′)) := L(ρ) ∪ L(ρ′)\nL(a) := {a} L((ρρ′)) := L(ρ)L(ρ′)\nL(0) := ∅ L(ρ∗) := L(ρ)∗\n! Often also “ |” for “ +” (choice) and “ ∅” for “ 0” and “ ε” for “ 1”.\nNotation here is standard for Kleene algebras and semirings.\n(Most programming languages and tools like grep use deviating syntax anyways.)\n▷ ρn is defined as usual as shorthand for “product of n copies of ρ” for n ∈ N 0.\n(It actually makes a difference whether we are given ρn as n copies or only as pair of ρ and\nn with n in binary – complexity theory later.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions 108 (282)\nD Regular expression (RegEx): For a given alphabet Σ, a regular expression is a\nword produced by the following context-free grammar:\nS − →G 0 | 1 | a (a ∈ Σ) S − →G (SS) S − →G (S + S) S − →G S∗\nThe language L(ρ) described by a regular expression ρ is defined inductively:\nL(1) := {ε} L((ρ + ρ′)) := L(ρ) ∪ L(ρ′)\nL(a) := {a} L((ρρ′)) := L(ρ)L(ρ′)\nL(0) := ∅ L(ρ∗) := L(ρ)∗\nF Let “ρ ≡ ρ′” denote “ L(ρ) = L(ρ′)” (same language/semantics).\nThen ≡ is an equivalence relation on the regular expressions, and the\nfollowing identities hold (as we compute in (2Σ∗\n, ∪, ◦, ∗, ∅, {ε})):\nρ + 0 ≡ ρ ≡ 0 + ρ ρ 1 ≡ ρ ≡ 1ρ\n(ρ + ρ′) + ρ′′ ≡ ρ + (ρ′ + ρ′′) ( ρρ′)ρ′′ ≡ ρ(ρ′ρ′′)\n(ρ + ρ′)ρ′′ ≡ ρρ′′ + ρ′ρ′′ ρ(ρ′ + ρ′′) ≡ ρρ′ + ρρ′′\nρ + ρ′ ≡ ρ′ + ρ 0ρ ≡ 0 ≡ ρ0\nρ + ρ ≡ ρ 0∗ ≡ 1 ≡ 1∗\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kozen’s characterization of the Kleene star 109 (283)\nT For all languages R, S, X⊆ Σ∗ (cf. DS, Kozen):\n▷ R∗ = {ε} ∪R R∗ = {ε} ∪R∗ R.\n▷ If S ∪ R X⊆ X, then R∗ S ⊆ X.\n▷ If S ∪ X R⊆ X, then S R∗ ⊆ X.\nC For all R, S, X⊆ Σ∗:\nD R+ := RR∗\n▷ R∗ = R∗R∗ = (R∗)∗ = (R+)∗ = (R∗)+\n▷ R∗S is the ⊆-least solution of S ∪ R X= X (symmetrically for S R∗)\n▷ If R ⊆ S, then R∗ ⊆ S∗ and R+ ⊆ S+.\n▷ (R ∪ S)∗ = R∗(SR∗)∗ = (R∗S)∗R∗\n▷ If RX = XS, then R∗X = XS∗.\n(In DS these results were already discussed for relations R, S, X⊆ A × A. As L ⊆ Σ∗ can\nbe represented as RL = {(u, uw) | u ∈ Σ∗, w∈ L} ⊆Σ∗ × Σ∗ with L = εRL and\nL∗ = εR∗\nL, the results can be transferred.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kozen’s characterization of the Kleene star 109 (284)\nT For all languages R, S, X⊆ Σ∗ (cf. DS, Kozen):\n▷ R∗ = {ε} ∪R R∗ = {ε} ∪R∗ R.\n▷ If S ∪ R X⊆ X, then R∗ S ⊆ X.\n▷ If S ∪ X R⊆ X, then S R∗ ⊆ X.\nC Let “ρ ⊑ ρ′” abbreviate “ L(ρ) ⊆ L(ρ′).\nThen for all regular expressions ρ, σ, ξwrt. Σ:\n▷ ρ∗ ≡ 1 + ρρ∗ ≡ 1 + ρ∗ρ.\n▷ If σ + ρξ ⊑ ξ, then ρ∗ σ ⊑ ξ.\n▷ If σ + ξ ρ⊑ ξ, then σ ρ∗ ⊑ ξ.\nand thus\n(ρ∗)∗ ≡ ρ∗ρ∗ ≡ ρ∗ ρ∗ρ ≡ ρρ∗ (ρ + σ)∗ ≡ ρ∗(σρ∗)∗ ≡ (ρ∗σ)∗ρ∗\nand ρ∗σ is the ⊑-least solution of σ + ρX ≡ X.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expression to ε-NFA 110 (285)\n! By definition we start from the simplest possible regular languages\nL(0) = ∅ L(1) = {ε} L(a) = {a}\nwhich are then combined using union, concatenation and the Kleene star.\nThe inductive definition of L(ρ) thus can be extended to a recursive\nprocedure which computes an ε-NFA M with L(ρ) = L(M).\nL A regular expression ρ describes a regular language L(ρ).\n▷ Proof/construction by induction/recursion over term structure:\n▷ Base cases: ρ = 0, ρ = 1, ρ = a ∈ Σ\nReturn the obvious NFAs for L(0) = ∅, L(1) = {ε}, and L(a) = {a},\nrespectively.\n▷ Recursive cases: ρ = (ρ1 + ρ2), ρ = (ρ1ρ2), ρ = ρ∗\n1:\n(Use ρ0 ≡ 0 ≡ 0ρ and ρ + 0 ≡ ρ ≡ 0 + ρ to simplify the expression.)\nRecursively compute FAs Mi = (Qi, Σ, δi, q0,i, Fi) with L(Mi) = L(ρi).\nApply the corresponding construction discussed under “closure properties”.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expression to ε-NFA 110 (286)\n! By definition we start from the simplest possible regular languages\nL(0) = ∅ L(1) = {ε} L(a) = {a}\nwhich are then combined using union, concatenation and the Kleene star.\nThe inductive definition of L(ρ) thus can be extended to a recursive\nprocedure which computes an ε-NFA M with L(ρ) = L(M).\nL A regular expression ρ describes a regular language L(ρ).\nρ = ((a + b)∗c)∗\nstart a\nstart b\nstart c\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expression to ε-NFA 110 (287)\n! By definition we start from the simplest possible regular languages\nL(0) = ∅ L(1) = {ε} L(a) = {a}\nwhich are then combined using union, concatenation and the Kleene star.\nThe inductive definition of L(ρ) thus can be extended to a recursive\nprocedure which computes an ε-NFA M with L(ρ) = L(M).\nL A regular expression ρ describes a regular language L(ρ).\nρ = ((a + b)∗c)∗\na\nb\nstart cstart\nε\nε\nε\nε\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expression to ε-NFA 110 (288)\n! By definition we start from the simplest possible regular languages\nL(0) = ∅ L(1) = {ε} L(a) = {a}\nwhich are then combined using union, concatenation and the Kleene star.\nThe inductive definition of L(ρ) thus can be extended to a recursive\nprocedure which computes an ε-NFA M with L(ρ) = L(M).\nL A regular expression ρ describes a regular language L(ρ).\nρ = ((a + b)∗c)∗\na\nb\nstart c\nε\nε\nε\nε\nstart\nε\nε\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expression to ε-NFA 110 (289)\n! By definition we start from the simplest possible regular languages\nL(0) = ∅ L(1) = {ε} L(a) = {a}\nwhich are then combined using union, concatenation and the Kleene star.\nThe inductive definition of L(ρ) thus can be extended to a recursive\nprocedure which computes an ε-NFA M with L(ρ) = L(M).\nL A regular expression ρ describes a regular language L(ρ).\nρ = ((a + b)∗c)∗\na\nb\nc\nε\nε\nε\nε\nstart\nε\nε ε\nε\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expression to ε-NFA 110 (290)\n! By definition we start from the simplest possible regular languages\nL(0) = ∅ L(1) = {ε} L(a) = {a}\nwhich are then combined using union, concatenation and the Kleene star.\nThe inductive definition of L(ρ) thus can be extended to a recursive\nprocedure which computes an ε-NFA M with L(ρ) = L(M).\nL A regular expression ρ describes a regular language L(ρ).\nρ = ((a + b)∗c)∗\na\nb\nc\nε\nε\nε\nεε\nε ε\nε\nstart ε ε\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expressions to NFAs (*) 111 (291)\n▷ ε-transitions are convenient for the basic construction, but usually need to be\nremoved at some point, e.g. for complementing.\n▷ An alternative is to use NFAs with (i) multiple initial states, and (ii) (at most)\none final state without outgoing transitions (and no ε-transitions)1:\n▷ Base cases ρ = 0, ρ = 1, ρ = a ∈ Σ: Return the obvious NFAs.\n▷ Recursive cases: ρ = (ρ1 + ρ2), ρ = (ρ1ρ2), ρ = ρ∗\n1:\nRecursively compute FAs Mi = (Qi, Σ, δi, Ii, {qf,i}) with L(Mi) = L(ρi).\nCase ρ = (ρ1 + ρ2): treat/collapse qf,1 and qf,2 as/into a single state qf ; if\nqf,1 or qf,2 were initial, also qf is initial; all initial states stay initial.\nCase ρ = (ρ1ρ2): for each transition ((q, a), qf,1) (a ∈ Σ) leading into qf,1 add\na transition ((q, a), q′) to every q′ ∈ I2; then delete qf,1; if qf,1 ∈ I1, both\nI1 \\ {qf,1} and I2 stay initial, else only I1 \\ {qf,1}.\nCase ρ = ρ∗\n1: for each transition ((q, a), qf,1) (a ∈ Σ) leading into qf,1 add a",
      "a transition ((q, a), q′) to every q′ ∈ I2; then delete qf,1; if qf,1 ∈ I1, both\nI1 \\ {qf,1} and I2 stay initial, else only I1 \\ {qf,1}.\nCase ρ = ρ∗\n1: for each transition ((q, a), qf,1) (a ∈ Σ) leading into qf,1 add a\ntransition ((q, a), q′) to every q′ ∈ I1; further add qf,1 to the initial states.\n! When determinizing simply start with the set of all initial states.\n1as noted by Klara J. Meyer and Salomon Sickert.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expressions to NFAs (*) 111 (292)\n▷ ε-transitions are convenient for the basic construction, but usually need to be\nremoved at some point, e.g. for complementing.\n▷ An alternative is to use NFAs with (i) multiple initial states, and (ii) (at most)\none final state without outgoing transitions (and no ε-transitions)1:\nρ = ((a + b)∗c)∗\nstart a\nstart b\nstart c\n1as noted by Klara J. Meyer and Salomon Sickert.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expressions to NFAs (*) 111 (293)\n▷ ε-transitions are convenient for the basic construction, but usually need to be\nremoved at some point, e.g. for complementing.\n▷ An alternative is to use NFAs with (i) multiple initial states, and (ii) (at most)\none final state without outgoing transitions (and no ε-transitions)1:\nρ = ((a + b)∗c)∗\nstart a\nstart\nb start c\n1as noted by Klara J. Meyer and Salomon Sickert.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expressions to NFAs (*) 111 (294)\n▷ ε-transitions are convenient for the basic construction, but usually need to be\nremoved at some point, e.g. for complementing.\n▷ An alternative is to use NFAs with (i) multiple initial states, and (ii) (at most)\none final state without outgoing transitions (and no ε-transitions)1:\nρ = ((a + b)∗c)∗\nstart\nstart\na\na\nstart\nb\nbab start c\n1as noted by Klara J. Meyer and Salomon Sickert.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expressions to NFAs (*) 111 (295)\n▷ ε-transitions are convenient for the basic construction, but usually need to be\nremoved at some point, e.g. for complementing.\n▷ An alternative is to use NFAs with (i) multiple initial states, and (ii) (at most)\none final state without outgoing transitions (and no ε-transitions)1:\nρ = ((a + b)∗c)∗\nstart\na\na\nstart\nb\nbab\nstart\nc\n1as noted by Klara J. Meyer and Salomon Sickert.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From regular expressions to NFAs (*) 111 (296)\n▷ ε-transitions are convenient for the basic construction, but usually need to be\nremoved at some point, e.g. for complementing.\n▷ An alternative is to use NFAs with (i) multiple initial states, and (ii) (at most)\none final state without outgoing transitions (and no ε-transitions)1:\nρ = ((a + b)∗c)∗\nstart\na\na\nstart\nb\nbab\nstart start\nc\nc\nc\nc\n1as noted by Klara J. Meyer and Salomon Sickert.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (297)\nV = {S, T} S − →G aS | T T − →G bT | S | ε\nS ≡ aS + T\nT ≡ bT + S + 1 fG(0) ≡\n\u00120\n1\n\u0013\n! It remains to show that regular expressions can describe every regular\nlanguages.\n▷ To see this, it is useful to read a regular grammar as systems of linear\nfixed-point equations x = fG(x) whose (least) solution is the language\ngenerated by its nonterminals/variables:\nfG(0) ⊑ f2\nG(0) ⊑ . . .⊑ fk\nG(0) ⊑ fk+1\nG (0) = fG(fk\nG(0) ⊑ . . .\nor using matrices (the matrix A is also the transition matrix of the ε-NFA)\nfk+1\nG (0) ≡\n\u0012\na 1\n1 b\n\u00130 \u0012\n0\n1\n\u0013\n+\n\u0012\na 1\n1 b\n\u00131 \u0012\n0\n1\n\u0013\n+ . . .+\n\u0012\na 1\n1 b\n\u0013k \u0012\n0\n1\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (298)\nV = {S, T} S − →G aS | T T − →G bT | S | ε\nS ≡ aS + T\nT ≡ bT + S + 1 f2\nG(0) ≡\n\u0012 1\nb + 1\n\u0013\n! It remains to show that regular expressions can describe every regular\nlanguages.\n▷ To see this, it is useful to read a regular grammar as systems of linear\nfixed-point equations x = fG(x) whose (least) solution is the language\ngenerated by its nonterminals/variables:\nfG(0) ⊑ f2\nG(0) ⊑ . . .⊑ fk\nG(0) ⊑ fk+1\nG (0) = fG(fk\nG(0) ⊑ . . .\nor using matrices (the matrix A is also the transition matrix of the ε-NFA)\nfk+1\nG (0) ≡\n\u0012\na 1\n1 b\n\u00130 \u0012\n0\n1\n\u0013\n+\n\u0012\na 1\n1 b\n\u00131 \u0012\n0\n1\n\u0013\n+ . . .+\n\u0012\na 1\n1 b\n\u0013k \u0012\n0\n1\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (299)\nV = {S, T} S − →G aS | T T − →G bT | S | ε\nS ≡ aS + T\nT ≡ bT + S + 1 f3\nG(0) ≡\n\u0012a + b + 1\nbb + b + 1\n\u0013\n! It remains to show that regular expressions can describe every regular\nlanguages.\n▷ To see this, it is useful to read a regular grammar as systems of linear\nfixed-point equations x = fG(x) whose (least) solution is the language\ngenerated by its nonterminals/variables:\nfG(0) ⊑ f2\nG(0) ⊑ . . .⊑ fk\nG(0) ⊑ fk+1\nG (0) = fG(fk\nG(0) ⊑ . . .\nor using matrices (the matrix A is also the transition matrix of the ε-NFA)\nfk+1\nG (0) ≡\n\u0012\na 1\n1 b\n\u00130 \u0012\n0\n1\n\u0013\n+\n\u0012\na 1\n1 b\n\u00131 \u0012\n0\n1\n\u0013\n+ . . .+\n\u0012\na 1\n1 b\n\u0013k \u0012\n0\n1\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (300)\nV = {S, T} S − →G aS | T T − →G bT | S | ε\nS ≡ aS + T\nT ≡ bT + S + 1 f4\nG(0) ≡\n\u0012 aa + ab + a + bb + b + 1\nbbb + bb + b + 1 +a + b + 1\n\u0013\n! It remains to show that regular expressions can describe every regular\nlanguages.\n▷ To see this, it is useful to read a regular grammar as systems of linear\nfixed-point equations x = fG(x) whose (least) solution is the language\ngenerated by its nonterminals/variables:\nfG(0) ⊑ f2\nG(0) ⊑ . . .⊑ fk\nG(0) ⊑ fk+1\nG (0) = fG(fk\nG(0) ⊑ . . .\nor using matrices (the matrix A is also the transition matrix of the ε-NFA)\nfk+1\nG (0) ≡\n\u0012\na 1\n1 b\n\u00130 \u0012\n0\n1\n\u0013\n+\n\u0012\na 1\n1 b\n\u00131 \u0012\n0\n1\n\u0013\n+ . . .+\n\u0012\na 1\n1 b\n\u0013k \u0012\n0\n1\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (301)\nV = {S, T} S − →G aS | T T − →G bT | S | ε\nS ≡ aS + T\nT ≡ bT + S + 1 f∞\nG (0) ≡\n\u0012a∗(b + a∗)∗\n(b + a∗)∗\n\u0013\n! It remains to show that regular expressions can describe every regular\nlanguages.\n▷ To see this, it is useful to read a regular grammar as systems of linear\nfixed-point equations x = fG(x) whose (least) solution is the language\ngenerated by its nonterminals/variables:\nfG(0) ⊑ f2\nG(0) ⊑ . . .⊑ fk\nG(0) ⊑ fk+1\nG (0) = fG(fk\nG(0) ⊑ . . .\nor using matrices (the matrix A is also the transition matrix of the ε-NFA)\nfk+1\nG (0) ≡\n\u0012\na 1\n1 b\n\u00130 \u0012\n0\n1\n\u0013\n+\n\u0012\na 1\n1 b\n\u00131 \u0012\n0\n1\n\u0013\n+ . . .+\n\u0012\na 1\n1 b\n\u0013k \u0012\n0\n1\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (302)\nV = {S, T} S − →G aS | T T − →G bT | S | ε\nS ≡ aS + T\nT ≡ bT + S + 1 f∞\nG (0) ≡\n\u0012a∗(b + a∗)∗\n(b + a∗)∗\n\u0013\nL For two languages L1, L2 ⊆ Σ∗ the least solution of X = L1X ∪ L2 is\nX = L∗\n1L2 =\n[\nk∈N 0\nLk\n1L2 = L2 ∪ L1L2 ∪ L1L1L2 . . .\n(cf. Kozen’s characterization; sometimes called “Arden’s lemma”.)\nC Rephrased wrt. regular expressions: For two regular expressions α, βthe least\nsolution of X ≡ αX + β is thus X ≡ α∗β.\n! E.g. X ≡ X + 1 has infinitely many solutions:\nAny ρ with ε ∈ L(ρ) is a solution, but 1∗1 = 1 is the least solution.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (303)\nV = {S, T} S − →G aS | T T − →G bT | S | ε\nS ≡ aS + T\nT ≡ bT + S + 1 f∞\nG (0) ≡\n\u0012a∗(b + a∗)∗\n(b + a∗)∗\n\u0013\n▷ This allows to compute a regular expression for L(G) by solving the linear\nequation system for the start symbol by means of substitution:\n(1)S ≡ aS + T\n(2)T ≡ bT + S + 1\n(1′)S ≡ a∗T (solve (1) for S)\n(2′)T ≡ (b + a∗)T + 1 ( subs. (1′) in (2))\n(2′′)T ≡ (b + a∗)∗ (solve (2′) for (T))\n(1′′)S ≡ a∗(b + a∗)∗ (subs. (2′′) in (1′))\n(As regular expression do not allow complement/difference, they have to describe all\naccepting runs of an FA which might get lengthy sometimes.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (304)\nV = {S, T} S − →G aS | T T − →G bT | S | ε\nS ≡ aS + T\nT ≡ bT + S + 1 f∞\nG (0) ≡\n\u0012a∗(b + a∗)∗\n(b + a∗)∗\n\u0013\nL For every regular language L there is a regular expression ρ with L = L(ρ).\n▷ Proof sketch:\n▷ Let G = (V, Σ, P, S) be a regular grammar. For each X ∈ V define:\nX ≡ P\n(X,aY )∈P aY + P\n(X,Y )∈P Y + P\n(X,a)∈P a + P\n(X,ε)∈P 1\nThe corresponding fixed-point iteration defined by X(0) := 0 and\nX(i+1) ≡ P\n(X,aY )∈P aY (i) + P\n(X,Y )∈P Y (i) + P\n(X,a)∈P a + P\n(X,ε)∈P 1\nconverges to the languages generated by the nonterminals which is the least\nsolution of resulting (right-)linear equation system.\n▷ Solve this system for S using that α∗β is the least solution of X ≡ αX + β for\nall regular expressions α, β(resp. for regular langauges L(α), L(β)).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (305)\nV = {S, T} S − →G aS | T T − →G bT | S | ε\nS ≡ aS + T\nT ≡ bT + S + 1 f∞\nG (0) ≡\n\u0012a∗(b + a∗)∗\n(b + a∗)∗\n\u0013\n▷ This also proves that we can allow regular expressions as abbreviations on the\nedges of an FA, e.g.:\npstart q ra∗b b+c\nabc\nThe represented/accepted language is still regular:\nXp ≡ a∗bXq Xq ≡ (b + c)Xr + 1 Xr ≡ abcXp\nThis of course also means that, if we allow regular expressions as “terminals”\nin regular grammars, we still generate a regular language (but this requires that\n0, 1, +, ∗ are treated as metasymbols and not as terminals) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Grammars as equations, languages as solutions/fixed points 112 (306)\nV = {S, T} S − →G aS | T T − →G bT | S | ε\nS ≡ aS + T\nT ≡ bT + S + 1 f∞\nG (0) ≡\n\u0012a∗(b + a∗)∗\n(b + a∗)∗\n\u0013\n▷ We also obtain from this that left-linear grammars also generate regular\nlanguages, e.g.:\nS − →G Sa | T b T − →G T b| c\ncorresponds to\nS ≡ Sa + T b T ≡ T b+ c\nwith\nT ≡ c b∗ S ≡ c b∗ba∗\n! But linear grammars in general generate non-regular languages:\nS − →G′ aT | ε T − →G′ Sb L (G′) = {anbn | n ∈ N 0}\n(See Myhill-Nerode for why {anbn | n ∈ N 0} is not regular.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions and matrix iteration (*) 113 (307)\nF Given a matrix A of regular expressions\nA =\n\n\na1,1 a1,2 . . .\na2,1 a2,2 . . .\n... ... ...\n\n =\n\u0012 a1,1 A1,2\nA2,1 A2,2\n\u0013\nwe can recursively compute A∗ by partitioning A wrt. a1,1\nA∗ =\n\u0012a∗\n1,1 + a∗\n1,1A1,2ΓA2,1a∗\n1,1 a∗\n1,1A1,2Γ\nΓA2,1a∗\n1,1 Γ\n\u0013\nΓ := (A2,2 + A2,1a∗\n1,1A1,2)∗\nso that the least solution of x = Ax + y is A∗y.\nV = {S, T} S − →G aS | T T − →G bT | S | ε\nS ≡ aS + T\nT ≡ bT + S + 1\n\u0012\nS\nT\n\u0013\n≡\n\u0012\na 1\n1 b\n\u0013\u0012\nS\nT\n\u0013\n+\n\u0012\n0\n1\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions and matrix iteration (*) 113 (308)\nF Given a matrix A of regular expressions\nA =\n\n\na1,1 a1,2 . . .\na2,1 a2,2 . . .\n... ... ...\n\n =\n\u0012 a1,1 A1,2\nA2,1 A2,2\n\u0013\nwe can recursively compute A∗ by partitioning A wrt. a1,1\nA∗ =\n\u0012a∗\n1,1 + a∗\n1,1A1,2ΓA2,1a∗\n1,1 a∗\n1,1A1,2Γ\nΓA2,1a∗\n1,1 Γ\n\u0013\nΓ := (A2,2 + A2,1a∗\n1,1A1,2)∗\nso that the least solution of x = Ax + y is A∗y.\nV = {S, T} S − →G aS | T T − →G bT | S | ε\nS ≡ aS + T\nT ≡ bT + S + 1\n\u0012\nS\nT\n\u0013\n≡\n\u0012\na∗ + a∗(b + a∗)∗a∗ a∗(b + a∗)∗\n(b + a∗)∗a∗ (b + a∗)∗\n\u0013\u0012\n0\n1\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions and matrix iteration (*) 113 (309)\nF Given a matrix A of regular expressions\nA =\n\n\na1,1 a1,2 . . .\na2,1 a2,2 . . .\n... ... ...\n\n =\n\u0012 a1,1 A1,2\nA2,1 A2,2\n\u0013\nwe can recursively compute A∗ by partitioning A wrt. a1,1\nA∗ =\n\u0012a∗\n1,1 + a∗\n1,1A1,2ΓA2,1a∗\n1,1 a∗\n1,1A1,2Γ\nΓA2,1a∗\n1,1 Γ\n\u0013\nΓ := (A2,2 + A2,1a∗\n1,1A1,2)∗\nso that the least solution of x = Ax + y is A∗y.\nV = {S, T} S − →G aS | T T − →G bT | S | ε\nS ≡ aS + T\nT ≡ bT + S + 1\n\u0012\nS\nT\n\u0013\n≡\n\u0012\na∗(b + a∗)∗\n(b + a∗)∗\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions and matrix iteration (*) 113 (310)\nF Given a matrix A of regular expressions\nA =\n\n\na1,1 a1,2 . . .\na2,1 a2,2 . . .\n... ... ...\n\n =\n\u0012 a1,1 A1,2\nA2,1 A2,2\n\u0013\nwe can recursively compute A∗ by partitioning A wrt. a1,1\nA∗ =\n\u0012a∗\n1,1 + a∗\n1,1A1,2ΓA2,1a∗\n1,1 a∗\n1,1A1,2Γ\nΓA2,1a∗\n1,1 Γ\n\u0013\nΓ := (A2,2 + A2,1a∗\n1,1A1,2)∗\nso that the least solution of x = Ax + y is A∗y.\n! The expressions a∗\n1,1 + a∗\n1,1A1,2ΓA2,1a∗\n1,1 simply describes how to go from\nstate 1 to state 1 in finitely (but unbounded) many steps:\n▷ a1,1: stay/loop in 1.\n▷ A1,2: enter some state k >1.\n▷ Γ: spend some time away from 1.\n▷ A2,1: return to 1.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions and matrix iteration (*) 113 (311)\nF Given a matrix A of regular expressions\nA =\n\n\na1,1 a1,2 . . .\na2,1 a2,2 . . .\n... ... ...\n\n =\n\u0012 a1,1 A1,2\nA2,1 A2,2\n\u0013\nwe can recursively compute A∗ by partitioning A wrt. a1,1\nA∗ =\n\u0012a∗\n1,1 + a∗\n1,1A1,2ΓA2,1a∗\n1,1 a∗\n1,1A1,2Γ\nΓA2,1a∗\n1,1 Γ\n\u0013\nΓ := (A2,2 + A2,1a∗\n1,1A1,2)∗\nso that the least solution of x = Ax + y is A∗y.\n▷ In general:\nGiven a finite directed graph ([n], E), we can turn it into a DFA by assigning\neach edge (i, j) a unique terminal ai,j.\nThen the (i, j)-entry of A∗ will be a regular expression that describes\n(“summarizes”) all finite i-j-paths.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions and matrix iteration (*) 113 (312)\nF Given a matrix A of regular expressions\nA =\n\n\na1,1 a1,2 . . .\na2,1 a2,2 . . .\n... ... ...\n\n =\n\u0012 a1,1 A1,2\nA2,1 A2,2\n\u0013\nwe can recursively compute A∗ by partitioning A wrt. a1,1\nA∗ =\n\u0012a∗\n1,1 + a∗\n1,1A1,2ΓA2,1a∗\n1,1 a∗\n1,1A1,2Γ\nΓA2,1a∗\n1,1 Γ\n\u0013\nΓ := (A2,2 + A2,1a∗\n1,1A1,2)∗\nso that the least solution of x = Ax + y is A∗y.\n▷ Analogously, every NFA (Q, Σ, δ, q0, F) with |Q| = n gives rise to adjacency\nmatrices Aa ∈ {0, 1}n×n (a ∈ Σ). Set A = P\na∈Σ aAa, then\nAk =\nX\na1...ak∈Σk\na1 . . . akAa1 ··· Aak\nsummarizes the behavior of the NFA for all words of length k, and\nA∗ = P∞\nk=0 Ak summarizes the complete behavior of the NFA.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions and matrix iteration (*) 113 (313)\nF Given a matrix A of regular expressions\nA =\n\n\na1,1 a1,2 . . .\na2,1 a2,2 . . .\n... ... ...\n\n =\n\u0012 a1,1 A1,2\nA2,1 A2,2\n\u0013\nwe can recursively compute A∗ by partitioning A wrt. a1,1\nA∗ =\n\u0012a∗\n1,1 + a∗\n1,1A1,2ΓA2,1a∗\n1,1 a∗\n1,1A1,2Γ\nΓA2,1a∗\n1,1 Γ\n\u0013\nΓ := (A2,2 + A2,1a∗\n1,1A1,2)∗\nso that the least solution of x = Ax + y is A∗y.\n▷ And in case of an ε-NFA we further have a adjacency matrix Aε wrt.\nε-transitions.\nRemoving ε-transitions means to replace Aa by A∗\nεAaA∗\nε:\nAs Aε is just a boolean adjacency matrix ( 1 + 1 ≡ 1), A∗\nε describes if some\nstate p can reach some state q only by means of ε-transitions.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Generalized Floyd-Warshall algorithm, Kleene’s algorithm, . . . 114 (314)\n▷ The recursive computation of A∗ without the use of idempotence\nA∗ =\n\u0012a∗\n1,1 + a∗\n1,1A1,2ΓA2,1a∗\n1,1 a∗\n1,1A1,2Γ\nΓA2,1a∗\n1,1 Γ\n\u0013\nΓ := (A2,2 + A2,1a∗\n1,1A1,2)∗\nunifies several important algorithms.\n▷ The Floyd-Warshall algorithm for shortest paths.\n▷ Kleene’s algorithm for computing regular expressions.\n▷ Solving systems of linear fixed-point equations over R via the\nNeumann/geometric series.\n▷ Computing the generating functions for counting the paths in the graph.\n▷ Computing the Cesaro mean by means of the Neumann series/generating\nfunction. Special case: computing the stationary distribution and expected\ntimes of first return/arrival.\n▷ . . .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Generalized Floyd-Warshall algorithm, Kleene’s algorithm, . . . 114 (315)\n▷ The recursive computation of A∗ without the use of idempotence\nA∗ =\n\u0012a∗\n1,1 + a∗\n1,1A1,2ΓA2,1a∗\n1,1 a∗\n1,1A1,2Γ\nΓA2,1a∗\n1,1 Γ\n\u0013\nΓ := (A2,2 + A2,1a∗\n1,1A1,2)∗\nunifies several important algorithms.\n▷ In general, (right/left) linear fixed-point systems\nAx + y = x resp x⊤A + y = x⊤\ndescribe the semantics/behavior/equilibrium of some abstract “flow” of\ndynamic linear systems.\n▷ In fact, also linear grammars can be represented as linear fixed-point systems\nusing “contexts” instead of words:\n▷ A context is a pair of words (u, v) ∈ Σ × Σ with the concatenation of contexts\nnow defined by (u, v) ◦ (x, y) := (ux, yv).\n▷ E.g. L = {anbn | n ∈ N 0} is then the least solution of X ≡ (a, b)X + (ε, ε)\nwhich can be described by (a, b)∗(ε, ε).\n▷ This allows to approximate context-free languages by means of Newton’s",
      "now defined by (u, v) ◦ (x, y) := (ux, yv).\n▷ E.g. L = {anbn | n ∈ N 0} is then the least solution of X ≡ (a, b)X + (ε, ε)\nwhich can be described by (a, b)∗(ε, ε).\n▷ This allows to approximate context-free languages by means of Newton’s\nmethod.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rational expressions for fixed-points (*) 115 (316)\n▷ Example: Consider for simplicity the case n = 2:\nx1\ny1\nx2\ny2\na b d\nc1 1\n\u0012\nx1\nx2\n\u0013\n=\n\u0012\na b\nc d\n\u0013\n· x +\n\u0012\ny1\ny2\n\u0013\n▷ Using z∗ as abbreviation for “least solution of X = 1 + zX” we can solve\nx1 = ax1 + bx2 + y1 ⇝ x1 = (1 − a)−1(bx2 + e) = a∗(bx2 + y1)\nx2 = ca∗(bx2 + y1) + dx2 + y2 ⇝ x2 = (d + ca∗b)∗(ca∗y1 + y2)\nor in matrix form\nx =\n\u0012a∗ + a∗b(d + ca∗b)∗ca∗ a∗b(d + ca∗b)∗\n(d + ca∗b)∗ca∗ (d + ca∗b)∗\n\u0013\n·\n\u0012y1\ny2\n\u0013\n(assuming that all expressions are defined)\n! For rational expressions, addition is not idempotent, i.e. 1 + 1 = 2 ̸= 1 and in\ngeneral ρ + ρ ̸= ρ and (ρ∗)∗ ̸= ρ∗.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rational expressions for fixed-points (*) 115 (317)\n▷ Example: Consider for simplicity the case n = 2:\nx1\ny1\nx2\ny2\na b d\nc1 1\n\u0012\nx1\nx2\n\u0013\n=\n\u0012\na∗ + a∗b(d + ca∗b)∗ca∗ a∗b(d + ca∗b)∗\n(d + ca∗b)∗ca∗ (d + ca∗b)∗\n\u0013\n·\n\u0012\ny1\ny2\n\u0013\n▷ (+, ·, ∗, 0, 1) can be instantiated in different ways:\n▷ (2Σ∗\n, ∪, ◦, ∅, {ε}) with L∗ = S\nk∈N 0 Lk\n▷ (2V ×V , ∪, ◦, ∅, IdV ) with R∗ = S\nk∈N 0 Rk.\n▷ (R , +, ·, 0, 1) with z∗ := P∞\nk=0 zk\n▷ ({0, 1}, max, min, 0, 1) with z∗ = 1.\n▷ (R ∪ {±∞}, min, +, +∞, 0) with z∗ = −∞, if z <0, else z∗ = 0.\n▷ (R ∪ {±∞}, max, +, −∞, 0) with z∗ = ∞, if z >0, else z∗ = 0.\n▷ ([0, 1], max, ·, 0, 1) with z∗ = 1.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rational expressions for fixed-points (*) 115 (318)\n▷ Example: Consider for simplicity the case n = 2:\nx1\ny1\nx2\ny2\na b d\nc1 1\n\u0012\nx1\nx2\n\u0013\n=\n\u0012\na∗ + a∗b(d + ca∗b)∗ca∗ a∗b(d + ca∗b)∗\n(d + ca∗b)∗ca∗ (d + ca∗b)∗\n\u0013\n·\n\u0012\ny1\ny2\n\u0013\n▷ Without the use of idempotence ( 1 + 1 ̸= 1) an expression like\n(d + ca∗b)∗\nis called a rational expression:\n▷ It summarizes the effect of all finite cycles leading from the node x2 to x2\nagain: Every such cycle can be partitioned wrt. the returns to x2,\ni.e. either return directly by means of the edge d\nor return by means of a detour to x1 as described by ca∗b.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rational expressions for fixed-points (*) 115 (319)\n▷ Example: Consider for simplicity the case n = 2:\nx1\ny1\nx2\ny2\na b d\nc1 1\n\u0012\nx1\nx2\n\u0013\n=\n\u0012\na∗ + a∗b(d + ca∗b)∗ca∗ a∗b(d + ca∗b)∗\n(d + ca∗b)∗ca∗ (d + ca∗b)∗\n\u0013\n·\n\u0012\ny1\ny2\n\u0013\n▷ E.g. when computing the generating function for the Fibonacci numbers, we\nhave to solve the following system over the field of rational functions in z:\n\u0012gF (z)\ngG(z)\n\u0013\n=\n\u0012z z\nz 0\n\u0013\n·\n\u0012gF (z)\ngG(z)\n\u0013\n+\n\u00120\n1\n\u0013\nSetting a = b = c = z and d = 0 in our generic solution, we obtain\n\u0012gF (z)\ngG(z)\n\u0013\n=\n\u0012z∗ + z∗z(z2z∗)∗zz∗ z∗z(z2z∗)∗\n(z2z∗)∗zz∗ (z2z∗)∗\n\u0013\n·\n\u00120\n1\n\u0013\nWith z∗ = (1 − z)−1 we obtain e.g. gF (z) = z\n1−z−z2\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rational expressions for fixed-points (*) 115 (320)\n▷ Example: Consider for simplicity the case n = 2:\nx1\ny1\nx2\ny2\na b d\nc1 1\n\u0012\nx1\nx2\n\u0013\n=\n\u0012\na∗ + a∗b(d + ca∗b)∗ca∗ a∗b(d + ca∗b)∗\n(d + ca∗b)∗ca∗ (d + ca∗b)∗\n\u0013\n·\n\u0012\ny1\ny2\n\u0013\n▷ As another application, recall that, if the Ces` aro mean exists, then\nlim\nl→∞\n1\n1 + l\nlX\nk=0\nPk = lim\nz→1−\n(1 − z)\n∞X\nk=0\n(zP )k\nFor instance, for the “Fibonacci graph” with d = 0\nzP =\n\u0012z/2 z/2\nz 0\n\u0013\nwe have a∗ = 2\n2−z and (d + ca∗b)∗ = 2−z\n(2+z)(1−z) for 0 ≤ z <1 so that:\n(1 − z)(zP )∗ =\n 2(1−z)\n(2−z) + 2z2\n(2−z)(2+z)\nz\n(2+z)\n2z\n(2+z)\n2−z\n(2+z)\n!\nz→1−\n− − − − →\n\u00122\n3\n1\n32\n3\n1\n3\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rational expressions for fixed-points (*) 115 (321)\n▷ Example: Consider for simplicity the case n = 2:\nx1\ny1\nx2\ny2\na b d\nc1 1\n\u0012\nx1\nx2\n\u0013\n=\n\u0012\na∗ + a∗b(d + ca∗b)∗ca∗ a∗b(d + ca∗b)∗\n(d + ca∗b)∗ca∗ (d + ca∗b)∗\n\u0013\n·\n\u0012\ny1\ny2\n\u0013\n▷ For instance, consider again the adjacency matrix underlying the Fibonacci\nnumbers with the d-loop disabled:\nA =\n\u00121 1\n1 0\n\u0013\nThen the standard reflexive-transitive closure is obtained wrt. the Boolean\nsemiring ({0, 1}, max, min, 0, 1) with z∗ = 1\nA∗ =\n\u00121 1\n1 1\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Rational expressions for fixed-points (*) 115 (322)\n▷ Example: Consider for simplicity the case n = 2:\nx1\ny1\nx2\ny2\na b d\nc1 1\n\u0012\nx1\nx2\n\u0013\n=\n\u0012\na∗ + a∗b(d + ca∗b)∗ca∗ a∗b(d + ca∗b)∗\n(d + ca∗b)∗ca∗ (d + ca∗b)∗\n\u0013\n·\n\u0012\ny1\ny2\n\u0013\n▷ If we instead treat the parameters a, b, c, das distances and compute wrt. the\nmin-tropical semiring (R ∪ {±∞}, min, +, +∞, 0)\nwith z∗ = −∞, if z <0, else z∗ = 0, then e.g. for\nD =\n\u00121 2\n3 + ∞\n\u0013\nD∗ =\n\u00120 2\n3 0\n\u0013\nbut for\nD =\n\u00121 2\n3 −1\n\u0013\nD∗ =\n\u0012−∞ −∞\n−∞ −∞\n\u0013\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Regular expressions in practice 116 (323)\n▷ Tools like grep and also most programming languages support extended forms\nof regular expressions (Python/re, C++11/regex) with deviating syntax.\n▷ E.g. usually “ +” and “ ∗” have different semantics, escapes have to be used,\nabbreviations for frequently used character classes, (complement), . . .\n▷ The typical use here is pattern-matching/searching; often the power-set\nconstruction is only applied “on-the-fly” for the given input.\n▷ Regular expressions allow to specify the desired behavior of a system from\nwhich then an implementation in form of a DFA can be “synthesized”.\n▷ Typical webpage:\nmainpage (advertisement content advertisement)∗\n▷ Input should be a valid number in decimal representation:\n(As we need 0, 1, +, − as alphabet symbols ε, ∅, | instead.)\n0 | (+ | − |ε)0∗(1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9)(1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9)∗\n▷ As we will see, we can compute “optimal” DFAs for a given regular\nexpressions.",
      "▷ Input should be a valid number in decimal representation:\n(As we need 0, 1, +, − as alphabet symbols ε, ∅, | instead.)\n0 | (+ | − |ε)0∗(1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9)(1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9)∗\n▷ As we will see, we can compute “optimal” DFAs for a given regular\nexpressions.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Remark: ω-regular expressions (*) 117 (324)\n▷ Let L ⊆ Σ∗ be a language of finite words:\nL∗ is obtained by finite repetition of words of L\nLω is obtained by infinite repetition of words of L assuming ε ̸∈ L:\nLω = {w(1)w(2)w(3) . . . w(i) . . .| for all i ∈ N : w(i) ∈ L \\ {ε}} ⊆Σω\n“ω-regular expressions = regular expressions with ω-operator”\n▷ While regular expressions can be used to describe the behavior terminating\nprograms, ω-regular expressions are used to specify the behavior of systems\nthat are supposed to run forever like processes, e.g.:\n▷ L((a + b + c)∗(a + b)ω): “w ∈ {a, b, c}ω must contain only finitely many cs”\n▷ L(((a + b)∗c)ω): “w ∈ {a, b, c}ω must contain infinitely many cs”\nAlso ω-regular expressions can be compiled to finite automata but now an\ninfinite word w ∈ Σω is accepted if a final state is visited infinitely often.\nThis is used both for the verification and synthesis of reactive systems (like\ncontrollers/robots/processes/circuits).",
      "Also ω-regular expressions can be compiled to finite automata but now an\ninfinite word w ∈ Σω is accepted if a final state is visited infinitely often.\nThis is used both for the verification and synthesis of reactive systems (like\ncontrollers/robots/processes/circuits).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "5 Regular languages\nRegular grammars and finite automata\nMoving between regular grammars and finite automata\nClosure properties\nRegular expressions\nMinimizing DFAs and Myhill-Nerode\nBasics of probabilistic finite automata (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Minimization of DFAs 119 (326)\n▷ The automatic translation of an regular expression ρ to a finite automaton\nyields in general an ε-NFA Mρ.\n▷ For the implementation as a program, we actually want the FA to be\ndeterministic, i.e. we need to apply the power-set construction to Mρ to\nobtain a DFA M′\nρ.\n▷ In general, M′\nρ might contain\n▷ states unreachable from the initial state.\n▷ states that cannot reach any final state.\n▷ states that actually “do the same”.\n▷ The first two kinds of useless states can be removed by treating the\nautomaton as finite graph (Q, Eδ) with\nEδ = {(q, r) | ((q, a), r) ∈ δ} (i.e. forget the input)\nThen restrict the automaton to the states q0E∗\nδ ∩ E∗\nδ F.\n▷ For the third kind, the Myhill-Nerode relation is important.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Equivalent states in a DFA 120 (327)\n▷ Example: Consider the following DFA for L(a∗b∗):\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n▷ A DFA is the “tail-recursive implementation” of a characteristic\nfunction/predicate, i.e. it can only return 1 (accept input) or 0 (reject input).\nEvery state corresponds to a recursive characteristic function that gets some\nword w ∈ Σ∗ as input.\nTwo states/functions p, qare “equivalent wrt. the DFA M” iff they\nimplement the same characteristic function iff both states accept the same\nlanguage.\n! In the following it is important that the DFA is complete, i.e. if necessary add\na rejecting state (as in the example) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Equivalent states in a DFA 120 (328)\n▷ Example: Consider the following DFA for L(a∗b∗):\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\nD Define the relation ∼M on the states Q of a (complete) DFA M by\np ∼M q if for all z ∈ Σ∗ : pz − →∗\nM p′ ∈ F iff qz − →∗\nM q′ ∈ F\n(i.e. for every word w either both states reach a final state or none reaches a final state.)\nF ∼M is an equivalence relation on Q by the properties of the biconditional.\n(|= (A ↔ A), |= ((A ↔ B) → (B ↔ A)), |= (((A ↔ B) ∧ (B ↔ C)) → (A ↔ C)).)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Equivalent states in a DFA 120 (329)\n▷ Example: Consider the following DFA for L(a∗b∗):\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n0start\n1 4\na\nb\nb\na\na, b\n▷ As ∼M -equvialent states implement the same characteristic function, it\nsuffices to keep one representative of each equivalence class (colors): all\ntransitions (calls) can be rerouted to the chosen representative.\nD The quotient of M wrt. ∼M is then\n(Q/ ∼M , Σ, {(([p]∼M , a), [q]∼M ) | pa − →∗\nM q}, [q0]∼M , F/∼M )\nTrivially, |Q/ ∼M | ≤ |Q|.\n▷ Closely related to ∼M is the Myhill-Nerode relation:\nWhile ∼M depends on a specific DFA M, the Myhill-Nerode relation only\ndepends on the language.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Equivalent states in a DFA 120 (330)\n▷ Example: Consider the following DFA for L(a∗b∗):\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n0start\n1 4\na\nb\nb\na\na, b\n▷ wlog assume that all states are reachable from the initial state q0.\nThen we can fix for every state q of M a word wq ∈ Σ∗ s.t. q0wq − →∗\nM q.\nWrt. these words, the definition of ∼M\np ∼M q if for all z ∈ Σ∗ : pz − →∗\nM p′ ∈ F iff qz − →∗\nM q′ ∈ F\nbecomes\np ∼M q if for all z ∈ Σ∗ : wpz ∈ L iff wqz ∈ L\n(The actual choice of wq does not matter as for any z there is only one computation\nstarting at q.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Equivalent states in a DFA 120 (331)\n▷ Example: Consider the following DFA for L(a∗b∗):\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n0start\n1 4\na\nb\nb\na\na, b\nD The Myhill-Nerode relation ∼L on Σ∗ is defined by\nu ∼L v if for all z ∈ Σ∗ : uz ∈ L iff vz ∈ L\nIt only depends on L but not on a specific DFA.\nF By choice/definition of wp, wq: p ∼M q iff wp ∼L wq.\n▷ As we will see: If L is regular, than |Σ∗/ ∼L | = |Q/ ∼M | ≤ |Q| for any\ncomplete DFA M with L(M) = L.\n▷ First how to decide “ p ∼M q” for a given DFA M in order to minimize M.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Minimizing DFAs 121 (332)\n▷ Let M = (Q, Σ, δ, q0, F) be a complete DFA with L := L(M).\nwlog all states of M are reachable from q0.\nBy definition:\np ̸∼M q\niff there ex. z ∈ Σ∗ s.t. (wlog) pz − →∗\nM p′ ∈ F and qz − →∗\nM q′ ̸∈ F\niff there ex. z ∈ Σ∗ that is accepted by the automaton with\nstates the unordered pairs\n\u0000Q\n2\n\u0001\n= {{r, s} ⊆Q | r ̸= s}\ntransitions {(({r, s}, a), {r′, s′}) | a ∈ Σ, ra− →M r′, sa− →M s′, r′ ̸= s′}\nfinal states {{r, s} |r ∈ F, s̸∈ F}\nand initial state {p, q}.\n(Product construction of the symmetric difference of M with itself.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Minimizing DFAs 121 (333)\n▷ Let M = (Q, Σ, δ, q0, F) be a complete DFA with L := L(M).\nwlog all states of M are reachable from q0.\nL p ̸∼M q can thus be decided by backwards reachability:\nS0 :=\nn\n{r, s} ∈\n\u0000Q\n2\n\u0001\n| r ∈ F, s̸∈ F\no\nSk+1 := Sk\n∪\nn\n{r, s} ∈\n\u0000Q\n2\n\u0001\n\\ Sk | a ∈ Σ, ra− →M r′, sa− →M s′, {r′, s′} ∈Sk\no\nThen: p ̸∼M q iff wp ̸∼L wq iff {p, q} ∈S|Q|2 .\n(If Sk = Sk+1, also Sk = S|Q|2 .)\nBy remembering any a ∈ Σ that led to the inclusion of {p, q} ∈Sk+1, we can\nalso compute a separating word for p and q (resp. wp and wq).\n▷ The quotient of M wrt. ∼M is then\n(Q/ ∼M , Σ, {(([p]∼M , a), [q]∼M ) | pa − →∗\nM q}, [q0]∼M , F/∼M )\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Minimizing DFAs: example 122 (334)\nS0 :=\nn\n{p, q} ∈\n\u0000Q\n2\n\u0001\n| p ∈ F, q̸∈ F\no\nSk+1 := Sk\n∪\nn\n{p, q} ∈\n\u0000Q\n2\n\u0001\n\\ Sk | a ∈ Σ, pa− →M p′, qa− →M q′, {p′, q′} ∈Sk\no\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n0 ∼\n0\n1 ∼\n1\n2 ∼\n2\n3 ∼\n3\n4 ∼\n4\n▷ We only need to consider unordered pairs\n\u0000Q\n2\n\u0001\n,\nbut usually it is simpler to include {q, q} into the table; trivially, q ∼L q.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Minimizing DFAs: example 122 (335)\nS0 :=\nn\n{p, q} ∈\n\u0000Q\n2\n\u0001\n| p ∈ F, q̸∈ F\no\nSk+1 := Sk\n∪\nn\n{p, q} ∈\n\u0000Q\n2\n\u0001\n\\ Sk | a ∈ Σ, pa− →M p′, qa− →M q′, {p′, q′} ∈Sk\no\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n0 ∼\n0\n1 ∼\n1\n2 ∼\n2\n3 ∼\n3\n4 ∼\n4\nε, 0 ε, 0 ε, 0 ε, 0\n▷ Initially mark all unordered final-non-final-pairs by ε\nas ε separates every final state from any non-final state.\n▷ Sometimes it helps to also add a time stamp.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Minimizing DFAs: example 122 (336)\nS0 :=\nn\n{p, q} ∈\n\u0000Q\n2\n\u0001\n| p ∈ F, q̸∈ F\no\nSk+1 := Sk\n∪\nn\n{p, q} ∈\n\u0000Q\n2\n\u0001\n\\ Sk | a ∈ Σ, pa− →M p′, qa− →M q′, {p′, q′} ∈Sk\no\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n0 ∼\n0\n1 ∼\n1\n2 ∼\n2\n3 ∼\n3\n4 ∼\n4\nε, 0 ε, 0 ε, 0 ε, 0\na, 1\na, 1\na, 1\na, 1\n▷ for all newly marked pairs {p′, q′} check if they have an unmarked\na-predecessor {p, q} (i.e. pa − →M p′ and qa − →M q′) and mark it by a:\ne.g. mark {2, 3} by a as 2a − →M 2 and 3a − →M 4 and {2, 4} ∈S0.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Minimizing DFAs: example 122 (337)\nS0 :=\nn\n{p, q} ∈\n\u0000Q\n2\n\u0001\n| p ∈ F, q̸∈ F\no\nSk+1 := Sk\n∪\nn\n{p, q} ∈\n\u0000Q\n2\n\u0001\n\\ Sk | a ∈ Σ, pa− →M p′, qa− →M q′, {p′, q′} ∈Sk\no\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n0 ∼\n0\n1 ∼\n1\n2 ∼\n2\n3 ∼\n3\n4 ∼\n4\nε, 0 ε, 0 ε, 0 ε, 0\na, 1\na, 1\na, 1\na, 1\n∼\n∼\n▷ Repeat until no new pair has been marked.\n▷ This is here already the case s.t. we may collapse {0, 2} to 0 and {1, 3} to 1.\n(For later: {0, 2} corresponds to [ε]∼L and {1, 3} to [b]∼L.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Minimizing DFAs: example 122 (338)\nS0 :=\nn\n{p, q} ∈\n\u0000Q\n2\n\u0001\n| p ∈ F, q̸∈ F\no\nSk+1 := Sk\n∪\nn\n{p, q} ∈\n\u0000Q\n2\n\u0001\n\\ Sk | a ∈ Σ, pa− →M p′, qa− →M q′, {p′, q′} ∈Sk\no\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\n0 ∼\n0\n1 ∼\n1\n2 ∼\n2\n3 ∼\n3\n4 ∼\n4\nε, 0 ε, 0 ε, 0 ε, 0\na, 1\na, 1\na, 1\na, 1\n∼\n∼\n02start 13 4\na\nb\nb\na\na, b\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode relation 123 (339)\nD For a language L ⊆ Σ∗ let ∼L denote the binary relation\n∼L:= {(w, w′) ∈ Σ∗ × Σ∗ | for all z ∈ Σ∗ : wz ∈ L iff w′z ∈ L}\n! If “w ∼L w′”, any FA accepting L has to treat wz and w′z the same.\nF ∼L is an equivalence relation again by the properties of the biconditional\n(“iff”, “↔”).\nD As ∼L is an equivalence relation:\n▷ [w]∼L := {w′ ∈ Σ∗ | w ∼L w′} denotes the (equivalence) class of w ∈ Σ∗.\n▷ Σ∗/ ∼L:= {[w]∼L | w ∈ Σ∗} is the quotient/partition.\n▷ z ∈ Σ∗ separates w, w′ ∈ Σ∗ wrt. ∼L if\n(i) wz ∈ L and w′z ̸∈ L or (ii) wz ̸∈ L and w′z ∈ L\n(i.e. z yields a counter-example for w ∼L w′ resp. z is a witness of w ̸∼L w′.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode relation 123 (340)\nD For a language L ⊆ Σ∗ let ∼L denote the binary relation\n∼L:= {(w, w′) ∈ Σ∗ × Σ∗ | for all z ∈ Σ∗ : wz ∈ L iff w′z ∈ L}\n▷ E.g. for L = L(a∗b∗) with Σ = {a, b}:\n[ε]∼L = L(a∗) [ b]∼L = L(a∗bb∗) [ ba]∼L = L(a∗b∗ba(a + b)∗)\nThis is a finite partition of Σ∗ and e.g.\n▷ z = a separates ε and b: ε a∈ L but b a̸∈ L.\n▷ z = ε separates ε and ba: ε ε∈ L but ba ε̸∈ L.\n▷ z = ε separates b and ba: b ε∈ L but ba ε̸∈ L.\nwhile for all z ∈ Σ∗:\n▷ ak ∼L ε as: ak z ∈ L iff z = albm iff εz ∈ L\n▷ akblb ∼L b as: akblb z∈ L iff z = bm iff bz ∈ L\n▷ akblbax ∼ ba as: akbax z̸∈ L\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode relation 123 (341)\nD For a language L ⊆ Σ∗ let ∼L denote the binary relation\n∼L:= {(w, w′) ∈ Σ∗ × Σ∗ | for all z ∈ Σ∗ : wz ∈ L iff w′z ∈ L}\n▷ E.g. for L = {akbk | k ∈ N 0} with Σ = {a, b}:\n\u0002\nak\u0003\n∼L\n= {ak} (k ∈ N 0)\u0002\nakab\n\u0003\n∼L\n= {ak+l+1bl+1 | l ∈ N 0} (k ∈ N 0)\n[b]∼L\n= {akbkx | k ∈ N 0, x∈ Σ+}\nThis is an infinite partition of Σ∗:\n▷ z = bk separates akab from b and any aiab with i ̸= k.\n▷ z = abk+1 separates ak from all other words\nwhile for all z ∈ Σ∗\n▷ ak+l+1bl+1 ∼L ak+1b as: ak+l+1bl+1 z ∈ L iff z = bk iff ak+1b z∈ L.\n▷ akbkx ∼L b (with x ̸= ε) as: akbkx z̸∈ L\n(Visualize akbl as going k times “up and right” and l times “down and right”.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode relation 123 (342)\nD For a language L ⊆ Σ∗ let ∼L denote the binary relation\n∼L:= {(w, w′) ∈ Σ∗ × Σ∗ | for all z ∈ Σ∗ : wz ∈ L iff w′z ∈ L}\nF By definition of ∼L:\nIf w ∼L w′, then also wa ∼L w′a for every a ∈ Σ.\n▷ In other words: if [w]∼L = [w′]∼L, then also [wa]∼L = [w′a]∼L for any a ∈ Σ.\n! Σ∗/ ∼L can be visualized as edge-labeled directed graph\n(Σ∗/ ∼L, {([w]∼L, a,[wa]∼L) | a ∈ Σ})\ni.e. we take the equivalence classes as nodes, and draw an edge labeled by a\nfrom [w]∼L to [wa]∼L.\n(This is the quotient of the infinite tree (Σ∗, RΣ) wrt. ∼L: the equivalence classes wrt. ∼L\n“color” the nodes of Σ∗, and when taking the quotient the tree is folded into an\nautomaton.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode relation 123 (343)\nD For a language L ⊆ Σ∗ let ∼L denote the binary relation\n∼L:= {(w, w′) ∈ Σ∗ × Σ∗ | for all z ∈ Σ∗ : wz ∈ L iff w′z ∈ L}\n▷ E.g. for L = L(a∗b∗):\nε\na b\naa ab ba bb\naaa aab aba abb baa bab bba bbb\nεstart b ba\na\nb\nb\na\na, b\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode relation 123 (344)\nD For a language L ⊆ Σ∗ let ∼L denote the binary relation\n∼L:= {(w, w′) ∈ Σ∗ × Σ∗ | for all z ∈ Σ∗ : wz ∈ L iff w′z ∈ L}\n▷ E.g. for L = {akbk | k ∈ N 0}: (red “sink” state not shown below)\nε\na b\naa ab ba bb\naaa aab aba abb baa bab bba bbb\n1\n2\n3 1\n0\n1\n0start 1\n1\n2\n1\n3\n2\n. . .\n. . .\na a\nb b\na\nb\nb\na\nb\nb\nb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode vs. DFA 124 (345)\nL If |Σ∗/ ∼L | < ∞, the quotient automaton wrt. ∼L\nML = (Σ∗/ ∼L, Σ, {(([w]∼L, a), [wa]∼L) | a ∈ Σ}, [ε]∼L, {[w]∼L | w ∈ L})\nis a complete DFA with L = L(ML).\n▷ By definition/construction:\nevery equivalence class [w]∼L has exactly one a-successor [wa]∼L, and\nthe maximal run on input w ∈ Σ∗ ends in [w]∼L.\nThus: ML accepts w iff w ∈ L.\n▷ If L is regular, then the equivalence class [u]∼L is the regular language of\nwords for which ML ends up in [u]∼L, i.e. the language accepted by\n(Σ∗/ ∼L, Σ, {(([w]∼L, a), [wa]∼L) | a ∈ Σ}, [ε]∼L, {[u]∼L})\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode vs. DFA 124 (346)\nL If |Σ∗/ ∼L | < ∞, the quotient automaton wrt. ∼L\nML = (Σ∗/ ∼L, Σ, {(([w]∼L, a), [wa]∼L) | a ∈ Σ}, [ε]∼L, {[w]∼L | w ∈ L})\nis a complete DFA with L = L(ML).\nC Let M = (Q, Σ, δ, q0, F) be a complete DFA M with L = L(M). Then\n(Q/ ∼M , Σ, {(([p]∼M , a), [q]∼M ) | pa − →∗\nM q}, [q0]∼M , F/∼M )\nis isomorphic with ML. In particular |Σ∗/ ∼L | ≤ |Q| < ∞.\n▷ Fix for any state q a word wq so that q0wq − →∗\nM q.\nThis defines an isomorphism Q/ ∼M → Σ∗/ ∼L, [q]∼M 7→ [wq]∼L.\ninjective: wp ∼L wq iff p ∼M q.\nsurjective: every w ∈ Σ∗ defines qw ∈ Q by q0w − →∗\nM qw where w ∼L wqw .\nhomomorphism: if pa − →M q, then wpa ∼L wq.\nC L := {akbk | k ∈ N 0} is not regular as |Σ∗/ ∼L | = ∞.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode vs. DFA: Examples 125 (347)\nε\na b\naa ab ba bb\naaa aab aba abb baa bab bba bbb\n0\n2\n2\n2\n1\n1\n1\n3\n3 3\n4\n4 4 44\n0start 2\n1\n3\n4\na\nb\na\nb\na\nb b\na\na, b\nεstart b ba\na\nb a\nb\na, b\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode vs. NFA 126 (348)\n▷ NFAs can be exponentially more succinct than the minimal DFA:\n▷ Fix any n ∈ N and consider L = L((a + b)∗a(a + b)n).\n▷ An NFA with n + 1 states can simply “guess” when the last n + 1 letters will\nbe read. For simplicity consider only n = 1:\n0start 1 2\na, b\na a, b\n▷ The power-set construction yields the DFA M (n = 1):\nq100start q110\nq101\nq111\nb\na a\nbb a b\na\nwhich is already minimal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode vs. NFA 126 (349)\n▷ NFAs can be exponentially more succinct than the minimal DFA:\n▷ Fix any n ∈ N and consider L = L((a + b)∗a(a + b)n).\n▷ The power-set construction yields the DFA M (n = 1):\nq100start q110\nq101\nq111\nb\na a\nbb a b\na\nwhich is already minimal.\nThe states thus represent the equivalence classes of ∼L\nq100 ˆ =[bb]∼L = L(1 + b + (a + b)∗bb) q100 ˆ =[ba]∼L = L(a + (a + b)∗ba)\nq101 ˆ =[ab]∼L = L((a + b)∗ab) q111 ˆ =[aa]∼L = L((a + b)∗aa)\ni.e. the states correspond to a “sliding window” of length 2:\n[x1x2]∼La − →M [x2a]∼L\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode vs. NFA 126 (350)\n▷ NFAs can be exponentially more succinct than the minimal DFA:\n▷ Fix any n ∈ N and consider L = L((a + b)∗a(a + b)n).\n▷ The power-set construction yields the DFA M (n = 1):\nbbstart ba\nab\naa\nb\na a\nbb a b\na\nwhich is already minimal.\nThe states thus represent the equivalence classes of ∼L\nq100 ˆ =[bb]∼L = L(1 + b + (a + b)∗bb) q100 ˆ =[ba]∼L = L(a + (a + b)∗ba)\nq101 ˆ =[ab]∼L = L((a + b)∗ab) q111 ˆ =[aa]∼L = L((a + b)∗aa)\ni.e. the states correspond to a “sliding window” of length 2:\n[x1x2]∼La − →M [x2a]∼L\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode vs. NFA 126 (351)\n▷ NFAs can be exponentially more succinct than the minimal DFA:\n▷ Fix any n ∈ N and consider L = L((a + b)∗a(a + b)n).\n▷ The power-set construction yields the DFA M (n = 1):\nbbstart ba\nab\naa\nb\na a\nbb a b\na\nwhich is already minimal.\n▷ In general |Σ∗/ ∼L | = 2n+1:\n▷ Fix any two words x = x1 . . . xn+1 and y = y1 . . . yn+1 of length n + 1.\nAssume x1 . . . xk = y1 . . . yk but wlog xk+1 = a and yk+1 = b.\nAppending bk pushes out the k first letters s.t. xbk ∈ L and ybk ̸∈ L.\nAs a sliding window of size n + 1 suffices, also |Σ∗/ ∼L | = 2n+1.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Using Myhill-Nerode 127 (352)\n▷ In order to show that L ⊆ Σ∗ is not regular:\nfind infinitely many pairwise inequivalent words (w(k))k∈N 0\n(i.e. w(i) ̸∼L w(j) if i ̸= j).\n▷ E.g. whenever we need to store some unbounded counter (or use in general\nunbounded memory) a natural first attempt is to pick for each counter value k\nsome corresponding word w(k), and then try to show their inequivalence.\n▷ Typical example: L = {akbk | k ∈ N 0} with w(k) = akbk.\n▷ In order to compute Σ∗/ ∼L for some given regular L ⊆ Σ∗:\nDetermine some FA for L, if necessary determinize it, then minimize it.\n▷ The states of a minimal DFA correspond to the equivalence classes:\nSimply determine the regular languages that lead from the initial state to a\nspecific state of the given minimal DFA.\n▷ See the example for L(a∗b∗).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Myhill-Nerode, minimization: remarks 128 (353)\n▷ Hopcroft’s algorithm and variants:\nInstead of building the |Q|2 table, we can start with the initial partition\nP0 = {F, Q\\ F} where all accepting resp. rejecting states are assumed to be\nequivalent, and then iteratively split every class C of Pk whose members can\nbe separated by a letter (i.e. the a-successors of at least two members of C are\ncontained in different classes of Pk); can be implemented to run in\nO(|Σ| · |Q|log |Q|).\n▷ Brzozowski’s algorithm:\nLet MR\nD denote the DFA obtained by first reversing the transitions of M and\napplying the power-set construction; then (MR\nD)R\nD is a minimal DFA for\nL(M).\n▷ L∗ algorithm/minimal adequate teacher by Dana Angluin:\nConsiders the setting where a DFA for an unkown L should be learned by\nmeans of examples (w ∈ L) and counterexamples (w ̸∈ L): it approximates ∼L\nby learning both representatives and separating words from counterexamples.",
      "L(M).\n▷ L∗ algorithm/minimal adequate teacher by Dana Angluin:\nConsiders the setting where a DFA for an unkown L should be learned by\nmeans of examples (w ∈ L) and counterexamples (w ̸∈ L): it approximates ∼L\nby learning both representatives and separating words from counterexamples.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Learning DFAs (*) 129 (354)\n▷ The “learner” is allowed to two kind of queries: “ w\n?\n∈ L” and “ L(M)\n?\n= L”.\nThe “teacher” answers correctly; if L(M) ̸= L, returns a counterexample.\n▷ In every iteration, we (the learner) have a (incomplete) system of representatives\n(states) Q wrt. ∼L and a set T of separating (test) words. We\nunderapproximate ∼L by means of (using the teacher to decide uz\n?\n∈ L)\nu ∼T v if for all z ∈ T : uz ∈ L iff vz\n▷ The (unknown) language is L = {w ∈ {a, b}∗ : |w|b ≡4 3} (cf. Worrell).\n▷ Initially: Q = {ew} and T = {ε}. Using the teacher we learn ε ̸∈ L (i.e.\nrejecting) and ε a∼T ε ∼T ε b. This yields the initial guess:\nεstart\na, b\n▷ Assume the “teacher” returns the counterexample bbb.\nWe use the run on bbb to find a mistake: as L ∋ ε bbb− →M ε bb̸∈ L we learn\nthat bb separates ε from b and accordingly update Q := {ε, b} and",
      "rejecting) and ε a∼T ε ∼T ε b. This yields the initial guess:\nεstart\na, b\n▷ Assume the “teacher” returns the counterexample bbb.\nWe use the run on bbb to find a mistake: as L ∋ ε bbb− →M ε bb̸∈ L we learn\nthat bb separates ε from b and accordingly update Q := {ε, b} and\nT := {ε, bb}. As b ̸∈ L, it has to be rejecting, too.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Learning DFAs (*) 129 (355)\n▷ The “learner” is allowed to two kind of queries: “ w\n?\n∈ L” and “ L(M)\n?\n= L”.\nThe “teacher” answers correctly; if L(M) ̸= L, returns a counterexample.\n▷ In every iteration, we (the learner) have a (incomplete) system of representatives\n(states) Q wrt. ∼L and a set T of separating (test) words. We\nunderapproximate ∼L by means of (using the teacher to decide uz\n?\n∈ L)\nu ∼T v if for all z ∈ T : uz ∈ L iff vz\n▷ Wrt. Q = {ε, b} and T = {ε, bb} we check, if the possible transitions are\nclosed: As ε a∼T ε ∼T b band b a∼T b ∼T ε b, we obtain:\nεstart b\na a\nb\nb\n▷ Assume the “teacher” returns again the counterexample bbb.\nWe again use the run L ∋ ε bbb− →M b bb− →M ε b̸∈ L to learn that b separates\nε from bb and hence update Q := {ε, b, bb} and T = {ε, b, bb}. Again, bb has\nto be rejecting as bb ̸∈ L.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Learning DFAs (*) 129 (356)\n▷ The “learner” is allowed to two kind of queries: “ w\n?\n∈ L” and “ L(M)\n?\n= L”.\nThe “teacher” answers correctly; if L(M) ̸= L, returns a counterexample.\n▷ In every iteration, we (the learner) have a (incomplete) system of representatives\n(states) Q wrt. ∼L and a set T of separating (test) words. We\nunderapproximate ∼L by means of (using the teacher to decide uz\n?\n∈ L)\nu ∼T v if for all z ∈ T : uz ∈ L iff vz\n▷ We again check, if the transition are closed wrt. Q = {ε, b, bb} and\nT = {ε, b, bb}.\nThe new separating word b yields that bb b̸∼T ε, b, bb, i.e. the b-transition\nleaving bb requires an additional state bbb (accepting as bbb ∈ L).\nThis leads to the final guess:\nεstart b bb bbb\na a a a\nb b b\nb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Learning DFAs (*) 129 (357)\n▷ The “learner” is allowed to two kind of queries: “ w\n?\n∈ L” and “ L(M)\n?\n= L”.\nThe “teacher” answers correctly; if L(M) ̸= L, returns a counterexample.\n▷ In every iteration, we (the learner) have a (incomplete) system of representatives\n(states) Q wrt. ∼L and a set T of separating (test) words. We\nunderapproximate ∼L by means of (using the teacher to decide uz\n?\n∈ L)\nu ∼T v if for all z ∈ T : uz ∈ L iff vz\n▷ Hence, we can also learn in this way an NDD representing a linear constraint\n⟨a, x⟩2 ≤ c (a, x∈ Z n, c∈ Z ) or more generally a constraint in Presburger\narithmetic.\n▷ Angluin discusses in the original article how this approach can be extended to\na teacher that gives the correct answer to L(M)\n?\n= L only with sufficiently\nhigh probability (cf. probably approximately correct learning) . This can be used\ne.g. to learn a model of an unknown system or environment.\n▷ E.g. Weiss et al. use the L∗ algorithm to approximate/compress recurrent",
      "a teacher that gives the correct answer to L(M)\n?\n= L only with sufficiently\nhigh probability (cf. probably approximately correct learning) . This can be used\ne.g. to learn a model of an unknown system or environment.\n▷ E.g. Weiss et al. use the L∗ algorithm to approximate/compress recurrent\nneural networks (RNNs). (In principle, an RNN uses R n as state space and input\nalphabet, but in practice only finite precision is used, see e.g. here. Still RNNs have a\nhuge state space with a hard to understand transition relation.)No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "5 Regular languages\nRegular grammars and finite automata\nMoving between regular grammars and finite automata\nClosure properties\nRegular expressions\nMinimizing DFAs and Myhill-Nerode\nBasics of probabilistic finite automata (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Probabilistic finite automata 131 (359)\nD Bra-ket notation: With R n = R n×1, let |i⟩ := ei ∈ R n denote the i-th\ncanonical unit column vector, and ⟨i| := e⊤\ni its transposed row vector.\nD Probabilistic finite automaton (PFA): M = (Σ, {Pa ∈ R n×n | a ∈ Σ}, F)\n▷ wlog [n] = {1, 2, . . . , n} is the set of states with 1 the initial state.\n▷ Σ: the finite input alphabet, e.g. Σ = {0, 1}\n▷ F: the final states with F ⊆ [n]\n▷ Pa: a row/right stochastic n × n-matrix (usually even rational), i.e.:\n⟨q|Pa|q′⟩ ≥0\nX\nq′∈[n]\n⟨q|Pa|q′⟩ = 1\n▷ For a word w = uv ∈ Σ∗ set (s.t. w 7→ Pw is a monoid homomorphism) :\nPε := Idn Puv := PuPv\nThen its acceptance probability is P\nqf ∈F ⟨1|Pw|qf ⟩.\n▷ For η ∈ [0, 1) the η-language accepted by M is\nL(M, η) := {w ∈ Σ∗ |\nX\nqf ∈F\n⟨1|Pw|qf ⟩ > η}\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Probabilistic finite automata: basic properties 132 (360)\nP0 :=\n\u00121 0\n1\n2\n1\n2\n\u0013\nP1 :=\n\u00121\n2\n1\n2\n0 1\n\u0013\n1start 2\n0: 1, 1: 12 1: 1, 0: 12\n1: 12\n0: 12\n▷ PFAs were introduced by Rabin (cf. here).\n! PFA: letter determines prob.; Markov model: prob. determines letter.\n▷ For a given η ∈ [0, 1) above PFA accepts\nL(M, η) = {a1 . . . al ∈ {0, 1}∗ | al2−1 + al−12−2 + . . .+ a12−l > η}\nFix any (computable) enumeration w(1), w(2), . . .of {0, 1}∗ and set\nη = 0.w(1)w(2) . . .; then L = L(M, η) will be non-regular (Myhill-Nerode).\nη itself is also not regular/rational; if the enumeration is not computable,\nthen L(M, η) is not enumerable/semidecidable/recognizable.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Probabilistic finite automata: basic properties 132 (361)\nP0 :=\n\u00121 0\n1\n2\n1\n2\n\u0013\nP1 :=\n\u00121\n2\n1\n2\n0 1\n\u0013\n1start 2\n0: 1, 1: 12 1: 1, 0: 12\n1: 12\n0: 12\n▷ Wrt. η = 0 every PFA M is just an NFA:\nIn above example L(M, 0) = L(0∗1(0 + 1)∗)\n▷ Every DFA M is also an PFA s.t. L(M, η) = L for all η ∈ [0, 1):\nFor every w ∈ L(M) (w ̸∈ L(M)) its acceptance probability is 1 (0).\n▷ PFAs cannot accept certain (linear) context-free languages (see e.g. here).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Probabilistic finite automata: basic properties 132 (362)\nP0 :=\n\u00121 0\n1\n2\n1\n2\n\u0013\nP1 :=\n\u00121\n2\n1\n2\n0 1\n\u0013\n1start 2\n0: 1, 1: 12 1: 1, 0: 12\n1: 12\n0: 12\n▷ For practical use, we actually want some error bound γ >0 s.t.\n▷ every word w ̸∈ L has an acceptance prob. of at most ≤ 1/2 − γ,\n▷ every word w ∈ L has an acceptance prob. of at least ≥ 1/2 + γ.\nη is called an isolated cut-point of a given PFA M if there exists a γ >0 s.t.\nfor all w ∈ Σ∗ :\n\f\f\f\f\f\f\nX\nqf ∈F\n⟨1|Pw|qf ⟩ −η\n\f\f\f\f\f\f\n≥ γ\nIf η is an isolated cut-point of M, then L(M, η) is regular (Rabin).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Probabilistic finite automata: basic properties 132 (363)\nP0 :=\n\u00121 0\n1\n2\n1\n2\n\u0013\nP1 :=\n\u00121\n2\n1\n2\n0 1\n\u0013\n1start 2\n0: 1, 1: 12 1: 1, 0: 12\n1: 12\n0: 12\n▷ Similar to NFAs, also PFAs with isolated cut-points can be more succinct\nthen the minimal DFA which was also shown by Rabin by adapting above\nPFA to the ternary representation of the Cantor set:\n▷ Then ⟨1|Pa1...al|2⟩ = al3−1 + . . .+ a13−l for a1 . . . al ∈ {0, 2}∗.\n▷ For η = 2 · 3−1 + . . .+ 2 · 3−n + 1 · 3−(n+1) + 1 · 3−(n+2):\nL(M, η) = L((0 + 2)∗2n+1)\nwhich can only be accepted by a DFA with at least n + 2 states.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Probabilistic finite automata: basic properties 132 (364)\nP0 :=\n\u00121 0\n2\n3\n1\n3\n\u0013\nP2 :=\n\u00121\n3\n2\n3\n0 1\n\u0013\n1start 2\n0: 1, 2: 13 2: 1, 0: 13\n2: 23\n0: 23\n▷ Similar to NFAs, also PFAs with isolated cut-points can be more succinct\nthen the minimal DFA which was also shown by Rabin by adapting above\nPFA to the ternary representation of the Cantor set:\n▷ Then ⟨1|Pa1...al|2⟩ = al3−1 + . . .+ a13−l for a1 . . . al ∈ {0, 2}∗.\n▷ For η = 2 · 3−1 + . . .+ 2 · 3−n + 1 · 3−(n+1) + 1 · 3−(n+2):\nL(M, η) = L((0 + 2)∗2n+1)\nwhich can only be accepted by a DFA with at least n + 2 states.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Probabilistic finite automata: basic properties 132 (365)\nP0 :=\n\u00121 0\n2\n3\n1\n3\n\u0013\nP2 :=\n\u00121\n3\n2\n3\n0 1\n\u0013\n1start 2\n0: 1, 2: 13 2: 1, 0: 13\n2: 23\n0: 23\n▷ In contrast to FAs the (strict) emptyness problem is undecidable for PFAs:\nGiven an PFA M and a threshold η ∈ (0, 1) decide L(M, η) = ∅.\nas a PCP instance can be encoded into a PFA (see e.g. here).\n▷ For more information see e.g. “introduction to probabilisitc automata” by Paz\n▷ Conceptually related are (measure once) quantum finite automaton.\nPhysics require that transition matrices are now unitary.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "6 Context-free languages\nContext-free grammars: basic properties\nPush-down automata\nDerivation trees\nPumping lemma and closure properties\nAlgebraic equations and EBNF (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-free languages and grammars (reminder) 135 (368)\nD A grammar G = (V, Σ, P, S) is context-free if P ⊆ V × (Σ ∪ V )∗.\n▷ Recall: the Chomsky hierarchy actually requires P ⊆ V × (Σ ∪ V )+, but\nε-rules only add ε to L(G) and can be removed.\nL ⊆ Σ∗ is context-free if L = L(G) for some context-free grammar G.\n▷ G is in Chomsky normal form if P ⊆ V × (Σ ∪ V V).\n▷ G is in Greibach normal form if P ⊆ V × ΣV ∗.\n! For both normal forms, the usual convention is that S − →G ε is allowed if S\ndoes not occur on the right-hand side of any rule.\nA derivation\nS − →G α1 − →G α2 − →G . . .\nis leftmost if in every step the leftmost variable X1 of the current sentential\nform αi = w0X1w1 . . . Xlwl (wi ∈ Σ∗, Xi ∈ V ) is rewritten:\n▷ E.g. consider S − →G ε | aSb | SS\nS − →G SS − →G SaSb − →G aSbaSb − →G aSbaaSbb − →G aSbaabb − →G abaabb\nS − →G SS − →G aSbS − →G abS − →G abaSb − →G abaaSbb − →G abaabb",
      "form αi = w0X1w1 . . . Xlwl (wi ∈ Σ∗, Xi ∈ V ) is rewritten:\n▷ E.g. consider S − →G ε | aSb | SS\nS − →G SS − →G SaSb − →G aSbaSb − →G aSbaaSbb − →G aSbaabb − →G abaabb\nS − →G SS − →G aSbS − →G abS − →G abaSb − →G abaaSbb − →G abaabb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A linear context-free grammar for {anbn | n ∈ N } 136(369)\nV = {S} Σ = {a, b}\nS − →G aSb | ab\nS − →G aSb\n− →G aaSbb\n− →G aaaSbbb\n− →G aaaabbbb\nD A context-free grammar is linear if on the right-hand side of every its rules at\nmost one nonterminal occurs.\n▷ Regular grammars “=” right-linear grammar.\n▷ Every right-linear grammar is equivalent to a left-linear grammar and v.v.\n▷ Recall that {anbn | n ∈ N 0} is not regular.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A linear context-free grammar for {anbn | n ∈ N } 136(370)\nV = {S} Σ = {a, b}\nS − →G aSb | ab\nS − →G aSb\n− →G aaSbb\n− →G aaaSbbb\n− →G aaaabbbb\nD A context-free grammar is linear if on the right-hand side of every its rules at\nmost one nonterminal occurs.\n▷ Regular grammars “=” right-linear grammar.\n▷ Every right-linear grammar is equivalent to a left-linear grammar and v.v.\n▷ Recall that {anbn | n ∈ N 0} is not regular.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A linear context-free grammar for {anbn | n ∈ N } 136(371)\nV = {S} Σ = {a, b}\nS − →G aSb | ab\nS − →G aSb\n− →G aaSbb\n− →G aaaSbbb\n− →G aaaabbbb\nD A context-free grammar is linear if on the right-hand side of every its rules at\nmost one nonterminal occurs.\n▷ Regular grammars “=” right-linear grammar.\n▷ Every right-linear grammar is equivalent to a left-linear grammar and v.v.\n▷ Recall that {anbn | n ∈ N 0} is not regular.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A linear context-free grammar for {anbn | n ∈ N } 136(372)\nV = {S} Σ = {a, b}\nS − →G aSb | ab\nS − →G aSb\n− →G aaSbb\n− →G aaaSbbb\n− →G aaaabbbb\nD A context-free grammar is linear if on the right-hand side of every its rules at\nmost one nonterminal occurs.\n▷ Regular grammars “=” right-linear grammar.\n▷ Every right-linear grammar is equivalent to a left-linear grammar and v.v.\n▷ Recall that {anbn | n ∈ N 0} is not regular.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A linear context-free grammar for palindromes 137 (373)\nV = {S} Σ = {a, b}\nS − →G aSa | bSb | ε | a | b\nS − →G aSa\n− →G abSba\n− →G abbSbba\n− →G abbaabba\nD wR := al . . . a1 for w = a1 . . . al ∈ Σ∗.\nD A word is a palindrome if w = wR.\n▷ {w ∈ {a, b}∗ | w = wR} is also not regular,\ne.g. [ab1ab2ab3a . . . abka]∼L have to be distinct equivalence classes for k ∈ N .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A linear context-free grammar for palindromes 137 (374)\nV = {S} Σ = {a, b}\nS − →G aSa | bSb | ε | a | b\nS − →G aSa\n− →G abSba\n− →G abbSbba\n− →G abbaabba\nD wR := al . . . a1 for w = a1 . . . al ∈ Σ∗.\nD A word is a palindrome if w = wR.\n▷ {w ∈ {a, b}∗ | w = wR} is also not regular,\ne.g. [ab1ab2ab3a . . . abka]∼L have to be distinct equivalence classes for k ∈ N .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A linear context-free grammar for palindromes 137 (375)\nV = {S} Σ = {a, b}\nS − →G aSa | bSb | ε | a | b\nS − →G aSa\n− →G abSba\n− →G abbSbba\n− →G abbaabba\nD wR := al . . . a1 for w = a1 . . . al ∈ Σ∗.\nD A word is a palindrome if w = wR.\n▷ {w ∈ {a, b}∗ | w = wR} is also not regular,\ne.g. [ab1ab2ab3a . . . abka]∼L have to be distinct equivalence classes for k ∈ N .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A linear context-free grammar for palindromes 137 (376)\nV = {S} Σ = {a, b}\nS − →G aSa | bSb | ε | a | b\nS − →G aSa\n− →G abSba\n− →G abbSbba\n− →G abbaabba\nD wR := al . . . a1 for w = a1 . . . al ∈ Σ∗.\nD A word is a palindrome if w = wR.\n▷ {w ∈ {a, b}∗ | w = wR} is also not regular,\ne.g. [ab1ab2ab3a . . . abka]∼L have to be distinct equivalence classes for k ∈ N .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Unary-binary trees and Dyck languages 138 (377)\nV = {S} Σ = {[, ]}\nS − →G [] | [S] | [SS]\n\"\u0014h\u0002\n[ ]\n\u0003ih\n[ ]\ni\u0015\u0014 hi \u0015#\n▷ Main use of context-free grammars is to define well-formed/structured\nwords/texts like propositional formulas, arithmetic expressions, regular\nexpressions, programs: nested parentheses impose a tree structure on the\nword.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (378)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (379)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (380)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (381)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (382)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (383)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\n− →G ((¬X op X ) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (384)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\n− →G ((¬X op X ) op F)\n− →G ((¬X op x) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (385)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\n− →G ((¬X op X ) op F)\n− →G ((¬X op x) op F)\n− →G ((¬X′ op x) op F)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (386)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\n− →G ((¬X op X ) op F)\n− →G ((¬X op x) op F)\n− →G ((¬X′ op x) op F)\n− →G ((¬X′ op x) op X)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (387)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\n− →G ((¬X op X ) op F)\n− →G ((¬X op x) op F)\n− →G ((¬X′ op x) op F)\n− →G ((¬X′ op x) op X)\n− →G ((¬X′ op x) op X′)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (388)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\n− →G ((¬X op X ) op F)\n− →G ((¬X op x) op F)\n− →G ((¬X′ op x) op F)\n− →G ((¬X′ op x) op X)\n− →G ((¬X′ op x) op X′)\n− →G ((¬x′ op x) op X′)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (389)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\n− →G ((¬X op X ) op F)\n− →G ((¬X op x) op F)\n− →G ((¬X′ op x) op F)\n− →G ((¬X′ op x) op X)\n− →G ((¬X′ op x) op X′)\n− →G ((¬x′ op x) op X′)\n− →G ((¬x′ op x) op x′)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (390)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\n− →G ((¬X op X ) op F)\n− →G ((¬X op x) op F)\n− →G ((¬X′ op x) op F)\n− →G ((¬X′ op x) op X)\n− →G ((¬X′ op x) op X′)\n− →G ((¬x′ op x) op X′)\n− →G ((¬x′ op x) op x′)\n− →G ((¬x′ op x) ∧ x′)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "A context-free grammar for propositional formulas 139 (391)\nV = {F, op, X} S = F Σ = {(, ), ¬, ∧, ∨, x,′ }\nF − →G ¬F | (F op F ) | X\nX − →G X′ | x\nop − →G ∧ | ∨\nF − →G (F op F )\n− →G ((F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op F ) op F)\n− →G ((¬F op X ) op F)\n− →G ((¬X op X ) op F)\n− →G ((¬X op x) op F)\n− →G ((¬X′ op x) op F)\n− →G ((¬X′ op x) op X)\n− →G ((¬X′ op x) op X′)\n− →G ((¬x′ op x) op X′)\n− →G ((¬x′ op x) op x′)\n− →G ((¬x′ op x) ∧ x′)\n− →G ((¬x′ ∨ x) ∧ x′)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CFG, straight-line programs and compression 140 (392)\nX1 − →G X2X2 . . . Xn−1 − →G XnXn Xn − →G a\nY1 − →G baY2Y2 Y2 − →G na\nD A straight-line program (SLP) for a word w ∈ Σ∗ is a context-free grammar\nG = (V, Σ, P, X1) with V = {X1, . . . , Xn} so that:\n▷ for every nonterminal Xi there is exactly one rule (Xi, γi) ∈ P,\nand if Xj occurs in the right-hand side γi, then j > i.\ni.e. Xi can only “call” Xj with j > i.\n(If we allow (Xi, XjXi) ∈ P with j > i, then L(G) will be regular.)\n▷ SLPs for words are related to LZ compression.\n▷ If we allow multiple rules for each nonterminal, we obtain a circuit (=a\ndirected acyclic graph) that compresses a finite language by means of\nsubterm sharing (=storing isomoprhic subtrees only once).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-free grammars and recursive programs with finite memory\n141(393)\ndef P(x):\n...\nif ...:\ny = Q(x)\nz = R(y)\nelse:\nz = S(x)\nreturn z\nPxz − →G QxyRyz | Sxz\n▷ Context-free grammars are closely related to recursive programs:\n▷ Nonterminals correspond to procedures.\n▷ Variables with finite ranges can be encoded into the nonterminals.\n▷ Every production rule encodes a potential sequence of recursive calls.\n▷ If we allow the symbols V ∪ Σ to commute, then context-free grammars\n(modulo commutativity) can describe basic parallelism without synchronization:\n▷ A rule X − →G Y Znow stands for: “process X splits into two independent\nparallel processes Y, Z” Also related to communication-free Petri nets.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Recursive descent parser 142 (394)\nstr w = ’¬ ∨0 ∧ 10’\nint i = 0\ndef eval_F():\nx = w[i]\ni += 1\nif x == ’0’:\nreturn False\nIf x == ’1’:\nreturn True\nif x == ’¬’:\nreturn not parse_F()\nl = parse_F()\nr = parse_F()\nif x == ’∨’:\nreturn l or r\nIf x == ’∧:\nreturn l and r\nassert False\n▷ Recursive descent parsers essentially go the\nother way.\n▷ In particular grammars in Greibach normal\nform where each terminal uniquely identifies\nits rule can be easily parsed.\n▷ A special case is prefix/polish notation.\n▷ Infix notation: ¬(0 ∨ (1 ∧ 0))\nF − →G ¬F | (F ∨ F) | (F ∧ F) | 0 | 1\nPrefix notation: ¬ ∨0 ∧ 10\nF − →G ¬F | ∨FF | ∧FF | 0 | 1\n▷ This leads to the formal computational\nmodel that corresponds to context-free\ngrammars: push-down automata (PDA):\nPDA “=” FA + call stack\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "6 Context-free languages\nContext-free grammars: basic properties\nPush-down automata\nDerivation trees\nPumping lemma and closure properties\nAlgebraic equations and EBNF (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Push-down automata 144 (396)\nD Push-down automaton (PDA) M = (Q, Σ, Γ, δ, q0, ⊥, F)\n▷ Q: finite set of states (memory content)\n▷ Σ: the finite input alphabet (user input), e.g. Σ = {0, 1}\n▷ Γ: finite stack alphabet (procedure names), e.g. Γ = {0, 1, ⊥}\n▷ ⊥: the bottom (of stack) symbol (“main procedure”) with ⊥ ∈Γ\n▷ q0: the initial state with q0 ∈ Q\n▷ F: the final states with F ⊆ Q\n▷ δ: the transition rules with δ ⊆ (Q × (Σ ∪ {ε}) × Γ) × (Q × Γ∗)\nA transition rule ((q, a, A), (r, γ)) ∈ δ stands for the instruction:\nif state == ’q’ and tape[pos] == ’a’ and stack[0] == ’A’:\nstate = ’r’, pos += 1 # as for FA: move head to the left\nstack = ’γ’ + stack[1:] # update call stack: pop then push γ\nAn ε-transition rule ((q, ε, A), (r, γ)) ∈ δ stands for the instruction:\nif state == ’q’ and stack[0] == ’A’:\nstate = ’r’ # as for FA: head does not move\nstack = ’γ’ + stack[1:] # update call stack: pop then push γ\nA PDA is deterministic (DPDA) if |(q, a, A)δ| + |(q, ε, A)δ| ≤1 for every",
      "An ε-transition rule ((q, ε, A), (r, γ)) ∈ δ stands for the instruction:\nif state == ’q’ and stack[0] == ’A’:\nstate = ’r’ # as for FA: head does not move\nstack = ’γ’ + stack[1:] # update call stack: pop then push γ\nA PDA is deterministic (DPDA) if |(q, a, A)δ| + |(q, ε, A)δ| ≤1 for every\n(q, a, A) ∈ Q × Σ × Γ, otherwise it is nondeterministic.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Remark: Push-down automata and recursive programs 145 (397)\n▷ A PDA can also be read as a (nondeterministic) recursive program, e.g.:\ndef A(state: Q) -> Q:\nx = input() # ∈ Σ ∪ {ε}\nif state == p1 and x == \"a1\": # ((p1, a1, A), (q1, ε))\nreturn q1\nif state == p2 and x == \"a2\": # ((p2, a2, A), (q2, B))\nreturn B(q2)\nif state == p3 and x == \"a3\": # ((p3, a3, A), (q3, BC))\nreturn C(B(q3))\n...\nraise Exception()\n▷ Programming languages internally handle recursive calls via a call stack:\n▷ The runtime environment thus implements essentially a (D)PDA to handle\nrecursion: In case of a recursive call a call frame is pushed on the (call) stack;\na call frame essentially states where to continue afterwards.\n▷ The states Q represent the potential memory content. The stack alphabet Γ\nconsists essentially of the potential call frames, in the simplest case just the\nprocedure names/entry addresses. (Γ × Q can be thought of as call frames.) The\ntransitions are the actual program code.",
      "▷ The states Q represent the potential memory content. The stack alphabet Γ\nconsists essentially of the potential call frames, in the simplest case just the\nprocedure names/entry addresses. (Γ × Q can be thought of as call frames.) The\ntransitions are the actual program code.\n▷ See also Stack machine and JVM.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Remark: Push-down automata and recursive programs 145 (398)\n▷ A PDA can also be read as a (nondeterministic) recursive program, e.g.:\ndef A(state: Q) -> Q:\nx = input() # ∈ Σ ∪ {ε}\nif state == p1 and x == \"a1\": # ((p1, a1, A), (q1, ε))\nreturn q1\nif state == p2 and x == \"a2\": # ((p2, a2, A), (q2, B))\nreturn B(q2)\nif state == p3 and x == \"a3\": # ((p3, a3, A), (q3, BC))\nreturn C(B(q3))\n...\nraise Exception()\n▷ An alternative computational model for context-free languages are thus\nrecursive state machines (RSM) (image source; cf. theorem 1) , i.e. finite\nautomata that can call each other recursively.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "PDA: configuration, run, accepted language 146 (399)\nD Let M = (Q, Σ, Γ, δ, q0, ⊥, F) be a PDA.\nA PDA is an FA extended with a stack (LIFO), i.e.\n▷ it must not write to the (input) tape,\n▷ it must read/consume its input completely from left to right,\n▷ it may continue from a final state,\n▷ it must pop and read the topmost symbol from the stack in every step,\nand may push an unbounded but finite number of new symboles on the stack.\nA configuration\nαqβ := (α, q, β) with q ∈ Q, β∈ Σ∗, α∈ Γ∗\nof an FA of the current state q, the remaining input β, and the current (call)\nstack content γ.\n▷ A PDA can also be considered a “ 1\n2 TM”: if we extend a PDA with a second\nstack, the two stacks can be used to simulate a 1TM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "PDA: configuration, run, accepted language 146 (400)\nD Let M = (Q, Σ, Γ, δ, q0, ⊥, F) be a PDA.\nThe rules δ generate the binary relation − →M on Γ∗ × Q × Σ∗:\n− →M := {(αAqaβ, αγRrεβ) | ((q, a\n∈Σ∪{ε}\n, A), (r, γ)) ∈ δ, β∈ Σ∗, α∈ Γ∗}\nA run of the FA is a path α0q0β0 − →M α1q1β1 − →M . . .wrt. − →M .\n▷ A run on input x1 . . . xl ∈ Σl is a path wrt. − →M starting at ⊥q0x1 . . . xl\n! A PDA always has to read the topmost symbol from the stack; if the stack is\nempty, the PDA “crashes”; for this reason, the stack is always initialized to ⊥:\nwhen reading ⊥, the PDA knows that it is at the bottom.\n! Infinite runs can only arise from ε-transitions as in the case of FAs.\n▷ δ is deterministic iff every configuration has at most one successor wrt. − →M\niff for every configuration there is exactly one maximal run.\n▷ Analogously to the transformation into Kuroda normal form, we can modify\nany PDA so that it pushes at most two symbols on the stack.",
      "▷ δ is deterministic iff every configuration has at most one successor wrt. − →M\niff for every configuration there is exactly one maximal run.\n▷ Analogously to the transformation into Kuroda normal form, we can modify\nany PDA so that it pushes at most two symbols on the stack.\n▷ A single dummy state suffices as we can store it on the stack instead.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "PDA: configuration, run, accepted language 146 (401)\nD Let M = (Q, Σ, Γ, δ, q0, ⊥, F) be a PDA.\nThe rules δ generate the binary relation − →M on Γ∗ × Q × Σ∗:\n− →M := {(αAqaβ, αγRrεβ) | ((q, a\n∈Σ∪{ε}\n, A), (r, γ)) ∈ δ, β∈ Σ∗, α∈ Γ∗}\nA run of the FA is a path α0q0β0 − →M α1q1β1 − →M . . .wrt. − →M .\nFor PDAs two difference “semantics” are commonly used: The language\naccepted wrt. final states (via “exit()” or “ raise Exception()”) is:\nLF (M) = {w ∈ Σ∗ | ⊥q0w − →∗\nG αqf ε for some qf ∈ F , α∈ Γ∗}\nwhile the language accepted wrt. empty stack (via “return”, no pending calls) is:\nLε(M) = {w ∈ Σ∗ | ⊥q0w − →∗\nG εqε for some q ∈ Q}\nAs in case of FAs, the input has to be completely processed.\nFor PDAs both can simulate each other, but for DPDAs only acceptance on\nfinal states can simulate acceptance on empty stack.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: PDA for a Dyck language 147 (402)\nq0start qf\na, A: AA; a,⊥: A⊥\nb, A: ε\nε,⊥: ε\n⊥q0abaabb\nq0\n. . .□ a b a a b b □ . . .\n⊥\n▷ Above PDA accepts\nS − →G ε | SS | aSb\nboth wrt. final state and empty\nstack.\n▷ It is not deterministic:\nFrom ⊥q0a we can transition to\n⊥Aq0ε or to εqf a.\n▷ PDAs can count in unary using the\nstack.\n▷ A second stack would allow to\nsimulate a 1TM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: PDA for a Dyck language 147 (403)\nq0start qf\na, A: AA; a,⊥: A⊥\nb, A: ε\nε,⊥: ε\n⊥Aq0baabb\nq0\n. . .□ a b a a b b □ . . .\nA\n⊥\n▷ Above PDA accepts\nS − →G ε | SS | aSb\nboth wrt. final state and empty\nstack.\n▷ It is not deterministic:\nFrom ⊥q0a we can transition to\n⊥Aq0ε or to εqf a.\n▷ PDAs can count in unary using the\nstack.\n▷ A second stack would allow to\nsimulate a 1TM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: PDA for a Dyck language 147 (404)\nq0start qf\na, A: AA; a,⊥: A⊥\nb, A: ε\nε,⊥: ε\n⊥q0aabb\nq0\n. . .□ a b a a b b □ . . .\n⊥\n▷ Above PDA accepts\nS − →G ε | SS | aSb\nboth wrt. final state and empty\nstack.\n▷ It is not deterministic:\nFrom ⊥q0a we can transition to\n⊥Aq0ε or to εqf a.\n▷ PDAs can count in unary using the\nstack.\n▷ A second stack would allow to\nsimulate a 1TM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: PDA for a Dyck language 147 (405)\nq0start qf\na, A: AA; a,⊥: A⊥\nb, A: ε\nε,⊥: ε\n⊥Aq0abb\nq0\n. . .□ a b a a b b □ . . .\nA\n⊥\n▷ Above PDA accepts\nS − →G ε | SS | aSb\nboth wrt. final state and empty\nstack.\n▷ It is not deterministic:\nFrom ⊥q0a we can transition to\n⊥Aq0ε or to εqf a.\n▷ PDAs can count in unary using the\nstack.\n▷ A second stack would allow to\nsimulate a 1TM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: PDA for a Dyck language 147 (406)\nq0start qf\na, A: AA; a,⊥: A⊥\nb, A: ε\nε,⊥: ε\n⊥AAq0bb\nq0\n. . .□ a b a a b b □ . . .\nA\nA\n⊥\n▷ Above PDA accepts\nS − →G ε | SS | aSb\nboth wrt. final state and empty\nstack.\n▷ It is not deterministic:\nFrom ⊥q0a we can transition to\n⊥Aq0ε or to εqf a.\n▷ PDAs can count in unary using the\nstack.\n▷ A second stack would allow to\nsimulate a 1TM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: PDA for a Dyck language 147 (407)\nq0start qf\na, A: AA; a,⊥: A⊥\nb, A: ε\nε,⊥: ε\n⊥Aq0b\nq0\n. . .□ a b a a b b □ . . .\nA\n⊥\n▷ Above PDA accepts\nS − →G ε | SS | aSb\nboth wrt. final state and empty\nstack.\n▷ It is not deterministic:\nFrom ⊥q0a we can transition to\n⊥Aq0ε or to εqf a.\n▷ PDAs can count in unary using the\nstack.\n▷ A second stack would allow to\nsimulate a 1TM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: PDA for a Dyck language 147 (408)\nq0start qf\na, A: AA; a,⊥: A⊥\nb, A: ε\nε,⊥: ε\n⊥q0\nq0\n. . .□ a b a a b b □ . . .\n⊥\n▷ Above PDA accepts\nS − →G ε | SS | aSb\nboth wrt. final state and empty\nstack.\n▷ It is not deterministic:\nFrom ⊥q0a we can transition to\n⊥Aq0ε or to εqf a.\n▷ PDAs can count in unary using the\nstack.\n▷ A second stack would allow to\nsimulate a 1TM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: PDA for a Dyck language 147 (409)\nq0start qf\na, A: AA; a,⊥: A⊥\nb, A: ε\nε,⊥: ε\nqf\nqf\n. . .□ a b a a b b □ . . .\nε\n▷ Above PDA accepts\nS − →G ε | SS | aSb\nboth wrt. final state and empty\nstack.\n▷ It is not deterministic:\nFrom ⊥q0a we can transition to\n⊥Aq0ε or to εqf a.\n▷ PDAs can count in unary using the\nstack.\n▷ A second stack would allow to\nsimulate a 1TM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for a Dyck language 148 (410)\nq0start q1\na,⊥: A⊥\na, A: AA; a, A⊥: AA⊥\nb, A: ε\nb, A⊥: ⊥\n⊥q0abaabb\nq0\n. . .□ a b a a b b □ . . .\n⊥\n▷ Above DPDA accepts\nS − →G ε | SS | aSb\nwrt. final states, but the empty\nlanguage wrt. empty stack.\n▷ Define a DPDA that accepts\nS − →G ab | aSSb | aSb\nwrt. both final states and empty\nstack.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for a Dyck language 148 (411)\nq0start q1\na,⊥: A⊥\na, A: AA; a, A⊥: AA⊥\nb, A: ε\nb, A⊥: ⊥\nA⊥q1baabb\nq1\n. . .□ a b a a b b □ . . .\nA⊥\n▷ Above DPDA accepts\nS − →G ε | SS | aSb\nwrt. final states, but the empty\nlanguage wrt. empty stack.\n▷ Define a DPDA that accepts\nS − →G ab | aSSb | aSb\nwrt. both final states and empty\nstack.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for a Dyck language 148 (412)\nq0start q1\na,⊥: A⊥\na, A: AA; a, A⊥: AA⊥\nb, A: ε\nb, A⊥: ⊥\n⊥q0aabb\nq0\n. . .□ a b a a b b □ . . .\n⊥\n▷ Above DPDA accepts\nS − →G ε | SS | aSb\nwrt. final states, but the empty\nlanguage wrt. empty stack.\n▷ Define a DPDA that accepts\nS − →G ab | aSSb | aSb\nwrt. both final states and empty\nstack.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for a Dyck language 148 (413)\nq0start q1\na,⊥: A⊥\na, A: AA; a, A⊥: AA⊥\nb, A: ε\nb, A⊥: ⊥\nA⊥q1abb\nq1\n. . .□ a b a a b b □ . . .\nA⊥\n▷ Above DPDA accepts\nS − →G ε | SS | aSb\nwrt. final states, but the empty\nlanguage wrt. empty stack.\n▷ Define a DPDA that accepts\nS − →G ab | aSSb | aSb\nwrt. both final states and empty\nstack.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for a Dyck language 148 (414)\nq0start q1\na,⊥: A⊥\na, A: AA; a, A⊥: AA⊥\nb, A: ε\nb, A⊥: ⊥\nA⊥Aq1bb\nq1\n. . .□ a b a a b b □ . . .\nA\nA⊥\n▷ Above DPDA accepts\nS − →G ε | SS | aSb\nwrt. final states, but the empty\nlanguage wrt. empty stack.\n▷ Define a DPDA that accepts\nS − →G ab | aSSb | aSb\nwrt. both final states and empty\nstack.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for a Dyck language 148 (415)\nq0start q1\na,⊥: A⊥\na, A: AA; a, A⊥: AA⊥\nb, A: ε\nb, A⊥: ⊥\nA⊥q1b\nq1\n. . .□ a b a a b b □ . . .\nA⊥\n▷ Above DPDA accepts\nS − →G ε | SS | aSb\nwrt. final states, but the empty\nlanguage wrt. empty stack.\n▷ Define a DPDA that accepts\nS − →G ab | aSSb | aSb\nwrt. both final states and empty\nstack.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for a Dyck language 148 (416)\nq0start q1\na,⊥: A⊥\na, A: AA; a, A⊥: AA⊥\nb, A: ε\nb, A⊥: ⊥\n⊥q0\nq0\n. . .□ a b a a b b □ . . .\n⊥\n▷ Above DPDA accepts\nS − →G ε | SS | aSb\nwrt. final states, but the empty\nlanguage wrt. empty stack.\n▷ Define a DPDA that accepts\nS − →G ab | aSSb | aSb\nwrt. both final states and empty\nstack.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for binary-unary trees in prefix notation 149 (417)\nq0start\na, S: SS\nb, S: S\nc, S: ε\nSq0aabccc\nq0\n. . .□ a a b c c c □ . . .\nS\n▷ Above DPDA accepts\nS − →G aSS | bS | c\nwrt. empty stack.\n▷ The grammar encodes binary-unary\ntrees/terms in polish/prefix\nnotation, e.g. abccˆ =a(b(c), c).\n▷ Such languages are prefix-free, i.e. if\nu ∈ L, then uv ̸∈ L for all v ∈ Σ+.\n! Show: wrt. empty stack, an DPDA\ncan only accept prefix-free\nlanguages.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for binary-unary trees in prefix notation 149 (418)\nq0start\na, S: SS\nb, S: S\nc, S: ε\nSSq0abccc\nq0\n. . .□ a a b c c c □ . . .\nS\nS\n▷ Above DPDA accepts\nS − →G aSS | bS | c\nwrt. empty stack.\n▷ The grammar encodes binary-unary\ntrees/terms in polish/prefix\nnotation, e.g. abccˆ =a(b(c), c).\n▷ Such languages are prefix-free, i.e. if\nu ∈ L, then uv ̸∈ L for all v ∈ Σ+.\n! Show: wrt. empty stack, an DPDA\ncan only accept prefix-free\nlanguages.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for binary-unary trees in prefix notation 149 (419)\nq0start\na, S: SS\nb, S: S\nc, S: ε\nSSSSq 0bccc\nq0\n. . .□ a a b c c c □ . . .\nS\nS\nS\n▷ Above DPDA accepts\nS − →G aSS | bS | c\nwrt. empty stack.\n▷ The grammar encodes binary-unary\ntrees/terms in polish/prefix\nnotation, e.g. abccˆ =a(b(c), c).\n▷ Such languages are prefix-free, i.e. if\nu ∈ L, then uv ̸∈ L for all v ∈ Σ+.\n! Show: wrt. empty stack, an DPDA\ncan only accept prefix-free\nlanguages.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for binary-unary trees in prefix notation 149 (420)\nq0start\na, S: SS\nb, S: S\nc, S: ε\nSSSq 0ccc\nq0\n. . .□ a a b c c c □ . . .\nS\nS\nS\n▷ Above DPDA accepts\nS − →G aSS | bS | c\nwrt. empty stack.\n▷ The grammar encodes binary-unary\ntrees/terms in polish/prefix\nnotation, e.g. abccˆ =a(b(c), c).\n▷ Such languages are prefix-free, i.e. if\nu ∈ L, then uv ̸∈ L for all v ∈ Σ+.\n! Show: wrt. empty stack, an DPDA\ncan only accept prefix-free\nlanguages.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for binary-unary trees in prefix notation 149 (421)\nq0start\na, S: SS\nb, S: S\nc, S: ε\nSSq0cc\nq0\n. . .□ a a b c c c □ . . .\nS\nS\n▷ Above DPDA accepts\nS − →G aSS | bS | c\nwrt. empty stack.\n▷ The grammar encodes binary-unary\ntrees/terms in polish/prefix\nnotation, e.g. abccˆ =a(b(c), c).\n▷ Such languages are prefix-free, i.e. if\nu ∈ L, then uv ̸∈ L for all v ∈ Σ+.\n! Show: wrt. empty stack, an DPDA\ncan only accept prefix-free\nlanguages.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for binary-unary trees in prefix notation 149 (422)\nq0start\na, S: SS\nb, S: S\nc, S: ε\nSq0c\nq0\n. . .□ a a b c c c □ . . .\nS\n▷ Above DPDA accepts\nS − →G aSS | bS | c\nwrt. empty stack.\n▷ The grammar encodes binary-unary\ntrees/terms in polish/prefix\nnotation, e.g. abccˆ =a(b(c), c).\n▷ Such languages are prefix-free, i.e. if\nu ∈ L, then uv ̸∈ L for all v ∈ Σ+.\n! Show: wrt. empty stack, an DPDA\ncan only accept prefix-free\nlanguages.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: DPDA for binary-unary trees in prefix notation 149 (423)\nq0start\na, S: SS\nb, S: S\nc, S: ε\nq0\nq0\n. . .□ a a b c c c □ . . .\nε\n▷ Above DPDA accepts\nS − →G aSS | bS | c\nwrt. empty stack.\n▷ The grammar encodes binary-unary\ntrees/terms in polish/prefix\nnotation, e.g. abccˆ =a(b(c), c).\n▷ Such languages are prefix-free, i.e. if\nu ∈ L, then uv ̸∈ L for all v ∈ Σ+.\n! Show: wrt. empty stack, an DPDA\ncan only accept prefix-free\nlanguages.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "PDA: final states vs. empty stack 150 (424)\nL From every PDA M = (Q, Σ, Γ, δ, q0, ⊥, F) we can construct M′ and M′′ s.t.\nLε(M) = LF (M′) LF (M) = Lε(M′′)\n(In general, LF (M) ̸= Lε(M) for a specific PDA!)\n▷ Lε(M) = LF (M′): Let ⊥′ ̸∈ Γ be an unused “below-bottom symbol”.\nM′ starts with (q′\n0w, ⊥′) − →M (q0w, ⊥⊥′) and then calls M on w until only\n⊥′ remains (M returns).\nAs M does not know ⊥′, we can let M′ take over and transition into a unique\nfinal state whenever M has emptied its stack (i.e. “M(w); exit(0);”).\n▷ LF (M) = Lε(M′′):\nAgain, M′′ starts with (q′\n0w, ⊥′) − →M (q0w, ⊥⊥′) and then calls M on w.\nNow add ε-transitions (leading to nondeterminism) that allow to empty the stack\nwhenever a final state is reached (potentially terminating the run prematurely) .\n(This is somewhat similar to what the runtime does in case of an exception.)\nAs before M cannot process ⊥′ itself; so only M′ can erase it while using its\nrules to empty the stack after M has reached a final state.",
      "whenever a final state is reached (potentially terminating the run prematurely) .\n(This is somewhat similar to what the runtime does in case of an exception.)\nAs before M cannot process ⊥′ itself; so only M′ can erase it while using its\nrules to empty the stack after M has reached a final state.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "PDA: final states vs. empty stack 150 (425)\nL From every PDA M = (Q, Σ, Γ, δ, q0, ⊥, F) we can construct M′ and M′′ s.t.\nLε(M) = LF (M′) LF (M) = Lε(M′′)\n(In general, LF (M) ̸= Lε(M) for a specific PDA!)\n! For DPDAs acceptance wrt. final state resp. on empty stack do not define\nthe same class of languages!\n▷ We still can construct for every DPDA M an DPDA M′ s.t.\nLε(M) = LF (M′) as M′ can deterministically proceed from ⊥′.\n▷ But the other direction is not true anymore: a DPDA can accept on empty\nstack only the strict subset of prefix-free languages accepted wrt. final state.\n“prefix-free” means that if w ∈ L, then ww′ ̸∈ L for any w′ ̸= ε.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From CFG to PDA (accept on empty stack) 151(426)\nS − →G aSS S − →G bS S − →G c\nq0start\nε, S: aSS\nε, S: bS\nε, S: c\na, a: ε; b, b: ε; c, c: ε\nL For every context-free G there is a PDA M with Lε(M) = L(G):\nThe PDA guesses nondeterministically a leftmost(=topmost) derivation on its stack.\nA rewrite rule A − →G γ gives rise to an ε-transition (guess) ((q0, ε, A), (q0, γ)).\nFor every a ∈ Σ add the transition ((q0, a, a), (q0, ε)) to check the current input.\nq0\n. . .□ a b c c □ . . .\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From CFG to PDA (accept on empty stack) 151(427)\nS − →G aSS S − →G bS S − →G c\nq0start\nε, S: aSS\nε, S: bS\nε, S: c\na, a: ε; b, b: ε; c, c: ε\nL For every context-free G there is a PDA M with Lε(M) = L(G):\nThe PDA guesses nondeterministically a leftmost(=topmost) derivation on its stack.\nA rewrite rule A − →G γ gives rise to an ε-transition (guess) ((q0, ε, A), (q0, γ)).\nFor every a ∈ Σ add the transition ((q0, a, a), (q0, ε)) to check the current input.\nq0\n. . .□ a b c c □ . . .\na\nS\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From CFG to PDA (accept on empty stack) 151(428)\nS − →G aSS S − →G bS S − →G c\nq0start\nε, S: aSS\nε, S: bS\nε, S: c\na, a: ε; b, b: ε; c, c: ε\nL For every context-free G there is a PDA M with Lε(M) = L(G):\nThe PDA guesses nondeterministically a leftmost(=topmost) derivation on its stack.\nA rewrite rule A − →G γ gives rise to an ε-transition (guess) ((q0, ε, A), (q0, γ)).\nFor every a ∈ Σ add the transition ((q0, a, a), (q0, ε)) to check the current input.\nq0\n. . .□ a b c c □ . . .\nS\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From CFG to PDA (accept on empty stack) 151(429)\nS − →G aSS S − →G bS S − →G c\nq0start\nε, S: aSS\nε, S: bS\nε, S: c\na, a: ε; b, b: ε; c, c: ε\nL For every context-free G there is a PDA M with Lε(M) = L(G):\nThe PDA guesses nondeterministically a leftmost(=topmost) derivation on its stack.\nA rewrite rule A − →G γ gives rise to an ε-transition (guess) ((q0, ε, A), (q0, γ)).\nFor every a ∈ Σ add the transition ((q0, a, a), (q0, ε)) to check the current input.\nq0\n. . .□ a b c c □ . . .\nb\nS\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From CFG to PDA (accept on empty stack) 151(430)\nS − →G aSS S − →G bS S − →G c\nq0start\nε, S: aSS\nε, S: bS\nε, S: c\na, a: ε; b, b: ε; c, c: ε\nL For every context-free G there is a PDA M with Lε(M) = L(G):\nThe PDA guesses nondeterministically a leftmost(=topmost) derivation on its stack.\nA rewrite rule A − →G γ gives rise to an ε-transition (guess) ((q0, ε, A), (q0, γ)).\nFor every a ∈ Σ add the transition ((q0, a, a), (q0, ε)) to check the current input.\nq0\n. . .□ a b c c □ . . .\nS\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From CFG to PDA (accept on empty stack) 151(431)\nS − →G aSS S − →G bS S − →G c\nq0start\nε, S: aSS\nε, S: bS\nε, S: c\na, a: ε; b, b: ε; c, c: ε\nL For every context-free G there is a PDA M with Lε(M) = L(G):\nThe PDA guesses nondeterministically a leftmost(=topmost) derivation on its stack.\nA rewrite rule A − →G γ gives rise to an ε-transition (guess) ((q0, ε, A), (q0, γ)).\nFor every a ∈ Σ add the transition ((q0, a, a), (q0, ε)) to check the current input.\nq0\n. . .□ a b c c □ . . .\nc\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From CFG to PDA (accept on empty stack) 151(432)\nS − →G aSS S − →G bS S − →G c\nq0start\nε, S: aSS\nε, S: bS\nε, S: c\na, a: ε; b, b: ε; c, c: ε\nL For every context-free G there is a PDA M with Lε(M) = L(G):\nThe PDA guesses nondeterministically a leftmost(=topmost) derivation on its stack.\nA rewrite rule A − →G γ gives rise to an ε-transition (guess) ((q0, ε, A), (q0, γ)).\nFor every a ∈ Σ add the transition ((q0, a, a), (q0, ε)) to check the current input.\nq0\n. . .□ a b c c □ . . .\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From CFG to PDA (accept on empty stack) 151(433)\nS − →G aSS S − →G bS S − →G c\nq0start\nε, S: aSS\nε, S: bS\nε, S: c\na, a: ε; b, b: ε; c, c: ε\nL For every context-free G there is a PDA M with Lε(M) = L(G):\nThe PDA guesses nondeterministically a leftmost(=topmost) derivation on its stack.\nA rewrite rule A − →G γ gives rise to an ε-transition (guess) ((q0, ε, A), (q0, γ)).\nFor every a ∈ Σ add the transition ((q0, a, a), (q0, ε)) to check the current input.\nq0\n. . .□ a b c c □ . . .\nc\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From CFG to PDA (accept on empty stack) 151(434)\nS − →G aSS S − →G bS S − →G c\nq0start\nε, S: aSS\nε, S: bS\nε, S: c\na, a: ε; b, b: ε; c, c: ε\nL For every context-free G there is a PDA M with Lε(M) = L(G):\nThe PDA guesses nondeterministically a leftmost(=topmost) derivation on its stack.\nA rewrite rule A − →G γ gives rise to an ε-transition (guess) ((q0, ε, A), (q0, γ)).\nFor every a ∈ Σ add the transition ((q0, a, a), (q0, ε)) to check the current input.\nq0\n. . .□ a b c c □ . . .\nε\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From PDA (accept on empty stack) to CFG 152 (435)\n0start 1 2\na,⊥: X⊥\na, X: XX\nb, X: ε\nb, X: ε\nε,⊥: ε\nS − →G ⊥0,r\n⊥0,r − →G aX0,r′⊥r′,r\n⊥1,2 − →G ε\nX0,r − →G aX0,r′Xr′,r\nX0,1 − →G b\nX1,1 − →G b\n(r′, r∈ {0, 1, 2})\ndef bot(q): # ⊥\nx = input()\nif q == 0 and x == \"a\":\nreturn bot(X(0))\nif q == 1 and x == \"\":\nreturn 2\nraise Exception()\ndef X(q):\nx = input()\nif q == 0 and x == \"a\":\nreturn X(X(0))\nif q == 0 and x == \"b\":\nreturn 1\nif q == 1 and x == \"b\":\nreturn 1\nraise Exception()\nL For every PDA M there is a context-free G with L(G) = Lε(M):\nThe states Q describe the memory content/state. A transition ((q, a, A), (q′, γ)) ∈ δ is a\ncall of the procedure A that updates the memory from state q to state q′ and leads to the\nsubsequent recursive calls γ.\nThe variable Ap,r produces all inputs that make the procedure A return the memory state\nr when originally called in memory state p.",
      "call of the procedure A that updates the memory from state q to state q′ and leads to the\nsubsequent recursive calls γ.\nThe variable Ap,r produces all inputs that make the procedure A return the memory state\nr when originally called in memory state p.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From PDA (accept on empty stack) to CFG 152 (436)\n0start 1 2\na,⊥: X⊥\na, X: XX\nb, X: ε\nb, X: ε\nε,⊥: ε\nS − →G ⊥0,r\n⊥0,r − →G aX0,r′⊥r′,r\n⊥1,2 − →G ε\nX0,r − →G aX0,r′Xr′,r\nX0,1 − →G b\nX1,1 − →G b\n(r′, r∈ {0, 1, 2})\ndef bot(q): # ⊥\nx = input()\nif q == 0 and x == \"a\":\nreturn bot(X(0))\nif q == 1 and x == \"\":\nreturn 2\nraise Exception()\ndef X(q):\nx = input()\nif q == 0 and x == \"a\":\nreturn X(X(0))\nif q == 0 and x == \"b\":\nreturn 1\nif q == 1 and x == \"b\":\nreturn 1\nraise Exception()\n▷ Simplify the grammar by removing non-productive and unreachable variables:\nAs only ⊥1,2, X0,1, X1,1 are directly productive, by backward propagation/resolution\nonly ⊥0,1 − →G aX0,1⊥1,2 and X0,1 − →G aX0,1X1,1 can terminate.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From PDA (accept on empty stack) to CFG 152 (437)\n0start 1 2\na,⊥: X⊥\na, X: XX\nb, X: ε\nb, X: ε\nε,⊥: ε\nS − →G ⊥0,2\n⊥1,2 − →G ε\nX0,1 − →G b\nX1,1 − →G b\n⊥0,2 − →G aX0,1⊥1,2\nX0,1 − →G aX0,1X1,1\ndef bot(q): # ⊥\nx = input()\nif q == 0 and x == \"a\":\nreturn bot(X(0))\nif q == 1 and x == \"\":\nreturn 2\nraise Exception()\ndef X(q):\nx = input()\nif q == 0 and x == \"a\":\nreturn X(X(0))\nif q == 0 and x == \"b\":\nreturn 1\nif q == 1 and x == \"b\":\nreturn 1\nraise Exception()\n▷ Simplify the grammar by removing non-productive and unreachable variables:\nAs only ⊥1,2, X0,1, X1,1 are directly productive, by backward propagation/resolution\nonly ⊥0,1 − →G aX0,1⊥1,2 and X0,1 − →G aX0,1X1,1 can terminate.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From PDA (accept on empty stack) to CFG 152 (438)\n0start 1 2\na,⊥: X⊥\na, X: XX\nb, X: ε\nb, X: ε\nε,⊥: ε\nS − →G ⊥0,2\n⊥1,2 − →G ε\nX0,1 − →G b\nX1,1 − →G b\n⊥0,2 − →G aX0,1⊥1,2\nX0,1 − →G aX0,1X1,1\ndef bot(q): # ⊥\nx = input()\nif q == 0 and x == \"a\":\nreturn bot(X(0))\nif q == 1 and x == \"\":\nreturn 2\nraise Exception()\ndef X(q):\nx = input()\nif q == 0 and x == \"a\":\nreturn X(X(0))\nif q == 0 and x == \"b\":\nreturn 1\nif q == 1 and x == \"b\":\nreturn 1\nraise Exception()\nS − →G ⊥0,2 − →G aX0,1⊥1,2\n− →G aaX0,1X1,1⊥1,2\n− →G aabX1,1⊥1,2 − →G aabb⊥1,2 − →G aabb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-free languages and PDAs 153 (439)\nT The languages accepted by (nondeterministic) PDAs (on empty stack) are\nexactly the context-free languages.\n▷ From grammar G = (V, Σ, P, S) to PDA M = (Q, Σ, Γ, δ, q0, ⊥, F):\nLet the PDA guess a leftmost derivation on the stack using ε-transitions, i.e.\n▷ Use S as bottom symbol (“ main()”).\n▷ For every production rule (A, γ) ∈ P add the transition ((q0, ε, A), (q0, γ)).\n(I.e. nondeterministically rewrite the leftmoste/topmost nonterminal.)\n▷ For every terminal a ∈ Σ add the transition ((q0, a, a), (q0, ε)).\n(I.e. check the current input symbol against the leftmost/topmost terminal.)\nThen L(G) = Lε(M).\n(The formal proof requires to show both L(G) ⊆ Lε(M) and Lε(M) ⊆ L(G) using\ninduction on the length of a derivation.)\n(Note that this requires only a single state: this is analogous to including the\ncomplete memory content into the stack frame.)",
      "Then L(G) = Lε(M).\n(The formal proof requires to show both L(G) ⊆ Lε(M) and Lε(M) ⊆ L(G) using\ninduction on the length of a derivation.)\n(Note that this requires only a single state: this is analogous to including the\ncomplete memory content into the stack frame.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-free languages and PDAs 153 (440)\nT The languages accepted by (nondeterministic) PDAs (on empty stack) are\nexactly the context-free languages.\n▷ From PDA M = (Q, Σ, Γ, δ, q0, ⊥, F) to grammar G = (V, Σ, P, S):\nwlog we assume that the PDA accepts on empty stack.\nWe thus need to define G s.t. L(G) = Lε(M).\nConsider an accepting run\n⊥q0w − →∗\nM γ′Apw′ − →∗\nM εq′ε\nAs the stack eventually has to become empty, there has to be some\nfactorization w′ = uw′′ and a state r ∈ Q s.t.\nApu − →∗\nM εrε ⊥q0w − →∗\nM γ′Apw′ − →∗\nM γ′rw′′ − →∗\nM εq′ε\n(The intuition is to read a rule ((p, a, A), (p′, BC)) as a call of the procedure A with\np the global memory state and a the current input which leads to the updated global\nmemory state p′ followed by recursive calls of first B and then C. The partial run\nApu − →∗\nM εrε thus means that a call of A in state p completely consumes the input u\nand returns the state r finally.)",
      "p the global memory state and a the current input which leads to the updated global\nmemory state p′ followed by recursive calls of first B and then C. The partial run\nApu − →∗\nM εrε thus means that a call of A in state p completely consumes the input u\nand returns the state r finally.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-free languages and PDAs 153 (441)\nT The languages accepted by (nondeterministic) PDAs (on empty stack) are\nexactly the context-free languages.\n▷ From PDA M = (Q, Σ, Γ, δ, q0, ⊥, F) to grammar G = (V, Σ, P, S):\nWe therefore let the nonterminal Ap,r produce exactly the language\n{u ∈ Σ∗ | Apu − →∗\nM r}\nTo this end, for each transition ((p, a, A), (p′, γ)) ∈ δ with a ∈ {ε} ∪Σ:\n▷ if γ = ε, add Ap,p′ − →G a\n▷ if γ = B, add Ap,r − →G aBp′,r for all r ∈ Q\n▷ if γ = BC, add Ap,r − →G aBp′,r′Cr′,r for all r′, r∈ Q.\n(Reduce |γ| > 2 to a sequence of calls as for grammars:\n[BCD]p,r − →G Bp,q[CD]q,r.)\ni.e. the rules guess the intermediate states M enters while emptying its stack\n(as in the composition of binary relations) .\nFinally, add S − →G ⊥q0,q for all q ∈ Q (i.e. guess the terminal state q). Then\nL(G) = Lε(M). (Again, the formal proof requires induction.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-free languages and PDAs 153 (442)\nT The languages accepted by (nondeterministic) PDAs (on empty stack) are\nexactly the context-free languages.\nD A context-free grammar G = (V, Σ, P, S) is simple if P ⊆ V × ΣV ∗ (Greibach\nnormal form) and for every pair (X, a) ∈ V × Σ there is at most one applicable\nrule (unambiguous).\n▷ E.g. every DFA yields a simple grammar;\n▷ E.g. F − →G ¬F | ∨F F| ∧F F| p | q is simple.\n▷ A simple grammar can obviously be accepted by a DPDA:\nThe production (A, aγ) becomes the transition ((q0, a, A), (q0, γ)).\n▷ As we will see, not all context-free languages can be accepted by an DPDA.\n(In contrast to FAs and 1TMs; recall, for LBAs we do not know so far.)\nThe subset of languages accepted by DPDAs is called deterministic\ncontext-free languages and it is a strict subset of the context-free languages.\n▷ In general, recursive descent therefore needs to use backtracking in order to do\na search/enumeration over all possible derivation trees, while deterministic",
      "context-free languages and it is a strict subset of the context-free languages.\n▷ In general, recursive descent therefore needs to use backtracking in order to do\na search/enumeration over all possible derivation trees, while deterministic\ncontext-free languages can be parsed in linear time (cf. CYK, LR parser).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From simple context-free grammar to DPDA 154 (443)\nS − →G aSS S − →G bS S − →G c\nq0start\na, S: SS\nb, S: S\nc, S: ε\n▷ In case of a simple grammar, every terminal already determines at most one applicable rule,\nso there is no need to guess.\nq0\n. . .□ a b c c □ . . .\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From simple context-free grammar to DPDA 154 (444)\nS − →G aSS S − →G bS S − →G c\nq0start\na, S: SS\nb, S: S\nc, S: ε\n▷ In case of a simple grammar, every terminal already determines at most one applicable rule,\nso there is no need to guess.\nq0\n. . .□ a b c c □ . . .\nS\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From simple context-free grammar to DPDA 154 (445)\nS − →G aSS S − →G bS S − →G c\nq0start\na, S: SS\nb, S: S\nc, S: ε\n▷ In case of a simple grammar, every terminal already determines at most one applicable rule,\nso there is no need to guess.\nq0\n. . .□ a b c c □ . . .\nS\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From simple context-free grammar to DPDA 154 (446)\nS − →G aSS S − →G bS S − →G c\nq0start\na, S: SS\nb, S: S\nc, S: ε\n▷ In case of a simple grammar, every terminal already determines at most one applicable rule,\nso there is no need to guess.\nq0\n. . .□ a b c c □ . . .\nS\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: From simple context-free grammar to DPDA 154 (447)\nS − →G aSS S − →G bS S − →G c\nq0start\na, S: SS\nb, S: S\nc, S: ε\n▷ In case of a simple grammar, every terminal already determines at most one applicable rule,\nso there is no need to guess.\nq0\n. . .□ a b c c □ . . .\nε\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "6 Context-free languages\nContext-free grammars: basic properties\nPush-down automata\nDerivation trees\nPumping lemma and closure properties\nAlgebraic equations and EBNF (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (449)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, ε) (S, aSb)\n(S, ε)\n1.\n2.\n3.\n4.\n5.\n6.\nS − →G SS\n− →G SaSb − →G aSbaSb − →G aSbaaSbb − →G aSbaabb − →G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, Σ, P, S) is an\nordered tree whose nodes are labeled by rules (X, γ) ∈ P s.t.\nif γ = w0X1w1 . . . Xrwr with w0, w1, . . . , wr ∈ Σ∗ and X1, . . . , Xr ∈ V ,\nthen the i-th child from the left is labeled by a rule (Xi, γi) ∈ P.\nF Every derivation X − →∗\nG w ∈ Σ∗ defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, γ) defines a unique\nleftmost derivation X − →∗\nG w ∈ Σ∗ via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (450)\n(S, SS)\n(S, aSb)\n(S, aSb)\n(S, ε) (S, aSb)\n(S, ε)\n1.\n2.\n3.\n4.\n5.\n6.\nS − →G SS − →G SaSb\n− →G aSbaSb − →G aSbaaSbb − →G aSbaabb − →G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, Σ, P, S) is an\nordered tree whose nodes are labeled by rules (X, γ) ∈ P s.t.\nif γ = w0X1w1 . . . Xrwr with w0, w1, . . . , wr ∈ Σ∗ and X1, . . . , Xr ∈ V ,\nthen the i-th child from the left is labeled by a rule (Xi, γi) ∈ P.\nF Every derivation X − →∗\nG w ∈ Σ∗ defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, γ) defines a unique\nleftmost derivation X − →∗\nG w ∈ Σ∗ via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (451)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, ε) (S, aSb)\n(S, ε)\n1.\n2.\n3.\n4.\n5.\n6.\nS − →G SS − →G SaSb − →G aSbaSb\n− →G aSbaaSbb − →G aSbaabb − →G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, Σ, P, S) is an\nordered tree whose nodes are labeled by rules (X, γ) ∈ P s.t.\nif γ = w0X1w1 . . . Xrwr with w0, w1, . . . , wr ∈ Σ∗ and X1, . . . , Xr ∈ V ,\nthen the i-th child from the left is labeled by a rule (Xi, γi) ∈ P.\nF Every derivation X − →∗\nG w ∈ Σ∗ defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, γ) defines a unique\nleftmost derivation X − →∗\nG w ∈ Σ∗ via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (452)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, ε)\n(S, aSb)\n(S, ε)\n1.\n2.\n3.\n4.\n5.\n6.\nS − →G SS − →G SaSb − →G aSbaSb − →G aSbaaSbb\n− →G aSbaabb − →G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, Σ, P, S) is an\nordered tree whose nodes are labeled by rules (X, γ) ∈ P s.t.\nif γ = w0X1w1 . . . Xrwr with w0, w1, . . . , wr ∈ Σ∗ and X1, . . . , Xr ∈ V ,\nthen the i-th child from the left is labeled by a rule (Xi, γi) ∈ P.\nF Every derivation X − →∗\nG w ∈ Σ∗ defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, γ) defines a unique\nleftmost derivation X − →∗\nG w ∈ Σ∗ via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (453)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, ε)\n(S, aSb)\n(S, ε)\n1.\n2.\n3.\n4.\n5.\n6.\nS − →G SS − →G SaSb − →G aSbaSb − →G aSbaaSbb − →G aSbaabb\n− →G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, Σ, P, S) is an\nordered tree whose nodes are labeled by rules (X, γ) ∈ P s.t.\nif γ = w0X1w1 . . . Xrwr with w0, w1, . . . , wr ∈ Σ∗ and X1, . . . , Xr ∈ V ,\nthen the i-th child from the left is labeled by a rule (Xi, γi) ∈ P.\nF Every derivation X − →∗\nG w ∈ Σ∗ defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, γ) defines a unique\nleftmost derivation X − →∗\nG w ∈ Σ∗ via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (454)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, ε) (S, aSb)\n(S, ε)\n1.\n2.\n3.\n4.\n5.\n6.\nS − →G SS − →G SaSb − →G aSbaSb − →G aSbaaSbb − →G aSbaabb − →G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, Σ, P, S) is an\nordered tree whose nodes are labeled by rules (X, γ) ∈ P s.t.\nif γ = w0X1w1 . . . Xrwr with w0, w1, . . . , wr ∈ Σ∗ and X1, . . . , Xr ∈ V ,\nthen the i-th child from the left is labeled by a rule (Xi, γi) ∈ P.\nF Every derivation X − →∗\nG w ∈ Σ∗ defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, γ) defines a unique\nleftmost derivation X − →∗\nG w ∈ Σ∗ via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (455)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, ε) (S, aSb)\n(S, ε)\n1.\n2.\n3.\n4.\n5.\n6.\nS − →G SS\n− →G aSbS − →G abS − →G abaSb − →G abaaSbb − →G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, Σ, P, S) is an\nordered tree whose nodes are labeled by rules (X, γ) ∈ P s.t.\nif γ = w0X1w1 . . . Xrwr with w0, w1, . . . , wr ∈ Σ∗ and X1, . . . , Xr ∈ V ,\nthen the i-th child from the left is labeled by a rule (Xi, γi) ∈ P.\nF Every derivation X − →∗\nG w ∈ Σ∗ defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, γ) defines a unique\nleftmost derivation X − →∗\nG w ∈ Σ∗ via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (456)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, ε) (S, aSb)\n(S, ε)\n1.\n2.\n3.\n4.\n5.\n6.\nS − →G SS − →G aSbS\n− →G abS − →G abaSb − →G abaaSbb − →G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, Σ, P, S) is an\nordered tree whose nodes are labeled by rules (X, γ) ∈ P s.t.\nif γ = w0X1w1 . . . Xrwr with w0, w1, . . . , wr ∈ Σ∗ and X1, . . . , Xr ∈ V ,\nthen the i-th child from the left is labeled by a rule (Xi, γi) ∈ P.\nF Every derivation X − →∗\nG w ∈ Σ∗ defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, γ) defines a unique\nleftmost derivation X − →∗\nG w ∈ Σ∗ via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (457)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, ε) (S, aSb)\n(S, ε)\n1.\n2.\n3.\n4.\n5.\n6.\nS − →G SS − →G aSbS − →G abS\n− →G abaSb − →G abaaSbb − →G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, Σ, P, S) is an\nordered tree whose nodes are labeled by rules (X, γ) ∈ P s.t.\nif γ = w0X1w1 . . . Xrwr with w0, w1, . . . , wr ∈ Σ∗ and X1, . . . , Xr ∈ V ,\nthen the i-th child from the left is labeled by a rule (Xi, γi) ∈ P.\nF Every derivation X − →∗\nG w ∈ Σ∗ defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, γ) defines a unique\nleftmost derivation X − →∗\nG w ∈ Σ∗ via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (458)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, ε) (S, aSb)\n(S, ε)\n1.\n2.\n3.\n4.\n5.\n6.\nS − →G SS − →G aSbS − →G abS − →G abaSb\n− →G abaaSbb − →G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, Σ, P, S) is an\nordered tree whose nodes are labeled by rules (X, γ) ∈ P s.t.\nif γ = w0X1w1 . . . Xrwr with w0, w1, . . . , wr ∈ Σ∗ and X1, . . . , Xr ∈ V ,\nthen the i-th child from the left is labeled by a rule (Xi, γi) ∈ P.\nF Every derivation X − →∗\nG w ∈ Σ∗ defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, γ) defines a unique\nleftmost derivation X − →∗\nG w ∈ Σ∗ via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (459)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, ε) (S, aSb)\n(S, ε)\n1.\n2.\n3.\n4.\n5.\n6.\nS − →G SS − →G aSbS − →G abS − →G abaSb − →G abaaSbb\n− →G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, Σ, P, S) is an\nordered tree whose nodes are labeled by rules (X, γ) ∈ P s.t.\nif γ = w0X1w1 . . . Xrwr with w0, w1, . . . , wr ∈ Σ∗ and X1, . . . , Xr ∈ V ,\nthen the i-th child from the left is labeled by a rule (Xi, γi) ∈ P.\nF Every derivation X − →∗\nG w ∈ Σ∗ defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, γ) defines a unique\nleftmost derivation X − →∗\nG w ∈ Σ∗ via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended derivation trees 156 (460)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, ε) (S, aSb)\n(S, ε)\n1.\n2.\n3.\n4.\n5.\n6.\nS − →G SS − →G aSbS − →G abS − →G abaSb − →G abaaSbb − →G abaabb\nD An extended derivation wrt. a context-free grammar G = (V, Σ, P, S) is an\nordered tree whose nodes are labeled by rules (X, γ) ∈ P s.t.\nif γ = w0X1w1 . . . Xrwr with w0, w1, . . . , wr ∈ Σ∗ and X1, . . . , Xr ∈ V ,\nthen the i-th child from the left is labeled by a rule (Xi, γi) ∈ P.\nF Every derivation X − →∗\nG w ∈ Σ∗ defines a unique derivation tree.\nEvery derivation tree whose root is labeled by (X, γ) defines a unique\nleftmost derivation X − →∗\nG w ∈ Σ∗ via pre-order traversal.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "(Simple) derivation/parse trees 157 (461)\n(S, SS)\n(S, aSb) (S, aSb)\n(S, ε) (S, aSb)\n(S, ε)\nS\nS S\nSa b\nε\nSa b\nSa b\nε\nD A (simple) derivation/parse/syntax tree is an ordered labeled tree s.t.:\n▷ Every leaf is labeled by a nonterminal or the empty word.\n▷ Every inner node is labeled by a variable X and\nif its children are labeled by γ0, γ1, . . . , γr ∈ {ε} ∪Σ ∪ V from left to right,\nthen (X, γ0γ1, . . . , γr) ∈ P; and if γi = ε, then r = 0 and (X, ε) ∈ P.\nF Extended and simple derivation trees are in bijection.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: propositional formulas 158 (462)\nS − →G [S ∧ S] | [S ∨ S] | ¬S | p | q\n(S, [S ∧ S])\n(S, ¬S)\n(S, p)\n(S, [S ∨ S])\n(S, q) (S, p)\n∧\n¬\np\n∨\nq p\nS\n[ ∧ ]S\n¬ S\np\nS\n[ ∨ ]S\nq\nS\np\n▷ Most textbooks use simple derivation trees and also call them syntax trees.\n▷ In case of formulas/expressions/terms/programs extended derivation trees are\ncloser to classic syntax trees than simple derivation trees.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Chomsky normal form and derivation trees 159 (463)\nS − →G SS | AB | AT T − →G SB A − →G a B − →G b\n(S, SS)\n(S, AB)\n(A, a) (B, b)\n(S, AT)\n(A, a) (T, SB)\n(S, AB) (B, b)\n(A, a) (B, b)\n▷ Assume G is in (strict) Chomsky normal form, i.e. P ⊆ V × (Σ ∪ V V).\nThen an extended derivation T wrt. G will be a binary tree.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Chomsky normal form and derivation trees 159 (464)\nS − →G SS | AB | AT T − →G SB A − →G a B − →G b\n(S, SS)\n(S, AB)\n(A, a) (B, b)\n(S, AT)\n(A, a) (T, SB)\n(S, AB) (B, b)\n(A, a) (B, b)\n▷ If w ∈ L(G), then every derivation S − →∗\nG w gives rise to a binary derivation\ntree with |w| leaves and thus |w| −1 inner nodes.\nS − →G SS − →G ABS − →G aBS − →G abS − →G . . .− →G abaabb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Chomsky normal form and derivation trees 159 (465)\nS − →G SS | AB | AT T − →G SB A − →G a B − →G b\n(S, SS)\n(S, AB)\n(A, a) (B, b)\n(S, AT)\n(A, a) (T, SB)\n(S, AB) (B, b)\n(A, a) (B, b)\n▷ If we remove the root (S, UV), the derivation tree will be split into two\nderivation trees each representing a derivation U − →∗\nG u and V − →∗\nG v,\nrespectively, with w = uv.\nS − →G SS S − →∗\nG ab S − →∗\nG aabbb\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Chomsky normal form and derivation trees 159 (466)\nS − →G SS | AB | AT T − →G SB A − →G a B − →G b\n(S, SS)\n(S, AB)\n(A, a) (B, b)\n(S, AT)\n(A, a) (T, SB)\n(S, AB) (B, b)\n(A, a) (B, b)\n▷ On the other hand, given a word w we can (i) choose some rule (S, UV), (ii)\nsplit (factorize) w = uv and (iii) recursively try compute to compute\nderivation trees for U − →∗\nG u and V − →∗\nG v.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Chomsky normal form and derivation trees 159 (467)\nS − →G SS | AB | AT T − →G SB A − →G a B − →G b\n(S, SS)\n(S, AB)\n(A, a) (B, b)\n(S, AT)\n(A, a) (T, SB)\n(S, AB) (B, b)\n(A, a) (B, b)\n▷ Replacing the nondeterministic choice of (S, UV) and w = uv by an iteration\nover all rules and the length of w (u = w[:i], v = w[i:]), respectively,\ncombined with dynamic programming yields the CYK algorithm.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cocke-Younger-Kasami (CYK) algorithm 160 (468)\ndef CYK(w, V , P, Σ, S): # w = a1 . . . al\nfor i = 1, 2, . . . , l:\n# determine all X ∈ V with X − →G ai\nVi,i = {X | (X, ai) ∈ P}\nfor d = 1, 2, . . . , l− 1: # d = k − i\nfor i = 1, 2, . . . , l− d:\nk = i + d\nVi,k = ∅\nfor j = i, i+ 1, . . . , k− 1:\n# determine all X ∈ P with X − →G Y Z and Y − →∗\nG ai . . . aj\n# and Z − →∗\nG aj+1 . . . ak\nVi,k = Vi,k ∪ {X | (X, Y Z) ∈ P, Y∈ Vi,j, Z∈ Vj+1,k}\nL Let G = (V, Σ, P, S) be in (strict) Chomsky normal form P ⊆ V × (Σ ∪ V V).\nFor w = a1 . . . al ∈ Σ∗ \\ {ε} we have Vi,k = {X ∈ V | X − →∗\nG ai . . . ak}.\nThus: w ∈ L(G) iff S ∈ V1,|w|\n▷ Simply induction on l = |w|.\n! If we remember in Vi,k not only X but (X, Y Z, j), then the Vi,j will be a\ncompressed representation (actually a context-free grammar itself) of all\nderivation trees of w wrt. G: if (X, Y Z, j) ∈ Vi,k, then Xi,k − →Yi,jZj+1,k.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: example (might contain errors, check it for yourself) 161 (469)\nS − →G AB | BC A − →G BA | a B − →G CC | b C − →G AB | a\n15\n14 25\n13 24 35\n12 23 34 45\n11 22 33 44 55\nb a a b a\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: example (might contain errors, check it for yourself) 161 (470)\nS − →G AB | BC A − →G BA | a B − →G CC | b C − →G AB | a\n15\n14 25\n13 24 35\n12 23 34 45\n11 22 33 44 55\nb a a b a\nB A, C A, C B A, C\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: example (might contain errors, check it for yourself) 161 (471)\nS − →G AB | BC A − →G BA | a B − →G CC | b C − →G AB | a\n15\n14 25\n13 24 35\n12 23 34 45\n11 22 33 44 55\nb a a b a\nB A, C A, C B A, C\nS1, A1 B2 S3, C3 S4, A4\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: example (might contain errors, check it for yourself) 161 (472)\nS − →G AB | BC A − →G BA | a B − →G CC | b C − →G AB | a\n15\n14 25\n13 24 35\n12 23 34 45\n11 22 33 44 55\nb a a b a\nB A, C A, C B A, C\nS1, A1 B2 S3, C3 S4, A4\nB2 B4\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: example (might contain errors, check it for yourself) 161 (473)\nS − →G AB | BC A − →G BA | a B − →G CC | b C − →G AB | a\n15\n14 25\n13 24 35\n12 23 34 45\n11 22 33 44 55\nb a a b a\nB A, C A, C B A, C\nS1, A1 B2 S3, C3 S4, A4\nB2 B4\nS2, C2\nA3\nS4, A4\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: example (might contain errors, check it for yourself) 161 (474)\nS − →G AB | BC A − →G BA | a B − →G CC | b C − →G AB | a\n15\n14 25\n13 24 35\n12 23 34 45\n11 22 33 44 55\nb a a b a\nB A, C A, C B A, C\nS1, A1 B2 S3, C3 S4, A4\nB2 B4\nS2, C2\nA3\nS4, A4\nS1, A1\nS2, C2\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: example (might contain errors, check it for yourself) 161 (475)\nS − →G AB | BC A − →G BA | a B − →G CC | b C − →G AB | a\n15\n14 25\n13 24 35\n12 23 34 45\n11 22 33 44 55\nb a a b a\nB A A B C\nA1 C3\nB4\nS2\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: applications 162 (476)\nD A context-free grammar G is unambiguous if every word w ∈ L(G) has\nexactly one derivation trees; otherwise G is ambiguous.\nL If a context-free language L is accepted by an DPDA, then there is also an\nunambiguous context-free grammar G with L = L(G).\n▷ Simply translate the DPDA into a grammar as discussed:\nAs there is at most once accepting run, the grammar will have at most one\nderivation tree by construction.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: applications 162 (477)\nD A context-free grammar G is unambiguous if every word w ∈ L(G) has\nexactly one derivation trees; otherwise G is ambiguous.\nL If a context-free language L is accepted by an DPDA, then there is also an\nunambiguous context-free grammar G with L = L(G).\n▷ Remarks:\n▷ For programming languages (including formulas/expressions/terms), we want\ndeterministic context-free languages to make parsing simpler:\n! CYK uses O(|w|3) time in general, whereas deterministic context-free\nlanguages can be parsed in time O(|w|) (cf. LR parser).\n▷ But we also want unambiguous grammars as the derivation tree actually tells\nus how to interpret the input, e.g.:\na ∨ (b ∧ c) vs a ∨ b ∧ c vs (a ∨ b) ∧ c\nWhile operator precedence allows to avoid parentheses, it complicates\nrecovering the derivation tree.\nThe parentheses actually encode the derivation tree, and thus how the term\nshould be evaluated (cf. Dyck language, binary trees, Catalan numbers).",
      "a ∨ (b ∧ c) vs a ∨ b ∧ c vs (a ∨ b) ∧ c\nWhile operator precedence allows to avoid parentheses, it complicates\nrecovering the derivation tree.\nThe parentheses actually encode the derivation tree, and thus how the term\nshould be evaluated (cf. Dyck language, binary trees, Catalan numbers).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: applications 162 (478)\nD A context-free grammar G is unambiguous if every word w ∈ L(G) has\nexactly one derivation trees; otherwise G is ambiguous.\nL If a context-free language L is accepted by an DPDA, then there is also an\nunambiguous context-free grammar G with L = L(G).\n▷ Remarks:\n▷ Context-free grammars have been used to model at least subsets of natural\nlanguages like English – Chomsky’s original interest was linguistics.\n▷ And while not all aspects of a natural language are context-free (see e.g.\ncross-serial dependencies), context-free grammars can capture many aspects.\n▷ Also in case of natural language, the actually meaning (semantics) of a\nsentence can depend on its derivation tree:\n“I saw the man on the hill with a telescope.”\nUsing parentheses to visualize the derivation tree/dependencies, e.g.:\n[I [saw [the man] [with [a telescope]]]]\nvs [I [saw [[the man] [with [a telescope]]]]]",
      "sentence can depend on its derivation tree:\n“I saw the man on the hill with a telescope.”\nUsing parentheses to visualize the derivation tree/dependencies, e.g.:\n[I [saw [the man] [with [a telescope]]]]\nvs [I [saw [[the man] [with [a telescope]]]]]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "CYK: applications 162 (479)\nD A context-free grammar G is unambiguous if every word w ∈ L(G) has\nexactly one derivation trees; otherwise G is ambiguous.\nL If a context-free language L is accepted by an DPDA, then there is also an\nunambiguous context-free grammar G with L = L(G).\n▷ Remarks:\n▷ For natural language processing, extending CYK so that it recovers all possible\nderivation trees is thus actually very useful.\n▷ If we further extend each rule of a context-free grammar with a weight, e.g. a\nprobability, then CYK can e.g. also be extended to compute of a derivation tree\nminimal/maximal total weight (sum/product over all rules used in the tree).\n(As the derivation trees then represent multiplicative terms over a semiring, like the\nViterbi semiring ([0, 1], max, ·, 0, 1), and multiplication distributes over addition in\nevery semiring.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "6 Context-free languages\nContext-free grammars: basic properties\nPush-down automata\nDerivation trees\nPumping lemma and closure properties\nAlgebraic equations and EBNF (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma 164 (481)\n(S, AX)\n(A, a)\n(X, Y B)\n(Y, AZ)\n(A, a)\n(Z, Y B)\n(Y, AZ)\n(A, a)\n(Z, AB)\n(A, a) (B, b) (B, b) (B, b)\nS − →G AX\nX − →G Y B\nY − →G AZ\nZ − →G Y B| AB\nA − →G a\nB − →G b\nS − →∗G aY b\n− →∗G a aY b b\n− →∗G a a aab bb\n▷ Recall: For every binary relation R ⊆ V × V we have R∗ = R<|V | as (by the\npigeon-hole principle) every path of length ≥ |V | has to contain a cycle.\n▷ Analogously, if a derivation tree has a path of length ≥ |V | from its root to\none of its leaves (i.e. the tree has height ≥ |V |), then at least one variable\nhas to occur multiple times along this path.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma 164 (482)\n(S, AX)\n(A, a)\n(X, Y B)\n(Y, AZ)\n(A, a)\n(Z, Y B)\n(Y, AZ)\n(A, a)\n(Z, AB)\n(A, a) (B, b) (B, b) (B, b)\nS − →G AX\nX − →G Y B\nY − →G AZ\nZ − →G Y B| AB\nA − →G a\nB − →G b\nS − →∗G aY b\n− →∗G a aY b b\n− →∗G a a aab bb\n▷ This corresponds to a derivation of the form\nS − →∗\nG xlY xr − →∗\nG xlylY yrxr − →∗\nG xlylzyrxr = w\nIn particular, we can repeat or skip the derivation Y − →∗\nG ylY yr s.t.\nxlylkzyrkxr ∈ L for all k ∈ N 0\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma 165 (483)\n(S, AX)\n(A, a)\n(X, Y B)\n(Y, AZ)\n(A, a)\n(Z, Y B)\n(Y, AZ)\n(A, a)\n(Z, AB)\n(A, a) (B, b) (B, b) (B, b)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma 165 (484)\n(S, AX)\n(A, a)\n(X, Y B)\n(Y, AZ)\n(A, a)\n(Z, AB)\n(A, a) (B, b)\n(B, b)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma 165 (485)\n(S, AX)\n(A, a)\n(X, Y B)\n(Y, AZ)\n(A, a)\n(Z, Y B)\n(Y, AZ)\n(A, a)\n(Z, Y B)\n(B, b)\n(Y, AZ)\n(A, a)\n(Z, AB)\n(A, a) (B, b)\n(B, b)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma 165 (486)\n(S, AX)\n(A, a)\n(X, Y B)\n(Y, AZ)\n(A, a)\n(Z, Y B)\n(Y, AZ)\n(A, a)\n(Z, Y B)\n(B, b)\n(Y, AZ)\n(A, a)\n(Z, Y B)\n(B, b)\n(Y, AZ)\n(A, a)\n(Z, AB)\n(A, a) (B, b)\n(B, b)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma for context-free languages 166 (487)\nL If L ⊆ Σ∗ is context-free, then there exists a constant nL ∈ Σ∗ s.t.:\nFor every word w ∈ L with |w| ≥nL there exists a factorization\nw = xlylzyrxr\ns.t.\n1 ylyr ̸= ε 2 |ylzyr| ≤nL 3 xlyk\nl zyk\nr xr ∈ L for all k ∈ N 0\n▷ If L is context-free, then there exists a context-free grammar G = (V, Σ, P, S)\nin Chomsky NF with L = L(G).\nEvery (extended) derivation tree of a word w ∈ L \\ {ε} is a binary tree with\n|w| leaves and |w| −1 inner nodes.\nThe height of a minimal binary tree with |w| leaves is ⌈log2 |w|⌉\nSo, if |w| ≥nL := 2|V |, then there has to be at least one path of length |V |\nfrom the root to a leaf of the derivation tree.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma for context-free languages 166 (488)\nL If L ⊆ Σ∗ is context-free, then there exists a constant nL ∈ Σ∗ s.t.:\nFor every word w ∈ L with |w| ≥nL there exists a factorization\nw = xlylzyrxr\ns.t.\n1 ylyr ̸= ε 2 |ylzyr| ≤nL 3 xlyk\nl zyk\nr xr ∈ L for all k ∈ N 0\n▷ Pick a leaf that has maximal distance to the root, and let Y be the first\nnonterminal that appears a second time when ascending to the root. This\ngives rise to a derivation\nS − →∗\nG xlY xr Y − →∗\nG ylY yr Y − →∗\nG z\nAs G is in Chomsky NF, we have ylyr ̸= ε.\nEvery path to a leaf from this second (from the bottom) occurrence of Y has\nlength at most |V |, so that |ylzyr| ≤2|V | = nL.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma for context-free languages 166 (489)\nL If L ⊆ Σ∗ is context-free, then there exists a constant nL ∈ Σ∗ s.t.:\nFor every word w ∈ L with |w| ≥nL there exists a factorization\nw = xlylzyrxr\ns.t.\n1 ylyr ̸= ε 2 |ylzyr| ≤nL 3 xlyk\nl zyk\nr xr ∈ L for all k ∈ N 0\n! The statement is trivially true if |L| < ∞: simply choose\nnL := 1 + max{|w|: w ∈ L}.\n! nL depends on the language resp. the underlying context-free grammar in\nChomsky NF. The factorization depends on w.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma for context-free languages 166 (490)\nL If L ⊆ Σ∗ is context-free, then there exists a constant nL ∈ Σ∗ s.t.:\nFor every word w ∈ L with |w| ≥nL there exists a factorization\nw = xlylzyrxr\ns.t.\n1 ylyr ̸= ε 2 |ylzyr| ≤nL 3 xlyk\nl zyk\nr xr ∈ L for all k ∈ N 0\n! Also a non-context-free language can satisfy the pumping lemma for\ncontext-free languages. But a language that does not satisfy the pumping\nlemma, can never be context-free:\n▷ Assume L is context-free.\n▷ Then fix some nL ∈ N with above properties, nothing else is known about nL.\n▷ Depending on nL fix a concrete word w ∈ L.\n▷ Finally show that every possible factorization of w violates at least one of the\nproperties 1 , 2 , 3 .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma for context-free languages 166 (491)\nL If L ⊆ Σ∗ is context-free, then there exists a constant nL ∈ Σ∗ s.t.:\nFor every word w ∈ L with |w| ≥nL there exists a factorization\nw = xlylzyrxr\ns.t.\n1 ylyr ̸= ε 2 |ylzyr| ≤nL 3 xlyk\nl zyk\nr xr ∈ L for all k ∈ N 0\nC If L ⊆ Σ∗ is regular, then there exists a constant nL ∈ Σ∗ s.t.:\nFor every word w ∈ L with |w| ≥nL there exists a factorization\nw = xlylz\ns.t.\n1 yl ̸= ε 2 |ylz| ≤nL 3 xlyk\nl z ∈ L for all k ∈ N 0\n▷ As then L = L(G) for some regular (=right-linear context-free) grammar G,\ni.e. all rules are of the form X → aY or X → a, so that xryr = ε.\nEvery (extended) derivation tree is just a path, hence we can choose\nnL = |V | + 1. (Alternatively consider a DFA that accepts L. Then every accepting\nrun of length ≥ |Q| has to contain a cycle that can be repeated/“pumped”.)No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma: {akbkck | k ∈ N 0} 167(492)\n▷ The pumping lemma is used to argue that a language is not context-free.\n▷ Consider L := {akbkck} and assume that L is context-free:\nThen there exists nL ∈ N 0 s.t. for every word w ∈ L with |w| ≥nL there\nexists a factorization w = xlylzyrxr s.t.:\n1 ylyr ̸= ε 2 |ylzyr| ≤nL 3 xlyk\nl zyk\nr xr ∈ L for all k ∈ N 0\nThe goal is to show a contradiction.\n▷ 2 means that the “pumpable” part has to be within a window of nL letters.\n▷ As akbkck has a specific block structure, we should choose k so large, that\nylzyr can range over at most two blocks/kinds of terminals.\nE.g. choose w = anLbnLcnL.\n! We do not need to know the precise value of nL, it suffices that we know\nthat there is some nL ∈ N under the assumption that L is context-free.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma: {akbkck | k ∈ N 0} 167(493)\n▷ The pumping lemma is used to argue that a language is not context-free.\n▷ Consider L := {akbkck} and assume that L is context-free:\nThen there exists nL ∈ N 0 s.t. for every word w ∈ L with |w| ≥nL there\nexists a factorization w = xlylzyrxr s.t.:\n1 ylyr ̸= ε 2 |ylzyr| ≤nL 3 xlyk\nl zyk\nr xr ∈ L for all k ∈ N 0\nThe goal is to show a contradiction.\n▷ We have to argue that there is no factorization of w = anLbnLcnL that\nsatisfies all properties 1 , 2 , 3 at the same time.\n▷ Because of 2 we have to have\nylzyr = aibj or ylzyr = bicj\nfor some i, j∈ N 0 s.t. i + j ≤ nL.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma: {akbkck | k ∈ N 0} 167(494)\n▷ The pumping lemma is used to argue that a language is not context-free.\n▷ Consider L := {akbkck} and assume that L is context-free:\nThen there exists nL ∈ N 0 s.t. for every word w ∈ L with |w| ≥nL there\nexists a factorization w = xlylzyrxr s.t.:\n1 ylyr ̸= ε 2 |ylzyr| ≤nL 3 xlyk\nl zyk\nr xr ∈ L for all k ∈ N 0\nThe goal is to show a contradiction.\n▷ Assume ylzyr = aibj.\n▷ Because of 1 yl or yr have to contain at least one a or one b, but neither\ncan contain a c.\n▷ Thus, if we introduce additional copies of yl and yr, we increase the number\nof as or bs, while the number cs stays unchanged.\n▷ So xlyk\nl zyk\nr xr ̸∈ L for any k >1.\n▷ Thus {akbkck | k ∈ N 0} is not context-free, but only context-sensitive.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Pumping lemma: {uu | u ∈ {a, b}∗} 168(495)\n▷ For another example, we show that L := {uu | w ∈ {a, b}∗} is not\ncontext-free.\n▷ Again, assume that L is context-free in order to obtain a contradiction:\nThen there exists nL ∈ N 0 s.t. for every word w ∈ L with |w| ≥nL there\nexists a factorization w = xlylzyrxr s.t.:\n1 ylyr ̸= ε 2 |ylzyr| ≤nL 3 xlyk\nl zyk\nr xr ∈ L for all k ∈ N 0\nThe goal is again to pick a word w ∈ L for which we can show that\n“pumping” will destroy the required structures.\n▷ Let w = uu = anLbnLanLbnL with u = anLbnL\n▷ Because of 2 , only three cases can arise: ylzyr is contained in the\n▷ left anLbnL: then xly0\nl zy0\nrxr = akblanLbnL.\n▷ right anLbnL: then xly0\nl zy0\nrxr = anLbnLakbl.\n▷ middle bnLanL: then xly0\nl zy0\nrxr = anLbkalbnL.\nwith k < nL or l < nL as ylyr ̸= ε. Complete the proof.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Closure properties 169 (496)\nL Context-free languages are closed wrt. union, concatenation and Kleene star.\n▷ Union: S − →G S1 | S2 (as for any grammar type).\n▷ Concatenation: S − →G S1S2.\n▷ Kleene star: S − →G ε | S1S.\nC Context-free languages are not closed wrt. intersection or complement.\n▷ as {akbkcl | k, l∈ N 0} ∩ {akblcl | k, l∈ N 0} = {akbkck | k ∈ N 0}.\n! Both languages are already deterministic context-free.\n▷ And because of de Morgan: L1 ∩ L2 = L1 ∪ L2.\n▷ (Recall: intersection and complement are still context-sensitive.)\n! Show: The intersection of a context-free language with a regular language is\nstill context-free.\nL (w/o proof): Deterministic cf. languages are closed wrt. complement;\nbut not closed wrt. union, intersection, concatenation, Kleene star.\nC The det. cf. languages are a strict subset of the cf. languages.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Decidable properties 170 (497)\nL |L(G)| = 0 and |L(G)| = ∞ are decidable for context-free grammars:\n▷ L(G) = ∅ iff there is a derivation tree of height at most |V | −1.\n▷ |L(G)| = ∞ iff there is a derivation tree of height at least |V |.\nL w ∈ L(G) is decidable for context-free grammars (CYK).\nT Given two DPDA M1, M2 it is decidable if L(M1) = L(M2).\n(S´ enizergues, G¨odel Prize 2002).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Undecidable properties 171 (498)\nT For two context-free grammars G1, G2\nL(G1)∩L(G2) = ∅ |L(G1)∩L(G2)| = ∞ L(G1) ⊆ L(G2) L(G1) = L(G2)\nare all undecidable.\n▷ We have shown the first two already by encoding a PCP instance\n(u(1), v(1)), . . . ,(u(n), v(n)) as two linear context-free grammars\nSu − →Gu u(i)Suai | u(i)#ai resp. Sv − →Gv v(i)Svai | v(i)#ai\nLet GR\nu and GR\nv be the grammars with the right-hand sides reversed, i.e.\n(X, γ) is replaced by (X, γR).\nThen L1 := L(GR\nu ) and L2 = L(GR\nv ) are also deterministic.\nThus also L1 and L2 are deterministic context-free. Then:\nL1 ∩ L2 = ∅ iff L1 ⊆ L2 iff L1 ∪ L2 = L2\nwith L3 := L1 ∪ L2 still context-free.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Undecidable properties 171 (499)\nT For two context-free grammars G1, G2\nL(G1)∩L(G2) = ∅ |L(G1)∩L(G2)| = ∞ L(G1) ⊆ L(G2) L(G1) = L(G2)\nare all undecidable.\nC For two deterministic context-free languages L1, L2\nL1 ∩ L2 = ∅ |L1 ∩ L2| = ∞ L1 ⊆ L2\nare all undecidable.\nC For a context-free grammar G it is in general undecidable if L(G) = Σ∗.\n▷ If L1, L2 are det. context-free, then L1 ∪ L2 is still context-free.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Undecidable properties 171 (500)\nT For two context-free grammars G1, G2\nL(G1)∩L(G2) = ∅ |L(G1)∩L(G2)| = ∞ L(G1) ⊆ L(G2) L(G1) = L(G2)\nare all undecidable.\nT W/o proofs (see e.g. Sch ¨oning):\nFor two (det.) cf. languages it is undecidable if L1 ∩ L2 is again context-free.\nFor a cf. grammar G it is undecidable if (i) G is ambiguous, (ii) L(G) is\ncontext-free, (iii) L(G) is regular, (iv) L(G) is deterministic context-free.\n▷ Thus it is also undecidable for a context-sensitive grammar G if L(G) is\ncontext-free as L1 ∩ L2 is context-sensitive.\n▷ Recall: for a unrestricted grammar G it is undecidable if L(G) is\ncontext-sensitive.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Chomsky hierarchy: Comparison 172 (501)\n▷ Closure properties (with 2d for deterministic context-free):\nType L1 ∪ L2 L1 ∩ L2 L L1L2 L∗\n3 3 3 3 3 3\n2d 2 1 2d 2 2\n2 2 1 1 2 2\n1 1 1 1 1 1\n0 0 0 − 0 0\n▷ (Un)decidable problems:\nType w ∈ L L = ∅ L = Σ∗ L1 = L2 L1 ⊆ L2 L1 ∩ L2 = ∅\n3 d, O(|w|) d d d d d\n2d d, O(|w|) d d d u u\n2 d, O(|w|3) d u u u u\n1 d, O(2|w|) u u u u u\n0 u u u u u u\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "6 Context-free languages\nContext-free grammars: basic properties\nPush-down automata\nDerivation trees\nPumping lemma and closure properties\nAlgebraic equations and EBNF (*)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Context-free grammars as equations 174 (503)\n▷ While regular grammars correspond to systems of linear fixed-point\nequations, context-free grammars correspond to systems of\npolynomial/algebraic fixed-point equations.\n▷ E.g.\nS − →G aSS | bS | c X = f(X) := aXX + bX + c\nAgain, L(G) is the least solution of X = f(X) which is the limit of the\nfixed-point iteration fk(0).\nIn the example\nf(0) = c f 2(0) = acc + bc + c . . .\n▷ Induction shows that fk+1(0) is exactly the language of words that posses an\nextended derivation of height k.\nIn the example, the grammar is unambiguous and every word is the prefix\nencoding of its derivation tree, e.g. abccˆ =a(b(c), c).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended Backus-Naur form (EBNF) 175 (504)\n▷ As context-free languages are closed wrt. regular/rational operations, i.e.\nunion, concatenation and Kleene star, we could (but won’t!) allow regular\nexpression over both Σ and V on the right-hand side, e.g.:\nS − →G XY S| Y Z| V S − →G (XY + Z)∗V\nThis requires that ∗, (, ) and + are treated as meta symbols just like |.\n▷ Alternatively, we could define context-free grammars using algebraic notation\nfor semirings extended by the Kleene star, but that makes the use of the then\nreserved meta symbols 0, 1, +, ∗, (, ) cumbersome.\n▷ In practice, extended Backus-Naur form (EBNF) is thus used e.g. for defining\nthe context-free grammars of programming languages.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended Backus-Naur form (EBNF) 176 (505)\n▷ EBNF uses the following meta symbols with the intended semantics:\n▷ Quotation marks ” or ′ are used to indicate terminals, cf. strings in Python.\n▷ Unquoted text (can include spaces) is treated as nonterminals.\n▷ γ, γ′ (comma!) is used for concatenation, i.e. γγ′.\n▷ γ|γ′ is still used for choice, i.e. γ + γ′.\n▷ X = γ; is used for a production rule, i.e. X − →G γ.\n▷ {γ} stands for the Kleene star, i.e. γ∗.\n▷ [γ] stands for “at most once/optional”, i.e. (γ + ε)\n▷ Parentheses (, ) are used for grouping as in regular expressions.\n▷ k∗ γ with k ∈ N stands for “ k copies”, i.e. γk.\n! These rules still only allow to define context-free languages.\nNonZeroDigit = ”1” | ”2” | ”3” | ”4” | ”5” | ”6” | ”7” | ”8” | ”9” ;\nDigit = ”0” | NonZeroDigit ;\nNaturalNumber = NonZeroDigit, {Digit};\nInteger = ”0” | [”-”], NaturalNumber ;\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Extended Backus-Naur form (EBNF) 176 (506)\n▷ EBNF uses the following meta symbols with the intended semantics:\n▷ Quotation marks ” or ′ are used to indicate terminals, cf. strings in Python.\n▷ Unquoted text (can include spaces) is treated as nonterminals.\n▷ γ, γ′ (comma!) is used for concatenation, i.e. γγ′.\n▷ γ|γ′ is still used for choice, i.e. γ + γ′.\n▷ X = γ; is used for a production rule, i.e. X − →G γ.\n▷ {γ} stands for the Kleene star, i.e. γ∗.\n▷ [γ] stands for “at most once/optional”, i.e. (γ + ε)\n▷ Parentheses (, ) are used for grouping as in regular expressions.\n▷ k∗ γ with k ∈ N stands for “ k copies”, i.e. γk.\n! EBNF also knows − (minus) for set difference, but recall that in general\n▷ L1 \\ L2 is not context-free anymore; but if L2 is regular, L1 \\ L2 = L1 ∩ L2 is\nguaranteed to remain context-free.\n▷ It is undecidable, if a variable X of a cf. grammar produces a regular language;",
      "! EBNF also knows − (minus) for set difference, but recall that in general\n▷ L1 \\ L2 is not context-free anymore; but if L2 is regular, L1 \\ L2 = L1 ∩ L2 is\nguaranteed to remain context-free.\n▷ It is undecidable, if a variable X of a cf. grammar produces a regular language;\nbut we can check, if the grammar X refers to is regular.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "7 Complexity theory: deterministic vs. nondeterministic polynomial time\nResource-bounded computation\nPolynomially bounded computation\nP vs NP\nNP-complete problems\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Measuring time and space 179 (509)\n▷ So far, we have mostly considered the question if some problem is\n(semi)decidable/computable or not. Recall:\n▷ Computability of a (partial) function f : {0, 1}∗ ,→ {0, 1}∗$ can be reduced to\nthe (semi)decidability of its “graph” {x#y | f(x) = yz}.\n▷ If L is given by a DPDA, we can decide w ∈ L in time and space O(|w|).\n▷ If L is given by cf. grammar in Chomsky NF, we can decide w ∈ L using CYK\nin time and space O(|w|3).\n▷ If L is given by a context-sensitive grammar (type 1), i.e. we impose a linear\nspace bound (LBA), w ∈ L can be decided in time and space 2O(|w|) using\nBFS (and nondeterministically in space O(|w|)).\n▷ If L is given by a unrestricted grammar (type 0), then w ∈ L is only\nsemidecidable. In particular, there cannot be a computable time or space\nbound.\n▷ Motivated by the word problem, the natural way of measuring time and space\nused is wrt. the length |w| of the input w.",
      "▷ If L is given by a unrestricted grammar (type 0), then w ∈ L is only\nsemidecidable. In particular, there cannot be a computable time or space\nbound.\n▷ Motivated by the word problem, the natural way of measuring time and space\nused is wrt. the length |w| of the input w.\n! For an integer n ∈ N its input length is usually the length of its binary\nrepresentation, i.e. |lsbf(n)| = |msbf(n)| ≈log2 n.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: Landau symbols 180 (510)\n▷ Reminder: If f, gare continuous and lim supx→∞\nf(x)\ng(x) < ∞, then f ∈ O(g).\n▷ Reminder: If f, gare continuous and limx→∞\nf(x)\ng(x) = 0, then f ∈ o(g), e.g.:\nlog(log(x)) <o log(x) <o\n√x <o x <o x log(x) <o x2 <o x3 <o e\n√x <o ex\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Multi-tape Turing machines 181 (511)\n▷ In short, a k-tape TM (Q, Σ, Γ, δ, q0, □, F) is a 1TM with a vector alphabet\nΓk but k independent heads, i.e.:\nδ ⊆ (Q × Γk) × (Q × (Γ × {−1, 0, +1})k)\nA k-tape TM can be thought of a k-core CPU (choose\nQ = Q0 × Q1 × . . .× Qk).\n▷ “TM” short for “multi-tape TM”, if the concrete k does not matter.\nCan be nondeterministic; also NTM for emphasis.\n▷ “DTM” short for “deterministic multi-tape TM”\n▷ Often multi-tape TMs have an exclusive read-only input tape.\n(Sometimes also an exclusive write-only output tape.)\n▷ Multi-tape TMs are just a very simple and convenient formalism to abstract\nfrom a concrete CPU model (number of cores, clock speed, architecture, . . . )\nwhen considering resource-bounded computations.\nIn the end, a 1TM can simulate a single step of k-tape TM by means of a\nvector alphabet and a sweep over the complete tape content.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Resource-bounded computation 182 (512)\nD Let f : N 0 → N 0 be some function, and M some (multi-tape) TM.\nM is f-time-bounded if there is some f′ ∈ O(f) s.t. M terminates for all\ninputs w ∈ Σ∗ after at most f′(|w|) steps.\nM is f-space-bounded if there is some f′ ∈ O(f) s.t. M for all inputs\nw ∈ Σ∗ moves every head at most f′(|w|) positions from the start.\n▷ In case of space-bounded computation, we sometimes need to use a separate\nread-only input tape, so that we can bound the space used for the actual\ncomputation.\n▷ The convention is to treat f : R → R as ⌈|f(|w|)|⌉, so that we can also use\nf(n) = n log(n) or f(n) = 2n or f(n) = 2\n√n as bounds.\n▷ For time-bounded computation, if f(n) < n(sublinear), then the TM cannot\nread the complete input w, so the output can only depend on a sublinear\nprefix of w.\n▷ For space-bounded computation, sublinear bounds like log(n) (excluding the\nexclusive input tape) still allow an NTM to decide graph reachability.",
      "read the complete input w, so the output can only depend on a sublinear\nprefix of w.\n▷ For space-bounded computation, sublinear bounds like log(n) (excluding the\nexclusive input tape) still allow an NTM to decide graph reachability.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Resource-bounded computation 182 (513)\nD Let f : N 0 → N 0 be some function, and M some (multi-tape) TM.\nM is f-time-bounded if there is some f′ ∈ O(f) s.t. M terminates for all\ninputs w ∈ Σ∗ after at most f′(|w|) steps.\nM is f-space-bounded if there is some f′ ∈ O(f) s.t. M for all inputs\nw ∈ Σ∗ moves every head at most f′(|w|) positions from the start.\n▷ The use of O(f) allows to abstract from the concrete TM (CPU):\n▷ The (non-unary) input alphabet Σ only matters up to a factor log2 |Σ| (“32bit\nvs. 64bit”).\n▷ By the Linear speedup theorem a k-tape TM with a t-time bound and an\ns-space bound can be simulated by t\nc -time, s-space bounded TM for any\nchoice of c >1 (“2GHz vs. 4GHz”).\n▷ A t-time, s-space bounded k-tape TM can be simulated by a t2-time, s-space\nbounded 1TM using a vector tape alphabet (“1 core vs. k cores”).\n▷ In all cases, if the original TM was deterministic, then also the simulator will\nbe deterministic; in particular, for space-bounded computation, there is no",
      "bounded 1TM using a vector tape alphabet (“1 core vs. k cores”).\n▷ In all cases, if the original TM was deterministic, then also the simulator will\nbe deterministic; in particular, for space-bounded computation, there is no\ndifference (cf. LBA).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Resource-bounded computation 182 (514)\nD Let f : N 0 → N 0 be some function, and M some (multi-tape) TM.\nM is f-time-bounded if there is some f′ ∈ O(f) s.t. M terminates for all\ninputs w ∈ Σ∗ after at most f′(|w|) steps.\nM is f-space-bounded if there is some f′ ∈ O(f) s.t. M for all inputs\nw ∈ Σ∗ moves every head at most f′(|w|) positions from the start.\n! In particular for an NTM M where we may have multiple runs:\n▷ If M is f-time-bounded, then every run of M on input |w| has length at most\nf(|w|).\n▷ If M is f-space-bounded, then every run of M on input |w| has to terminate\nor become cyclic after at most (Q × Γ)f(|w|) steps.\nF If M is f-time-bounded, it is also f-space-bounded.\n▷ As M can move its tape heads by at most one position in every step.\nF If M is f-space-bounded, it can be made 2f -time-bounded (BFS),\nat least if f is constructible.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Resource-bounded computation 182 (515)\nD Let f : N 0 → N 0 be some function, and M some (multi-tape) TM.\nM is f-time-bounded if there is some f′ ∈ O(f) s.t. M terminates for all\ninputs w ∈ Σ∗ after at most f′(|w|) steps.\nM is f-space-bounded if there is some f′ ∈ O(f) s.t. M for all inputs\nw ∈ Σ∗ moves every head at most f′(|w|) positions from the start.\n▷ If M is f-time/space-bounded, then M can be turned into a\nf-time/space-bounded TM that on input 1n outputs 1m with m ≤ f(n).\nD f : N 0 → N 0 is time/space-constructible if there exists a\nf-time/space-bounded (multi-tape) DTM that on input 1n outputs 1f(n).\n▷ For space-bounds f(n) < nan exclusive read-only input tape is used and the\nadditional space is bounded by f.\nF If f is time/space-constructible, then we can impose a f-time/space-bound\non every other TM.\nF Every polynomial p ∈ Z [X] is time/space constructible.\nAlso n log(n) and n√n and 2n are space/time-constructible.",
      "additional space is bounded by f.\nF If f is time/space-constructible, then we can impose a f-time/space-bound\non every other TM.\nF Every polynomial p ∈ Z [X] is time/space constructible.\nAlso n log(n) and n√n and 2n are space/time-constructible.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Resource-bounded computation 182 (516)\nD Let f : N 0 → N 0 be some function, and M some (multi-tape) TM.\nM is f-time-bounded if there is some f′ ∈ O(f) s.t. M terminates for all\ninputs w ∈ Σ∗ after at most f′(|w|) steps.\nM is f-space-bounded if there is some f′ ∈ O(f) s.t. M for all inputs\nw ∈ Σ∗ moves every head at most f′(|w|) positions from the start.\nT As mentioned already for LBAs (w/o proofs):\nSavitch’s theorem:\nFor every f-space-bounded NTM M, there exists a f2-space-bounded DTM\nM′ s.t. L(M) = L(M′).\nImmerman–Szelepcs´ enyi theorem:\nFor every f-space-bounded NTM M with f(|w|) ≥ log(|w|), there exists a\nf-space-bounded NTM M′ s.t. L(M′) = L(M).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Resource-bounded computation 182 (517)\nD Let f : N 0 → N 0 be some function, and M some (multi-tape) TM.\nM is f-time-bounded if there is some f′ ∈ O(f) s.t. M terminates for all\ninputs w ∈ Σ∗ after at most f′(|w|) steps.\nM is f-space-bounded if there is some f′ ∈ O(f) s.t. M for all inputs\nw ∈ Σ∗ moves every head at most f′(|w|) positions from the start.\n! In case of time-bounded computation, we do not know in general\n▷ if nondeterminism can be simulated by determinism with less than an\nexponential blow up, and\n▷ if nondeterminism can be complemented more efficiently then via determinism.\n▷ The class of “tractable decision problem” is thus defined as the set of all\nlanguages L that can be decided by some polynomial-time-bounded DTM.\n▷ Polynomial bounds have the advantage that for p, q∈ Z [X] also\np + q, pq, p◦ q ∈ Z [X]:\nE.g. if a p-time bounded DTM computes w′ from w and then calls a q-time\nbounded DTM on w′ to obtain w′′, we have |w′′| ≤q(|w′|) ≤ q(p(|w|)).",
      "languages L that can be decided by some polynomial-time-bounded DTM.\n▷ Polynomial bounds have the advantage that for p, q∈ Z [X] also\np + q, pq, p◦ q ∈ Z [X]:\nE.g. if a p-time bounded DTM computes w′ from w and then calls a q-time\nbounded DTM on w′ to obtain w′′, we have |w′′| ≤q(|w′|) ≤ q(p(|w|)).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "7 Complexity theory: deterministic vs. nondeterministic polynomial time\nResource-bounded computation\nPolynomially bounded computation\nP vs NP\nNP-complete problems\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Complexity classes for decision problems 184 (519)\nD For simplicity, let L ⊆ {0, 1}∗ denote a binary language:\n▷ DTIME(f) is the set of all L decidable by a f-time bounded DTM.\n▷ NTIME(f) is the set of all L decidable by a f-time bounded NTM.\n▷ DSPACE(f) is the set of all L decidable by a f-space bounded DTM.\n▷ NSPACE(f) is the set of all L decidable by a f-space bounded NTM.\n▷ P := S\nk∈N 0 DTIME(nk) (polynomial time)\n▷ NP := S\nk∈N 0 NTIME(nk) (nondeterministic polynomial time)\n▷ PSPACE := S\nk∈N 0 DSPACE(nk) (polynomial space)\nF P ⊆ NP ⊆ PSPACE\nL By Savitch’s theorem: PSPACE = S\nk∈N 0 NSPACE(nk)\n▷ Let ⌊G, w⌉ denote some fixed binary encoding of a grammar G = (V, Σ, P, S)\nand a word w ∈ Σ∗. Then the word problem for languages of type τ is the\nlanguage Lτ = {⌊G, w⌉ |G = (V, Σ, P, S) is of type τ; w ∈ L(G)}.\nAs seen: L2 ∈ P and L1 ∈ PSPACE and L0 ̸∈ PSPACE.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Complexity classes for decision problems 184 (520)\nD For simplicity, let L ⊆ {0, 1}∗ denote a binary language:\n▷ DTIME(f) is the set of all L decidable by a f-time bounded DTM.\n▷ NTIME(f) is the set of all L decidable by a f-time bounded NTM.\n▷ DSPACE(f) is the set of all L decidable by a f-space bounded DTM.\n▷ NSPACE(f) is the set of all L decidable by a f-space bounded NTM.\n▷ P := S\nk∈N 0 DTIME(nk) (polynomial time)\n▷ NP := S\nk∈N 0 NTIME(nk) (nondeterministic polynomial time)\n▷ PSPACE := S\nk∈N 0 DSPACE(nk) (polynomial space)\n! It is still open whether P = NP or even P = PSPACE.\nF If L ∈ P, then also L ∈ P; if L ∈ PSPACE, then also L ∈ PSPACE.\n! It is open whether NP = coNP := {L | L ∈ NP}.\nF If NP ̸= coNP, then P ̸= NP and P ̸= PSPACE.\n! But also P ̸= NP = coNP is still possible.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Complexity classes for decision problems 184 (521)\nD For simplicity, let L ⊆ {0, 1}∗ denote a binary language:\n▷ DTIME(f) is the set of all L decidable by a f-time bounded DTM.\n▷ NTIME(f) is the set of all L decidable by a f-time bounded NTM.\n▷ DSPACE(f) is the set of all L decidable by a f-space bounded DTM.\n▷ NSPACE(f) is the set of all L decidable by a f-space bounded NTM.\n▷ P := S\nk∈N 0 DTIME(nk) (polynomial time)\n▷ NP := S\nk∈N 0 NTIME(nk) (nondeterministic polynomial time)\n▷ PSPACE := S\nk∈N 0 DSPACE(nk) (polynomial space)\n▷ P is the usual formal definition of “tractable decision problem” as it is\nindependent of the number of tapes/cores, alphabet/architecture, . . . .\n▷ In case of quantum computers, BQP is commonly used as “class of\nquantum-tractable decision problems”.\n▷ We know that P ⊆ BQP ⊆ PSPACE (via counting),\nbut as P = PSPACE is still possible, so is P = BQP.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Overview 185 (522)\nP\n(type2)\n(type3)\nNPcoNP\nBQP\nPSPACE(type1, LBAs)\nEXPTIME\n...\nRE(type0) coRE\n(tikz code based on [here])\n▷ P := S\nk∈N 0 DTIME(nk) (polynomial time)\n▷ NP := S\nk∈N 0 NTIME(nk) (nondeterministic polynomial time)\n▷ PSPACE := S\nk∈N 0 DSPACE(nk) (polynomial space)\n▷ EXPTIME := S\nk∈N 0 DTIME(2nk\n) (exponential time)\n▷ RE: recursively/computably enumerable, semi-decidable, (Turing) recognizable, type 0\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Overview 185 (523)\nP\n(type2)\n(type3)\nNPcoNP\nBQP\nPSPACE(type1, LBAs)\nEXPTIME\n...\nRE(type0) coRE\n(tikz code based on [here])\n▷ Recall, if L = L(G) is of type τ, then w\n?\n∈ L can be decided in\nτ = 1: (constext-sensitive, LBAs) NSPACE(|w|) ⊆ DSPACE(|w|2).\nBut e.g. {anbncn | n ∈ N 0} is already decidable in DTIME(|w|).\nτ = 2: (context-free, PDAs) DTIME(|w|3) using e.g. CYK.\nτ = 3: (regular, FAs) in DTIME(|w|). In fact, τ = 3 corresponds to DSPACE(1) (i.e. 2-way\nFAs) as for space-bounded computation the read-only input is not counted, cf. here.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Overview 185 (524)\nP\n(type2)\n(type3)\nNPcoNP\nBQP\nPSPACE(type1, LBAs)\nEXPTIME\n...\nRE(type0) coRE\n(tikz code based on [here])\n▷ DTIME(f) ⊆ NTIME(f) ⊆ DSPACE(f) ⊆ DTIME(2O(f)) for f constructible.\n! A constructible time bound implies decidability (recall the Busy Beaver functions).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Overview 185 (525)\nP\n(type2)\n(type3)\nNPcoNP\nBQP\nPSPACE(type1, LBAs)\nEXPTIME\n...\nRE(type0) coRE\n(tikz code based on [here])\n▷ Hierachy theorems (w/o proofs):\nDTIME(f) ⊊ DTIME(f(log f)2) and DSPACE(f) ⊊ DSPACE(f log f)\nThus: P ⊊ EXPTIME\n▷ Savitch: NSPACE(f) ⊆ DSPACE(f2) if f(n) ∈ Ω(log(n)) constructible.\nThus PSPACE = NPSPACE.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Overview 185 (526)\nP\n(type2)\n(type3)\nNPcoNP\nBQP\nPSPACE(type1, LBAs)\nEXPTIME\n...\nRE(type0) coRE\n(tikz code based on [here])\n▷ coX(f) := {L | L ∈ X(f)}\n! The complement L is usually taken wrt. some specific universe.\nOften L = Σ∗ \\ L if L ⊆ Σ∗.\nBut e.g. if SAT denotes the set of all satisfiable propositional formulas, then\nSAT = UNSAT is the set of all unsatisfiable boolean formulas (as we can decide in\npolynomial time, if some x is a syntactically valid propositional formula).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Overview 185 (527)\nP\n(type2)\n(type3)\nNPcoNP\nBQP\nPSPACE(type1, LBAs)\nEXPTIME\n...\nRE(type0) coRE\n(tikz code based on [here])\n▷ coX(f) := {L | L ∈ X(f)}\n▷ RE ̸= coRE (recall Halting problem and friends)\n▷ If X is deterministic, then X(f) = coX(f), so: P = coP, PSPACE = coPSPACE\n▷ Immerman–Szelepcs´ enyi:NSPACE(f) = coNSPACE(f) if f(n) ∈ Ω(log(n)) constructible.\n! Wrt. nondeterministic time complementation seems to require an exponential blow-up.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Overview 185 (528)\nP\n(type2)\n(type3)\nNPcoNP\nBQP\nPSPACE(type1, LBAs)\nEXPTIME\n...\nRE(type0) coRE\n(tikz code based on [here])\n▷ BQP (“bounded-error quantum polynomial time”): BQP ⊆ PSPACE\n! Open: NP ?= coNP (i.e. SAT\n?\n∈ coNP, see NP-completeness)\n! Open: P ?= NP (i.e. SAT ∈ P; then also NP = coNP).\n! Open: P ?= PSPACE (i.e. type 1 decidable in P; then also NP = coNP = BQP).\n▷ Only very rough picture, see e.g. complexity zoo for more information.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Decision vs. general computation in polynomial time 186 (529)\n▷ Recall: f : {0, 1}∗ ,→ {0, 1}∗{$} is computable\niff Lf = {w#u | f(w) = uv} is semidecidable.\nD FP denotes the set of all functions f : {0, 1}∗ → {0, 1}∗ computable by some\npolynomial-time bounded DTM.\n! If f ∈ FP, then there exists some polynomial p ∈ N 0[X] s.t. |f(w)| ≤p(|w|)\nfor all w ∈ {0, 1}∗. Hence, also Lf := {w#u | f(w) = uv, |u| ≤p(|w|)} ∈P.\nAnd if Lf ∈ P, then f ∈ FP via search using at most p(|w|) decision calls:\n▷ Binary search: iteratively decide whether the i-th bit of f(w) is 0 or 1.\n▷ We currently do not know whether “nondeterministic polynomial-time function\ncomputation” can be reduced to “nondeterministic polynomial-time decision”\nby means of search (see e.g. Bellare, Goldwasser and complexity zoo).\n(See Shafi Goldwasser and Mihir Bellare on wikipedia; both researchers are known for\nthe important contributions to both complexity theory and cryptography, as modern",
      "by means of search (see e.g. Bellare, Goldwasser and complexity zoo).\n(See Shafi Goldwasser and Mihir Bellare on wikipedia; both researchers are known for\nthe important contributions to both complexity theory and cryptography, as modern\ncryptography is building up on complexity theor; e.g. cryptography requires P ̸= NP.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "7 Complexity theory: deterministic vs. nondeterministic polynomial time\nResource-bounded computation\nPolynomially bounded computation\nP vs NP\nNP-complete problems\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "P: Big-Oh and worst-case complexity 188 (531)\n▷ Some “drawbacks” of P:\n▷ Worst-case computation: not always that important.\nOften “efficient for most problems” is good enough (cf. simplex algorithm,\nsmoothed analysis); and Cryptography actually wants “almost always\nworst-case”.\n▷ Big-Oh notation: hidden constant factor can be forbiddingly large.\n▷ For internet applications, already quadratic time can be too high; e.g. consider\nencryption of a video stream.\n▷ Cubic time is often already problematic e.g. numerical simulations lead to large\nsystems of linear equations where Gaussian elimination (see iterative methods).\n▷ Still, the first step is to show L ∈ P resp. f ∈ FP.\n▷ E.g. only in 2002 Agrawal, Kayal, and Saxena could give a\nO((log n)12(log logn)) deterministic algorithm for deciding if a given n ∈ N is\na prime; but within the next few years, the complexity was reduced to\nO((log n)6(log logn)).\n! Recall: the input length of n is |lsbf(n)| = log2 n.",
      "▷ E.g. only in 2002 Agrawal, Kayal, and Saxena could give a\nO((log n)12(log logn)) deterministic algorithm for deciding if a given n ∈ N is\na prime; but within the next few years, the complexity was reduced to\nO((log n)6(log logn)).\n! Recall: the input length of n is |lsbf(n)| = log2 n.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Important polynomial-time decision problems 189 (532)\n▷ Important problems included in P = S\nk∈N 0 DTIME(nk):\n▷ Word problem for a cf. grammar G: given w and G decide w ∈ L(G).\n▷ Circuit value problem (CVP): given a circuit C (“formula with subterm\nsharing”, boolean SLP) and an input x compute the output of a specific gate.\n▷ 2SAT: given a clause set F where every clause contains at most two literals\ndecide if F is satisfiable.\n▷ Horn-SAT: given a clause set F where every clause contains at most one\npositive literal decide if F is satisfiable.\n▷ 2COL: given a finite graph G = (V, E) decide if there exists a node coloring\nχ: V → [2] using at most 2 colors.\n▷ Linear programming (LP): given A ∈ Z m×n, b∈ Z n, c∈ Z m, γ∈ Z decide if\nthere is some x ∈ Q n with x ≥ 0, Ax ≤ b and c⊤x ≥ γ.\n▷ Graph reachability: decide sE∗t for a graph G = (V, E) and nodes s, t∈ V .\n▷ Reachability games: decide if player 1 can guarantee a visit to node t starting",
      "▷ Linear programming (LP): given A ∈ Z m×n, b∈ Z n, c∈ Z m, γ∈ Z decide if\nthere is some x ∈ Q n with x ≥ 0, Ax ≤ b and c⊤x ≥ γ.\n▷ Graph reachability: decide sE∗t for a graph G = (V, E) and nodes s, t∈ V .\n▷ Reachability games: decide if player 1 can guarantee a visit to node t starting\nin s no matter how player 2 chooses to move in G = (V1 ⊎V2, E) and s, t∈ V .\n▷ Primality: given n ∈ N in binary decide if n is prime.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Important nondeterministic polynomial-time decision problems 190 (533)\n▷ Important problems included in NP = S\nk∈N 0 NTIME(nk), but so far not\nknown to be included in P:\n▷ SAT: given a propositional formula F decide if F is satisfiable.\n▷ 3SAT: given a clause set F where every clause contains at most three literals\ndecide if F is satisfiable.\n▷ 3COL: given a finite graph G = (V, E) decide if there exists a node coloring\nχ: V → [3] using at most 3 colors.\n▷ Hamiltonian cycle problem: given a finite graph G = (V, E) decide if G is\nhamiltonian, i.e. if there is a simple cycle that visits every node of G exactly\nonce.\n▷ Integer linear programming (ILP): given A ∈ Z m×n, b∈ Z n, c∈ Z m, γ∈ Z\ndecide if there is some x ∈ Z n with x ≥ 0, Ax ≤ b and c⊤x ≥ γ.\n▷ Graph isomorphism: given G1 = (V1, E1) and G2 = (V2, E2) decide if\nG1 ∼= G2.\n▷ Subgraph isomorphism: given G1 = (V1, E1) and G2 = (V2, E2) decide if there\nis some H1 ≤ G1 with H1 ∼= G2.",
      "decide if there is some x ∈ Z n with x ≥ 0, Ax ≤ b and c⊤x ≥ γ.\n▷ Graph isomorphism: given G1 = (V1, E1) and G2 = (V2, E2) decide if\nG1 ∼= G2.\n▷ Subgraph isomorphism: given G1 = (V1, E1) and G2 = (V2, E2) decide if there\nis some H1 ≤ G1 with H1 ∼= G2.\n▷ Factoring: given n, k∈ N in binary decide if n has a nontrivial factor in [k, n).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "NP: problems with polynomial-time verifiable solutions 191 (534)\n▷ The characterizing property of decision problems contained in NP is that in\npolynomial-time a potential solution to the related computational problem\ncan be guessed and verified.\n▷ SAT: guess an assignment, then evaluate the formula (cf. CVP).\n▷ 3SAT: guess an assignment, then evaluate the formula (cf. CVP).\n▷ 3COL: guess an coloring, then check for each node its neighbors\n▷ Hamiltonian cycle problem: guess a permutation of the nodes, then check if it\nis a cycle.\n▷ Integer linear programming (ILP): guess some polynomial-size z ∈ Z n (!)\n▷ Graph isomorphism: guess a bijection of the nodes, then check that the edges\nare respected.\n▷ Subgraph isomorphism: additionally, guess a subgraph first; seems to make\nthings more difficult (cf. here).\n▷ Factoring: guess a number in [k, n) and check that it divides n.\n! In general, the nondeterministic choices that led to an accepting computation\nof a NTM.",
      "are respected.\n▷ Subgraph isomorphism: additionally, guess a subgraph first; seems to make\nthings more difficult (cf. here).\n▷ Factoring: guess a number in [k, n) and check that it divides n.\n! In general, the nondeterministic choices that led to an accepting computation\nof a NTM.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Nondeterminism as deterministic verifier 192 (535)\nu0 u1 u2 u3 u4 u5 u6 u7\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\nM(w, ui)\n▷ Sometimes it is easier to view an NTM M as a DTM which takes as\nadditional input a “tie-breaker string” u that tells M which applicable\ntransition rule to apply ( wlog always at most two rules are applicable).\nIf M runs in time O(p(|w|), then we can assume that u ∈ {0, 1}p(n) for all\nw ∈ {0, 1}n and that M reads in the i-th step the i-th bit of u. This yields:\nL L ∈ NP iff there exists a polynomial p and a p-time bounded DTM M s.t.\nL = {w ∈ {0, 1}∗ | M accepts w#u for some u ∈ {0, 1}p(|w|)}\nu is a certificate for w if M accepts (“verifies”) w#u.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Nondeterminism as deterministic verifier 192 (536)\nu0 u1 u2 u3 u4 u5 u6 u7\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\nM(w, ui)\n! For L ∈ NP:\n▷ If x ∈ L, then at least one ui is a certificate.\n▷ If x ̸∈ L, then no ui is a certificate.\n! For L ∈ coNP (i.e. L ∈ NP):\n▷ If x ∈ L, then all ui are certificates.\n▷ If x ̸∈ L, then at least one ui is not a certificate.\n▷ E.g. SAT ∈ NP and TAUT“=”UNSAT“=”SAT ∈ coNP.\nexistential (exists) vs. universal (for all) acceptance\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Polynomial-time reductions: motivation 193 (537)\n▷ The Cook-Levin theorem states that for any decision problem L ∈ NP there\nis a polynomial-time computable function ρL ∈ FP so that:\nw ∈ L iff ρL(w) ∈ SAT.\n▷ I.e. the function ρL translates a problem instance w for L in polynomial time\ninto a propositional formula ρL(w) so that ρL(w) hast at least one satisfying\nassignment iff w ∈ L.\n▷ E.g. if L is the Hamiltonian cycle problem, then w = ⌊G⌉ will be the binary\nencoding of a directed graph, and a satisfying assignment of ρL(⌊G⌉) will in\nfact already describe a Hamiltonian cycle in G.\n▷ I.e. any polynomial-time algorithm for deciding the satisfiability of a\npropositional formula (SAT) could be used as a subroutine to obtain a\npolynomial-time algorithm for any problem in NP,\nThis function ρL is called a polynomial-time (Karp) reduction from L to SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Polynomial-time reductions 194 (538)\nD For A ⊆ Σ∗\nA, B⊆ Σ∗\nB, we say that A is polynomial-time reducible to B\n(short: A ≤p B) if there exists a polynomial-time computable function\nf : Σ∗\nA → Σ∗\nB so that for all w ∈ Σ∗\nA we have\nw ∈ A iff f(w) ∈ B\n▷ As polynomials are closed wrt. addition, multiplication and composition:\nL If B ∈ P and A ≤p B, then also A ∈ P.\nIf B ∈ NP and A ≤p B, then also A ∈ NP.\nIf A ≤p B and B ≤p C, then A ≤p C.\nT Every L ∈ PSPACE is polynomial-time reducible to the word problem for\ncontext-sensitive grammars.\n▷ If L ∈ PSPACE, then there is polynomial p and some p-space bounded DTM\nM that decides L.\n▷ We can give M enough space by simply inflating the input w to\nw′ = □p(|w|)w□p(|w|), and adapting M so that it skips the initial blanks.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cook-Levin theorem 195 (539)\nT For L ∈ NP: L ≤p SAT := {⌊ϕ⌉ |ϕ is a satisfiable propositional formula }.\n(⌊ϕ⌉ is some fixed binary encoding, e.g. UTF8 or ASCII.)\n▷ Contributed to both Stephen Cook and Leonid Levin, but sometimes also only\nshort Cook’s theorem.\n▷ Main idea: As M is p-time-bounded, all of its heads can move at most p(|w|)\nsteps in total. Hence, for a given fixed w we can restrict all tapes to the cells\nat positions between −p(|w|) and p(|w|).\nI.e. every tape can be treated as a word of length 2p(|w|).\nWe can describe the successor relation − →M using a propositional formula of\nlength O(p(|w|)):\n▷ In principle just as in circuit design via binary encoding as truth tables.\n▷ The main point is that the change to a cell only depends on the current state,\nhead position/read symbol and the applied rule, i.e. on a constant number of\nvalues, so we need to encode 2p(|w|) independent relations of constant size.\n(This is also what was used in the encoding of a TM as PCP instance.)",
      "head position/read symbol and the applied rule, i.e. on a constant number of\nvalues, so we need to encode 2p(|w|) independent relations of constant size.\n(This is also what was used in the encoding of a TM as PCP instance.)\nAll possible runs on w can thus be described as the conjunction of p(|w|) steps\nyielding a formula of length O(p(|w|)2).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cook-Levin theorem 195 (540)\nT For L ∈ NP: L ≤p SAT := {⌊ϕ⌉ |ϕ is a satisfiable propositional formula }.\n(⌊ϕ⌉ is some fixed binary encoding, e.g. UTF8 or ASCII.)\n▷ Let p be a polynomial and M a p-time-bounded NTM s.t. L = L(M).\nwlog we may assume that M is a 1TM.\nWe use the following propositional variables\n▷ Qt,q: “At time t, M is in state q”\n▷ Ht,i: “At time t, the head is at position i”\n▷ Tt,i,a: “At time t, the content of the i-the tape cell is the symbol a”\n▷ Rt,r: “At time t, the rule r ∈ δ is applied”\nfor t ∈ {0, 1, . . . , p(|w|)}, i ∈ {−p(|w|), . . . , p(|w|)}, q ∈ Q, a ∈ Γ, r ∈ δ.\n▷ As auxiliary means we need the formula of the lenght O(m2):\nEO(x1, . . . , xm) =\n m_\ni=1\nxi\n!\n∧\n^\n1≤i<j≤m\n(¬xi ∨ ¬xj)\nEO(x1, . . . , xm) is true iff exactly one of the variables x1, . . . , xk is true.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cook-Levin theorem 195 (541)\nT For L ∈ NP: L ≤p SAT := {⌊ϕ⌉ |ϕ is a satisfiable propositional formula }.\n(⌊ϕ⌉ is some fixed binary encoding, e.g. UTF8 or ASCII.)\n▷ Using EO we can specify that at time t the TM should be in exactly one state,\nits head at exactly one position, in every cell exactly one symbol, exactly one\nrule must be applied:\nFEO,t := EO(Qt,q0 , . . . , Qt,qk )\n∧ EO(Ht,−p(|w|), . . . , Ht,p(|w|))\n∧ EO(Rt,r1 , . . . , Rt,rm)\n∧\n^\ni\nEO(Tt,i,a1 , . . . , Tt,i,al)\nfor fixed enumerations Q = {q0, . . . , qk}, Γ = {a1, . . . , al}, δ = {r1, . . . , rm}.\nNote that |FEO,t| ∈ O(p(|w|)).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cook-Levin theorem 195 (542)\nT For L ∈ NP: L ≤p SAT := {⌊ϕ⌉ |ϕ is a satisfiable propositional formula }.\n(⌊ϕ⌉ is some fixed binary encoding, e.g. UTF8 or ASCII.)\n▷ We then specify that, if Rt,r is true, then r ∈ δ has to be applicable, and how\nits application changes the state and tape content:\nFδ,t :=\n^\ni,a\n\u0010\n(¬Ht,i ∧ Tt,i,a) → Tt+1,i,a\n\u0011\n∧\n^\ni,r=(p,a,q,r,d)∈δ\n\u0010\n(Ht,i ∧ Rt,r) → (Qt,p ∧ Tt,i,a ∧ Qt+1,q ∧ Tt+1,i,b ∧ Ht,i+d)\n\u0011\nNote that |Fδ,t| ∈ O(p(|w|)).\nIn conjunction with FEO,t exactly one Rt,r will be true and thus enforce the\ncorrection application of the rule r = (p, a, q, r, d) (if it is applicable).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cook-Levin theorem 195 (543)\nT For L ∈ NP: L ≤p SAT := {⌊ϕ⌉ |ϕ is a satisfiable propositional formula }.\n(⌊ϕ⌉ is some fixed binary encoding, e.g. UTF8 or ASCII.)\n▷ Next we specify the initial configuration for the given w = w1 . . . wn:\nFinit := Q0,q0 ∧ H0,0 ∧\nn^\ni=1\nT0,i,wi ∧\n^\n|i|≤p(|w|),i̸∈[n]\nT0,i,□\nNote that |Finit| ∈ O(p(|w|)).\n▷ And we specify that we want the computation to accept w:\nFacc :=\n_\nqf ∈F\nQp(|w|),qf\nwlog we can assume that M always does exactly p(|w|) steps e.g. by forcing\nM to count till p(|w|) (in unary or binary).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Cook-Levin theorem 195 (544)\nT For L ∈ NP: L ≤p SAT := {⌊ϕ⌉ |ϕ is a satisfiable propositional formula }.\n(⌊ϕ⌉ is some fixed binary encoding, e.g. UTF8 or ASCII.)\n▷ All possible accepting runs of M on w are then described by the formula\nρL(w) := Finit ∧ Facc ∧\np(|w|)−1^\nt=0\nFδ,t ∧\np(|w|)^\nt=0\nFEO,t\nwhich is of length O(p(|w|2).\n! By construction, the accepting runs of M on w are then in bijection with the\nsatisfying assignments to ρL(w) by means of the truth values of the Rt,r.\nIn particular, given a satisfying assignment, we can recover from Rt,r the\naccepting run of M on w in linear time; i.e. SAT is used to “guess” via Rt,r\nthe rules used in an accepting run.\nIf M is deterministic, we can simply hard-code the uniquely determined values\nfor the Rt,r; then above construction essentially says that for every L ∈ P and\nevery fixed input length n we can compute a polynomial-sized circuit C with n\ninputs so that: C(w1, . . . , wn) = 1 iff w ∈ L.",
      "If M is deterministic, we can simply hard-code the uniquely determined values\nfor the Rt,r; then above construction essentially says that for every L ∈ P and\nevery fixed input length n we can compute a polynomial-sized circuit C with n\ninputs so that: C(w1, . . . , wn) = 1 iff w ∈ L.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Oracles (*) 196 (545)\nD Let O be any concrete Turing machine called the oracle. Then MO denotes a\npolynomial-time deterministic (multi-tape) TM M with additional oracle\naccess to O.\n▷ M has an additional query tape for calling O in order to obtain O(x):\nDuring any time of a computation, M may also write some “query” x to its\nquery tape (e.g. “does the Turing machine Mw encoded by w halt on w?”).\nM then may call O on x (by changing into a specific state) and within a single\ntime step, the content of the query tape will be overwritten by the output\nO(x) of O. (writing the argument x and reading the actual ouput O(x) still take\ntime |x| resp. |O(x)|)\n▷ Otherwise, MO behaves like a normal deterministic polynomial-time TM.\nIn particular, there is a polynomial p so that MO halts after at most p(|w|)\nsteps for every input w.\n▷ For example, we can combine any polynomial time reduction from some\nL ∈ NP to SAT with a SAT solver.",
      "time |x| resp. |O(x)|)\n▷ Otherwise, MO behaves like a normal deterministic polynomial-time TM.\nIn particular, there is a polynomial p so that MO halts after at most p(|w|)\nsteps for every input w.\n▷ For example, we can combine any polynomial time reduction from some\nL ∈ NP to SAT with a SAT solver.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Oracles (*) 196 (546)\n▷ As we do not know if a concrete oracle O is reliable, we usually expect O to\nnot only return “yes” or “no”, but also some sort of certificate (proof,\ncomputation) that can be verified by M:\nD A language L is an element of POracle if there exists a deterministic\npolynomial-time TM M s.t. both:\n▷ there exists at least one oracle O s.t. L(MO) = L, and\n▷ for every possible oracle O′ we have L(MO′\n) ⊆ L.\nI.e. M is a “correct proof checker” that cannot be tricked by any O′ into\naccepting a word w ̸∈ L as M will detect an erroneous certificate.\nL NP = POracle\n▷ NP = PSAT by virtue of Cook-Levin.\n▷ If L ∈ POracle, then turn the oracle TM M into a NTM M′ that guesses the\n(necessarily polynomial) output of the oracle.\n! I.e. as M has to be deterministic and run in polynomial time, any oracle more\npowerful than a SAT-solver is useless to it.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Oracles (*) 196 (547)\n▷ In an interactive proof system M must still be “deterministic” (up to\nrandomization) and polynomial time, but it may further throw a fair coin.\nD A language L is an element of IP if there exists a randomized\npolynomial-time TM M s.t. both:\n▷ there is one oracle O s.t. MO accepts every w ∈ L with prob. ≥ 2/3, and\n▷ for every oracle O′ the prob. that MO′\naccepts a word w ̸∈ L is ≤ 1/3.\nT PSPACE = IP (see e.g. Papadimitriou) .\n▷ Extensions/applications:\n▷ Zero-knowledge proofs (ZKP).\n▷ multi prover interactive proofs (MIP): M has access to two (or more)\nindependent, strictly separated orcales; M can now e.g. use one oracle to\ncheck the answers of the other oracle; MIP = NEXPTIME.\n▷ probabilistically checkable proofs (PCP): How much randomization, how much\ninteraction is required to decide certain problems?\nPCP theorem (G¨odelpreis 2001): O(log n) bits of randomness and O(1) bits of\nthe proof from the oracle suffice for NP.",
      "▷ probabilistically checkable proofs (PCP): How much randomization, how much\ninteraction is required to decide certain problems?\nPCP theorem (G¨odelpreis 2001): O(log n) bits of randomness and O(1) bits of\nthe proof from the oracle suffice for NP.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "7 Complexity theory: deterministic vs. nondeterministic polynomial time\nResource-bounded computation\nPolynomially bounded computation\nP vs NP\nNP-complete problems\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Hardness and completeness 198 (549)\nD A language L is X-hard if L′ ≤p L for all L′ ∈ X.\nA language L is X-complete if L ∈ X and L is X-hard.\nT Cook-Levin: SAT is NP-complete.\nT The word problem for context-sensitive grammars is NP-hard and\nPSPACE-complete.\n▷ As discussed before by means of inflating the input in polynomial time.\n▷ As P\n?\n= PSPACE is still open, the world problem for context-sensitive\ngrammars might be even NP-complete.\n▷ Should someone be able to show the word problem for context-sensitive\ngrammars can be decided in polynomial time, then P = NP = PSPACE.\n▷ Should someone be able to show that SAT ∈ P, then P = NP.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Hardness and completeness 198 (550)\nD A language L is X-hard if L′ ≤p L for all L′ ∈ X.\nA language L is X-complete if L ∈ X and L is X-hard.\nT Cook-Levin: SAT is NP-complete.\n! The X-complete problems can be consider the “hardest” problems within X:\n▷ E.g. every L ∈ NP with SAT ≤p L is itself NP-complete.\nA polynomial-time algorithm for L would also yield a polynomial-time\nalgorithm for SAT and thus any other decision problem in NP.\n▷ If we can show that L is NP-complete, it is deemed very unlikely that we will\nfind a polynomial-time algorithm for L; instead we should consider a restricted\nsubset of L.\nC If SAT ∈ P, then P = NP.\nC SAT =p UNSAT =p TAUT is coNP-complete. (A =p B short for A ≤p B ≤p A.)\nC If SAT ∈ coNP, then NP = coNP.\nT Ladner’s theorem (w/o proof): If P ̸= NP, then the NP-complete problems\nare a strict subset of NP.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "NP-complete problems 199 (551)\n▷ E.g. the following problems are known to be NP-complete:\n▷ SAT: given a propositional formula F decide if F is satisfiable.\n▷ 3SAT: given a clause set F where every clause contains at most three literals\ndecide if F is satisfiable.\n▷ 3COL: given a finite graph G = (V, E) decide if there exists a node coloring\nχ: V → [3] using at most 3 colors.\n▷ Hamiltonian cycle problem: given a finite graph G = (V, E) decide if G is\nhamiltonian, i.e. if there is a simple cycle that visits every node of G exactly\nonce.\n▷ Integer linear programming (ILP): given A ∈ Z m×n, b∈ Z n, c∈ Z m, γ∈ Z\ndecide if there is some x ∈ Z n with x ≥ 0, Ax ≤ b and c⊤x ≥ γ.\n▷ Subgraph isomorphism: given G1 = (V1, E1) and G2 = (V2, E2) decide if there\nis some H1 ≤ G1 with H1 ∼= G2.\nbut e.g. factoring and graph isomorphism are not known to be NP-hard.\nFor both we know sub-exponential time algorithms in contrast to SAT.",
      "▷ Subgraph isomorphism: given G1 = (V1, E1) and G2 = (V2, E2) decide if there\nis some H1 ≤ G1 with H1 ∼= G2.\nbut e.g. factoring and graph isomorphism are not known to be NP-hard.\nFor both we know sub-exponential time algorithms in contrast to SAT.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: construction of an equisatisfiable 3CNF 200 (552)\n∨qε :\n∧q0 : ∧q1 :\np1 p2 p3 p4\n▷ Intermediate step: Auxiliary variables “store” intermediate results.\nqε ∧ (qε ↔ (q0 ∨ q1))| {z }\n=:Gε\n∧(q0 ↔ (p1 ∧ p2))| {z }\n=:G0\n∧(q1 ↔ (p3 ∧ p4))| {z }\n=:G1\nD Clause: conjunction of literals W\ni Li.\n! Every formula with 3 variables has a 3CNF with at most ≤ 4 clauses:\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: construction of an equisatisfiable 3CNF 200 (553)\n∨qε :\n∧q0 : ∧q1 :\np1 p2 p3 p4\n▷ Replace Gε, G0, G1 by semantically equivalent 3CNF:\nEF := qε ∧ (¬qε ∨ q0 ∨ q1) ∧ (qε ∨ ¬q0) ∧ (qε ∨ ¬q1)\n∧ (q0 ∨ ¬p1 ∨ ¬p2) ∧ (¬q0 ∨ p1) ∧ (¬q0 ∨ p2)\n∧ (q1 ∨ ¬p3 ∨ ¬p4) ∧ (¬q1 ∨ p3) ∧ (¬q1 ∨ p4)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: construction of an equisatisfiable 3CNF 200 (554)\n∨qε :\n∧q0 : ∧q1 :\np1 p2 p3 p4\n▷ Replace Gε, G0, G1 by semantically equivalent 3CNF:\nEF := qε ∧ (¬qε ∨ q0 ∨ q1) ∧ (qε ∨ ¬q0) ∧ (qε ∨ ¬q1)\n∧ (q0 ∨ ¬p1 ∨ ¬p2) ∧ (¬q0 ∨ p1) ∧ (¬q0 ∨ p2)\n∧ (q1 ∨ ¬p3 ∨ ¬p4) ∧ (¬q1 ∨ p3) ∧ (¬q1 ∨ p4)\n▷ Every satisfying EF -assignment is also satisfying for F.\neg β(p1) = β(p2) = β(p3) = 1, β(p4) = 0, β(q0) = 1, β(q1) = 0, β(qew) = 1.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: construction of an equisatisfiable 3CNF 200 (555)\n∨qε :\n∧q0 : ∧q1 :\np1 p2 p3 p4\n▷ Replace Gε, G0, G1 by semantically equivalent 3CNF:\nEF := qε ∧ (¬qε ∨ q0 ∨ q1) ∧ (qε ∨ ¬q0) ∧ (qε ∨ ¬q1)\n∧ (q0 ∨ ¬p1 ∨ ¬p2) ∧ (¬q0 ∨ p1) ∧ (¬q0 ∨ p2)\n∧ (q1 ∨ ¬p3 ∨ ¬p4) ∧ (¬q1 ∨ p3) ∧ (¬q1 ∨ p4)\n▷ Every satisfying F-assignment can be adapted to a sat. EF -assignment\neg β(p1) = β(p2) = β(p3) = 1, β(p4) = 0 and β(q0) = 1, β(q1) = 0, β(qε) = 1.\nbut also to a unsatisfying EF -assignment:\neg β(p1) = β(p2) = β(p3) = 1, β(p4) = 0 and β(q0) = 1, β(q1) = 0, β(qε) = 0.\nThus F ≡e EF but F ̸≡ EF !\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Reminder: construction of an equisatisfiable 3CNF 200 (556)\n∨qε :\n∧q0 : ∧q1 :\np1 p2 p3 p4\n▷ Replace Gε, G0, G1 by semantically equivalent 3CNF:\nEF := qε ∧ (¬qε ∨ q0 ∨ q1) ∧ (qε ∨ ¬q0) ∧ (qε ∨ ¬q1)\n∧ (q0 ∨ ¬p1 ∨ ¬p2) ∧ (¬q0 ∨ p1) ∧ (¬q0 ∨ p2)\n∧ (q1 ∨ ¬p3 ∨ ¬p4) ∧ (¬q1 ∨ p3) ∧ (¬q1 ∨ p4)\n! The computation of the equisatisfiable 3CNF is polynomial in the length of\nthe original formula: the syntax tree can be computed using CYK (or even in\nlinear time using a deterministic grammar), after that a constant amount of\nwork is done for each inner node.\nC 3SAT := {⌊ϕ⌉ |ϕ is a propositional formula in 3CNF } is NP-complete.\nL 2SAT := {⌊ϕ⌉ |ϕ is a propositional formula in 2CNF } ∈P (see e.g. here).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "More NP-complete problems 201 (557)\n▷ The restricted form of 3CNF makes it easier to encode a formula F in 3CNF\nin time polynomial in |F| into a graph GF so that\nF is satisfiable iff the graph GF has e.g.\n▷ a node coloring with 3 colors, and a 3-coloring\n▷ a Hamiltonian cycle, and a Hamiltonian cycle\n▷ a subgraph isomorphic with Kl (a l-clique), and a l-clique\nencodes a satisfying assignment, i.e. all of these problems are also NP-hard.\n▷ In particular, the last means that not only subgraph isomorphism, but the\nrestricted question whether there is a clique is already NP-hard.\n▷ All these problems can also easily described in propositional logic in time\npolynomial in the size of the graph, i.e. all problems are NP-complete.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "More NP-complete problems 201 (558)\n▷ The restricted form of 3CNF makes it easier to encode a formula F in 3CNF\nin time polynomial in |F| into a graph GF so that\nF is satisfiable iff the graph GF has e.g.\n▷ a node coloring with 3 colors, and a 3-coloring\n▷ a Hamiltonian cycle, and a Hamiltonian cycle\n▷ a subgraph isomorphic with Kl (a l-clique), and a l-clique\nencodes a satisfying assignment, i.e. all of these problems are also NP-hard.\n▷ We have a brief look at the basic ideas of these reductions.\n▷ For more examples, see e.g.:\n▷ Karp’s 21 NP-complete problems.\n▷ List of NP-complete problems on wikipedia.\n▷ But “anything” that can encode somehow a graph and a path (=run of a TM)\ncan be turned into a NP-complete problem e.g. including Tetris, Sokoban,\nZelda, Super Mario (Kart), . . . (of course all “TM”), see e.g. here.\n▷ “computation = reachability in a compressed graph given by rewrite rules”",
      "▷ But “anything” that can encode somehow a graph and a path (=run of a TM)\ncan be turned into a NP-complete problem e.g. including Tetris, Sokoban,\nZelda, Super Mario (Kart), . . . (of course all “TM”), see e.g. here.\n▷ “computation = reachability in a compressed graph given by rewrite rules”\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "3-Colorability: CHECK ME 202 (559)\na ¬a b ¬b c ¬c d ¬d\nL1,1\nL1,2 L1,3\nL2,1\nL2,2 L2,3\nL3,1\nL3,2 L3,3\nL4,1\nL4,2 L4,3\n▷ The triangles at the top with the common center node require 3 colors.\n▷ wlog the common node has to be colored blue.\n▷ wlog the single node of degree 1 is colored green (not needed actually).\n▷ wlog the remaining nodes are colored either green (true) or red (false).\n▷ Exactly one of the two nodes representing literals of the same variable can be\ncolored green.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "3-Colorability: CHECK ME 202 (560)\na ¬a b ¬b c ¬c d ¬d\nL1,1\nL1,2 L1,3\nL2,1\nL2,2 L2,3\nL3,1\nL3,2 L3,3\nL4,1\nL4,2 L4,3\n▷ Each house encodes a clause, e.g. here the first clause is ¬a ∨ ¬b ∨ c.\n▷ The nodes of the house are connected to the top as shown:\n▷ The center of each house has to be colored red; its neighbors green or blue.\n▷ The node Li,j can only be colored blue or the negated truth value of the\nliteral it is connected to.\n▷ In particular, if all literal nodes a house is connected to are colored red (false),\nthe nodes Li,1, Li,2, Li,3 can only be colored green or blue.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "3-Colorability: CHECK ME 202 (561)\na ¬a b ¬b c ¬c d ¬d\nL1,1\nL1,2 L1,3\nL2,1\nL2,2 L2,3\nL3,1\nL3,2 L3,3\nL4,1\nL4,2 L4,3\n▷ Observe, that each house without its center is a simple cycle of length 5,\ni.e. we need at least three colors.\n▷ The neighbors of the center can only be colored green or blue.\n▷ Thus: a house is 3-colorable iff\nat least one literal it is connected to is colored green (true).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "3-Colorability: CHECK ME 202 (562)\na ¬a b ¬b c ¬c d ¬d\nL1,1\nL1,2 L1,3\nL2,1\nL2,2 L2,3\nL3,1\nL3,2 L3,3\nL4,1\nL4,2 L4,3\n▷ Determine the underlying formula and complete the node coloring.\n▷ Remark:\nThe additional green node in the middle is actually not required, but\nsimplifies the argument.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Hamiltonian Cycle Problem 203 (563)\ni1\ni2\ni3\no1\no2\no3\n▷ This graph is has no Hamiltonian cycle as we cannot go from right to left.\n▷ But it has a Hamiltonian path: i1, i2, i3, o3, o2, o1.\n▷ In particular, there are exactly three Hamitonian paths which are defined by\nthe “bridge” they use to go from left to right.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Hamiltonian Cycle Problem 203 (564)\ni1\ni2\ni3\no1\no2\no3\n▷ Each clause will be represented by a copy of the graph on the right:\n▷ E.g. consider ¬a ∨ ¬b ∨ c.\n▷ Then i1 and o1 represent ¬a.\n▷ Then i2 and o2 represent ¬b.\n▷ Then i3 and o3 represent c.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Hamiltonian Cycle Problem 203 (565)\na\nb\nc\nd\na\nb\nc\nd\nia\ni¬b\nic\noa\no¬b\noc\ni¬a\ni¬c\nid\no¬a\no¬c\nod\nia\nib\ni¬d\noa\nob\no¬d\nib\ni¬c\nid\nob\no¬c\nod\n▷ The graphs will be connected in series with the nodes labeled ik as “inputs”,\nand the nodes labeled by ok as “outputs”.\n▷ The nodes labeled a, b, c, drepresent the variables, and have to be identified.\n▷ From every variable x node on the left, we start two paths, one for each truth\nvalue. Both paths will end in the “next” variable so that we cycle through all\nvariables, e.g. in the order a, b, c, d, a.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Hamiltonian Cycle Problem 203 (566)\na\nb\nc\nd\na\nb\nc\nd\nia\ni¬b\nic\noa\no¬b\noc\ni¬a\ni¬c\nid\no¬a\no¬c\nod\nia\nib\ni¬d\noa\nob\no¬d\nib\ni¬c\nid\nob\no¬c\nod\n▷ E.g. the graph with the paths for the variable a.\n▷ The important point is: a satisfying assignment gives rise to a “meta cycle”\nthat visits every “clause graph” at least once and at most three times.\n▷ By construction, each “clause graph” has exactly three Hamiltonian paths that\ncan be used to extend the “meta cycle” into an Hamiltonian cycle.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Hamiltonian Cycle Problem 203 (567)\na\nb\nc\nd\nia\ni¬b\nic\noa\no¬b\noc\ni¬a\ni¬c\nid\no¬a\no¬c\nod\nia\nib\ni¬d\noa\nob\no¬d\nib\ni¬c\nid\nob\no¬c\nod\n▷ The final graph. The graph can also be made undirected. See e.g. Sch ¨oning\nfor both proofs.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Clique/subgraph isomorphism 204 (568)\nx ¬y z\nx\ny\nz ¬x\n¬y\n¬z\n▷ Here, the encoding is straight-forward:\n▷ Every clause Li,1 ∨ Li,2 ∨ Li,3 gives rise to three nodes Li,1, Li,2, Li,3.\n▷ There is an edge between Li,j and Lk,l iff\ni ̸= k (i.e. different clauses) and Li,j ∨ Lk,l is satisfiable (i.e. Li,j ̸= Lk,l).\n▷ If F has l clauses, a satisfying assignment thus gives rise to subgraph that is\nisomorphic to Kl (=l-clique).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Integer linear programming 205 (569)\nA B\nE\nC\nD\n\n\n1 0\n2 1\n−1 1\n−1 0\n0 −1\n\n\nx ≤\n\n\n4\n11\n5\n0\n0\n\n\n▷ Linear program (LP) in normal form: Given A ∈ R m×n, b∈ R m, c∈ R n\ndetermine some x ∈ R n s.t. x ≥ 0, Ax ≤ b and c⊤x is maximized.\n▷ As we need to be able to represent A, b, con the computer, we usually have\nthat A, b, care rational and thus in fact integers.\n▷ Any x ∈ R n satisfying Ax ≤ b and x ≥ 0 is called a feasible solution.\n▷ “Ax ≤ b and x ≥ 0” describes a convex region whose vertices are the\nintersection of some of the contraints, and thus, in practice also rational.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Integer linear programming 205 (570)\nA B\nE\nC\nD\n\n\n1 0\n2 1\n−1 1\n−1 0\n0 −1\n\n\nx ≤\n\n\n4\n11\n5\n0\n0\n\n\n▷ Linear program (LP) in normal form: Given A ∈ R m×n, b∈ R m, c∈ R n\ndetermine some x ∈ R n s.t. x ≥ 0, Ax ≤ b and c⊤x is maximized.\n▷ If the maximum of c⊤x exists, it will be a vertex of “ Ax ≤ b and x ≥ 0”.\n▷ If we instead ask whether for a given z ∈ R there is some solution s.t.\nc⊤x ≥ z, then we intersect “ Ax ≤ b and x ≥ 0” with “ c⊤x ≥ z” and can use\nbinary search to find an optimal solution.\nSee also branch-and-bound (essentially quantitative DPLL) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Integer linear programming 205 (571)\nA B\nE\nC\nD\n\n\n1 0\n2 1\n−1 1\n−1 0\n0 −1\n\n\nx ≤\n\n\n4\n11\n5\n0\n0\n\n\n▷ We actually know how to solve an LP in polynomial time using so-called\ninterior-point methods.\n▷ But in practice often the simplex method is used which (depending on the\npivot heuristic) is known to run in PSPACE/exponential time in the\nworst-case.\n▷ Yet, simplex is a very robust algorithm that “on-average” runs in\npolynomial-time, see smoothed analysis.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Integer linear programming 205 (572)\nA B\nE\nC\nD\n\n\n1 0\n2 1\n−1 1\n−1 0\n0 −1\n\n\nx ≤\n\n\n4\n11\n5\n0\n0\n\n\n▷ In an integer linear program (ILP) we only allow integer solutions which\nmakes the problem harder in general.\n▷ Remark: Matiyasevich’s theorem essentially states that we can encode a DTM\nM into a polynomial p ∈ Z [X0, X1, . . . , Xk] so that n ∈ L(M) iff\np(n, x1, . . . , xk) = 0 has an integer solution; i.e. Peano arithmetic is\nundecidable. But if we also consider real solutions (existential theory of the\nreals) the problem is in PSPACE.\n(Also Presburger arithmetic ( Z but only multiplication with constants) is decidable.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Integer linear programming 205 (573)\nA B\nE\nC\nD\n\n\n1 0\n2 1\n−1 1\n−1 0\n0 −1\n\n\nx ≤\n\n\n4\n11\n5\n0\n0\n\n\n▷ It is straight-forward to encode a 3CNF formula into an “0-1-ILP” instance:\n▷ For each propositional variable pi add two ILP variables xi and yi\n▷ Then require 0 ≤ xi ≤ 1, 0 ≤ yi ≤ 1, and xi + yi = 1.\nI.e. yi = 1 − xi ∈ {0, 1}.\n▷ A clause {pi, ¬pj, pk} yields the constraint xi + yj + xk = 1.\n▷ “a⊤x = b” can be replaced by “ a⊤x ≤ b, −a⊤x ≤ −b”.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Integer linear programming 205 (574)\nA B\nE\nC\nD\n\n\n1 0\n2 1\n−1 1\n−1 0\n0 −1\n\n\nx ≤\n\n\n4\n11\n5\n0\n0\n\n\n▷ The difficult part is to show that there is always a polynomial-sized integer\nsolution if there is some integer solution at all.\n▷ This can be done by using e.g. Cramer’s rule, see e.g. Papadimitriou.\n▷ Hence, if Ax ≤ b has an integer solution, it also has an integer solution of\npolynomial length which we can guess and validate in NP.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Remark: optimization vs. decision 206 (575)\n▷ Recall that f : {0, 1}∗ → {0, 1}∗ is computable in deterministic polynomial\ntime iff we can e.g. decide for input x in polynomial time whether the i-th\nbit of f(x) is 1.\n▷ E.g. computing an satisfying assignment for a given propositional formula can\nbe reduced to deciding if some propositional formula is satisfiable.\n▷ A special case is the question of computing some optimum, e.g.:\n▷ If we can decide for a given graph G = (V, E) and k ∈ N in polynomial time,\nif it has a node coloring using at most k colors (“kCOL”), then we can also\ncompute its chromatic number i.e. the minimal k for which a node coloring\nexits by means of binary search on {0, 1, . . . ,|V |}.\n▷ If we can decide for a given graph G = (V, E) and k ∈ N in polynomial time,\nif it has a clique of size at least k (“kCOL”), then we can also compute the\nmaximal k for which G contains a k-clique again by means of binary on\n{0, 1, . . . ,|V |}.",
      "▷ If we can decide for a given graph G = (V, E) and k ∈ N in polynomial time,\nif it has a clique of size at least k (“kCOL”), then we can also compute the\nmaximal k for which G contains a k-clique again by means of binary on\n{0, 1, . . . ,|V |}.\n(Note that if we fix the value of k, i.e. k is not part of the input, then deciding\nwhether G has a k-clique can be done in polynomial time.)\n▷ Similarly, the computing the optimal solution of an ILP can reduced to\ndeciding if an ILP has a solution.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Remark: reductions in cryptography 207 (576)\n▷ In modern cryptography the security of cryptographic schemes (e.g. for\nencryption or digital signatures) is assessed by means of (probablisitc)\npolynomial-time reductions to long-standing open problems:\n▷ E.g. the security of the Rabin cryptosystems can be reduced to the problem of\nfinding a (non-trivial) factorization of a composite integer n.\n▷ Currently, we do not know how factorize an integer n in time polynomial in\nlog2 n.\n▷ In particular, it is conjectured that factorizing n = pq with p, qtwo distinct\nprime numbers with log2 p ≈ log2 q is a worst case – at least if p, qare chosen\nuniformly at random.\n▷ The reduction underlying a “proof of security” of a Rabin cryptosystems thus\nallows to turn an attack into a factorization algorithm.\n▷ As we conjecture that there is no (deterministic) efficient factorization\nalgorithm, we necessarily also have to conjecture that there is no efficient",
      "▷ The reduction underlying a “proof of security” of a Rabin cryptosystems thus\nallows to turn an attack into a factorization algorithm.\n▷ As we conjecture that there is no (deterministic) efficient factorization\nalgorithm, we necessarily also have to conjecture that there is no efficient\nattack on a Rabin cryptosystem. (But see Shor’s algorithm.)\n▷ One can show that cryptography requires the existence of one-way functions\nwhich can only exist if P ̸= coNP.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: Learning with errors 208 (577)\n▷ Generation of problem instances:\n▷ Input: Positive integers q, n∈ N , vector v ∈ Z n\nq , distribution ϕ on R\n▷ Output: Samples (u1, t1), . . . ,(ul, tl) ∈ Z n\nq × [0, q) with l polynomial wrt. q, n\nwhere each sample is obtained as follows:\n▷ Choose ui ∈ Z n\nq uniformly at random.\n▷ Choose ei ∈ R according to ϕ at random (error/pertubation).\n▷ Set ti := (⟨ui,v⟩2\nq + ei) mod 1 with x mod 1 := x − ⌊x⌋\n▷ Learning problem: Given q, n, ϕand samples (u1, t1), . . . ,(ul, tl) recover v.\n▷ Decision problem: Given q, n, ϕand samples (u1, t1), . . . ,(ul, tl) decide\nwhether the samples have been generated as described above or whether they\nwere generated uniformly at random.\n▷ Oded Regev: If q is a prime polynomial in n and ϕ is a Gaussian distribution,\nboth problems can be reduced to each other in prob. polynomial time.\n▷ Extension RLWE: GF(qn) instead of Z n\nq for q prime.\n▷ Used in some post-quantum cryptographic schemes.",
      "were generated uniformly at random.\n▷ Oded Regev: If q is a prime polynomial in n and ϕ is a Gaussian distribution,\nboth problems can be reduced to each other in prob. polynomial time.\n▷ Extension RLWE: GF(qn) instead of Z n\nq for q prime.\n▷ Used in some post-quantum cryptographic schemes.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "8 Basics of information theory: Shannon entropy, Huffman codes, compression\nPrefix codes and Huffman’s algorithm\nOptimal codes and Shannon’s entropy\nEntropy and information\nArithmetic codes\nData compression\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Uniquely decodable codes 211 (580)\nD A (binary) code for Σ is a homomorphism c: Σ∗ → {0, 1}∗.\n▷ I.e. c(ε) = ε and c(uv) = c(u)c(v) s.t. it suffices to define c(a) for all a ∈ Σ.\nc is said to uniquely decodable if c is injective.\nF Every code c with |c(Σ)| < |Σ| is not uniquely decodable.\nEvery code c with c(Σ≤n) ⊆ Σ<n for some n ∈ N is not uniquely decodable.\nD c is a prefix code if c(Σ) is prefix-free.\nc is a block code if c(Σ) ⊆ {0, 1}m for some m.\nF All prefix codes and all block codes are uniquely decodable if |c(Σ)| = |Σ|.\n▷ The PCP can be understood as the question if two codes c, c′ map the same\nword w to the same code word c(w) = c′(w).\nIn contrast to the PCP, we can decide whether a single code c is\ninjective/uniquely decodable – in fact in polynomial time.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Uniquely decodable codes 211 (581)\nD A (binary) code for Σ is a homomorphism c: Σ∗ → {0, 1}∗.\n▷ I.e. c(ε) = ε and c(uv) = c(u)c(v) s.t. it suffices to define c(a) for all a ∈ Σ.\nc is said to uniquely decodable if c is injective.\n▷ Remark:\nL Let c: Σ∗ → {0, 1}∗ be a code and set C := c(Σ). wlog |C| = |Σ|.\nS1 := {v | w = uv ∈ C, u∈ C} \\ {ε}\nSi+1 := {v | w = uv ∈ Si, u∈ C} ∪ {v | w = uv ∈ C, u∈ Si}\nc is uniquely decodable iff ε ̸∈ Si for all i ∈ N 0.\n▷ Informally, the Si consist of the possible differences between c(a1 . . . al) and\nc(a′\n1 . . . a′\nl′) depending on which of the two words is longer.\n▷ We can always assume that a1 ̸= a′\n1 and thus also c(a1) ̸= c(a′\n1).\n▷ S1 consists exactly of these initial differences; Si+1 is then obtained by either\nextending a1 . . . al or a′\n1 . . . a′\nl′.\n▷ As Si only consists of suffixes of C, eventually Si = Sj for some 0 ≤ i < j.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Prefix codes and Huffman’s algorithm 212 (582)\nε\n0 1\n00 01 10 11\n000 001 010 011 100 101 110 111\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\na1 a2 a3 a4 a5\n▷ Recall: any finite alphabet Σ can be encoded using binary alphabet.\n▷ The simplest approach is uniform encoding (“block code”):\n▷ Fix an enumeration Σ = {a1, . . . , ak}\n▷ Fix n s.t. k ≤ 2n.\n▷ Map ak e.g. to the n-bit MSBF representation of k − 1:\na1 7→ 0n−200 a2 7→ 0n−201 a3 7→ 0n−210 . . .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Prefix codes and Huffman’s algorithm 212 (583)\nε\n0 1\n00 01 10 11\n000 001 010 011 100 101 110 111\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\na1 a2 a3 a4 a5\n▷ In other words: We use the leaves {0, 1}n of the perfect binary tree of height\nn for representing a1, . . . , ak.\n▷ If k <2n we therefore waste leaves (=code words).\n▷ The better approach is to prune the tree down to exactly k leaves.\n▷ Of which there are exactly Ck−1 = 1\nk\n\u00002(k−1)\nk−1\n\u0001\n.\n(k − 1 is the number of inner nodes.)\nSee DS/Catalan numbers or prove the recursion Ck+1 = Pk\nl=0 ClCk−l.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Prefix codes and Huffman’s algorithm 212 (584)\nε\n0 1\n00 01\n000 001 010 011\n0 1\n0 1\n0 1 0 1\na1 a4 a5 a3\na2\nε\n0 1\n00 01 10 11\n100 101\n0 1\n0 1 0 1\n0 1\na1\na2\na3\na4a5\n▷ In other words: We use the leaves {0, 1}n of the perfect binary tree of height\nn for representing a1, . . . , ak.\n▷ If k <2n we therefore waste leaves (=code words).\n▷ The better approach is to prune the tree down to exactly k leaves.\n▷ Of which there are exactly Ck−1 = 1\nk\n\u00002(k−1)\nk−1\n\u0001\n.\n(k − 1 is the number of inner nodes.)\nSee DS/Catalan numbers or prove the recursion Ck+1 = Pk\nl=0 ClCk−l.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Prefix codes and Huffman’s algorithm 212 (585)\nε\n0 1\n00 01\n000 001 010 011\n0 1\n0 1\n0 1 0 1\na1 a4 a5 a3\na2\nε\n0 1\n00 01 10 11\n100 101\n0 1\n0 1 0 1\n0 1\na1\na2\na3\na4a5\n▷ Every finite binary tree corresponds to a finite prefix-closed subset of {0, 1}∗,\n▷ Every inner node u has exactly two children u0 and u1.\nand its leaves give rise to prefix (free) code which can be used as alphabet.\n▷ The tree gives rise to a DFA/finite state transducer for decoding:\n▷ MyHill-Nerode: The tree yields a minimal DFA with initial state ε and finial\nstates the leaves/code words wi.\n▷ For the transducer: add ε-transitions from the leaves/code words wi to the\nroot that emit the corresponing symbol ai.\n(Contract the ε-transitions e.g. by skipping the leaves.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Prefix codes and Huffman’s algorithm 212 (586)\nε\n0 1\n00 01\n000 001 010 011\n0 1\n0 1\n0 1 0 1\nE(13)\nT(9)\nA(8)O(7) I(7)\n(114) ε\n0 1\n00 01 10 11\n100 101\n0 1\n0 1 0 1\n0 1\nE(13)\nT(9)\nA(8)\nO(7)I(7)\n(109)\n▷ Often, each symbol ai will be assigned some weight γi e.g. its absolute or\nrelative frequency.\n▷ Already single letter frequencies can be used e.g. to break simple\ncryptographic schemes (cf. frequency analysis).\n▷ See e.g. letter frequencies on wikipedia\n▷ Remember, the concept of “alphabet” is quite general: the ai can be lexemes,\nwords, vectors, tuples, n-grams themselves which can be used to model some\ncontext-dependent frequencies.\nI.e. instead of Σ = {A, B, . . . ,Z} you can also use Σ2 = {AA, AB, . . . ,ZZ} as\n“source alphabet” which can help to reduce the average/expected code word\nlength.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Prefix codes and Huffman’s algorithm 212 (587)\nε\n0 1\n00 01\n000 001 010 011\n0 1\n0 1\n0 1 0 1\nE(13)\nT(9)\nA(8)O(7) I(7)\n(114) ε\n0 1\n00 01 10 11\n100 101\n0 1\n0 1 0 1\n0 1\nE(13)\nT(9)\nA(8)\nO(7)I(7)\n(109)\nD Assume with every symbol ai a positive weight γi ∈ R >0 is associated.\n▷ Often, relative frequencies or probabilities are used: γi 7→ γi\nγ1+...+γk\nLet wi be the leaf/code word assigned to ai. Then\nkX\ni=1\nγi|wi|\nis the weighted code (word) length resp. weighted (tree) height.\nIf Pk\ni=1 γi = 1, then this is the expected/average code (word) length resp.\nexpected/average (tree) height.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Prefix codes and Huffman’s algorithm 212 (588)\nε\n0 1\n00 01 10 11\n100 101\n0 1\n0 1 0 1\n0 1\nE(13)\nT(9)\nA(8)\nO(7)I(7)\n(109) ε\n0 1\n00 01 10 11\n0 1\n0 1 0 1\nT(9) [AE](21) O(7)I(7)\n(88)\n▷ The question is thus how to determine a tree of minimal weighted height:\n▷ Consider a minimal tree for given weights γ1 ≥ . . .≥ γk.\nwlog we may assume that γk−1 and γk are assigned to two leaves wk−1 = u0\nand wk = u1 with a common parent u.\nTreat ak−1 and ak as a single symbol [ak−1, ak] encoded by u.\nThe weighted height of the reduced tree is then:\nX\ni∈[k]\n|wi|γi − (|u0|γk−1 + |u1|γk) +|u|(γk−1 + γk) =\nX\ni∈[k]\n|wi|γi − (γk−1 + γk)\nAs the original tree is minimal, also the reduced tree has to be minimal as\nsplitting u again into u0 and u1 increases the weight always by γk−1 + γk.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Prefix codes and Huffman’s algorithm 212 (589)\nε\n0 1\n00 01 10 11\n100 101\n0 1\n0 1 0 1\n0 1\nE(13)\nT(9)\nA(8)\nO(7)I(7)\n(109)\nE(13) T(9) A(8) I(7) O(7)\n[IO](14) E(13) T(9) A(8)\n[AT](17) [IO](14) E(13)\n[[IO]E](27) [AT](17)\n[[[IO]E][AT]](44)\nD This leads to the algorithm by Huffman:\n▷ Sort the symbols a1, . . . , ak−1, ak s.t. γ1 ≥ . . .≥ γk−1 ≥ γk.\n▷ If k ≥ 2, reduce the symbols to a1, . . . , ak−2, [ak−1, ak] with [ak−1, ak] a new\nsymbol of weight γk−1 + γk and proceed recursively.\n▷ If k = 1, the nesting of the brackets (Dyck word) of the final single combined\nsymbol encodes the binary tree of the prefix code ( wlog initially k ≥ 2).\n▷ Not required, but useful: I.e. extend the weights so that the second component\nremembers the height of the resulting subtree: γi 7→ (γi, 0) with\n(γ, h) + (γ′, h′) := (γ + γ′, max(h, h′) + 1) and “≤” now the lexico. order.\nThe resulting code is called a Huffman code.\nF The Huffman algorithm yields a binary tree of minimal weighted height.",
      "remembers the height of the resulting subtree: γi 7→ (γi, 0) with\n(γ, h) + (γ′, h′) := (γ + γ′, max(h, h′) + 1) and “≤” now the lexico. order.\nThe resulting code is called a Huffman code.\nF The Huffman algorithm yields a binary tree of minimal weighted height.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Prefix codes and Huffman’s algorithm 212 (590)\nε\n0 1\n00 01 10 11\n100 101\n0 1\n0 1 0 1\n0 1\nE(13)\nT(9)\nA(8)\nO(7)I(7)\n(109) ε\n0 1\n00 01 10 11\n000 001\n0 1\n0 1 0 1\n0 1 E(13) A(8) T(9)\nO(7)I(7)\n(102)\nD This leads to the algorithm by Huffman:\n▷ Sort the symbols a1, . . . , ak−1, ak s.t. γ1 ≥ . . .≥ γk−1 ≥ γk.\n▷ If k ≥ 2, reduce the symbols to a1, . . . , ak−2, [ak−1, ak] with [ak−1, ak] a new\nsymbol of weight γk−1 + γk and proceed recursively.\n▷ If k = 1, the nesting of the brackets (Dyck word) of the final single combined\nsymbol encodes the binary tree of the prefix code ( wlog initially k ≥ 2).\n▷ Not required, but useful: I.e. extend the weights so that the second component\nremembers the height of the resulting subtree: γi 7→ (γi, 0) with\n(γ, h) + (γ′, h′) := (γ + γ′, max(h, h′) + 1) and “≤” now the lexico. order.\nThe resulting code is called a Huffman code.\nF The Huffman algorithm yields a binary tree of minimal weighted height.",
      "remembers the height of the resulting subtree: γi 7→ (γi, 0) with\n(γ, h) + (γ′, h′) := (γ + γ′, max(h, h′) + 1) and “≤” now the lexico. order.\nThe resulting code is called a Huffman code.\nF The Huffman algorithm yields a binary tree of minimal weighted height.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Huffman codes: examples 213 (591)\n▷ The Huffman algorithm is nondeterministic in the choice of the least weights;\nthus, in general there will be several non-isomorphic solutions, e.g.:\nA(2) B(1) C(1) D(1) E(1)\ncan be combined to both\n[[A[DE]][BC]](6) [A[[DE][BC]]](6)\nε\n0 1\n00 01 10 11\n010 011\n0 1\n0 1 0 1\n0 1A(2)\nD(1) E(1)\nO(1) I(1)\n(14)\nε\n0 1\n10 11\n100 101 110 111\n0 1\n0 1\n0 1 0 1\nA(2)\nD(1) E(1) O(1) I(1)\n(14)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Huffman codes and Fibonacci numbers 214 (592)\n! Assume γ1 ≤ . . .≤ γk (for simplicity). If Pr\ni=1 γi < γr+2, then the Huffman\nalgorithm will deterministically merge the weights in the given order.\n▷ For instance [[[[20 + 20] + 21] + . . .] + 2r] = 2r+1 < 2r+2.\n▷ If γi ∈ N , the least possible choice for γr+2 is Pr\ni=1 γi + 1.\nStarting with γ1 := 1 this yields the Fibonacci numbers:\nF0 := 0 F1 := 1 Fk+2 = Fk+1 + Fk\nkX\ni=0\nFi = Fk+2 − 1\nε\n0 1\n00 01\n000 001\n0000 0001\n00000 00001\n0 1\n0 1\n0 1\n0 1\n0 1\n(8)\n(5)\n(3)\n(2)\n(1) (1)\n(45)\nε\n0 1\n00 01 10 11\n100 101 110 111\n0 1\n0 1 0 1\n0 1 0 1(8) (5)\n(3) (2) (1) (1)\n(47)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Huffman codes and Fibonacci numbers 215 (593)\nL (cf. here)\nAssuming that γ = Pk\ni=1 γi and γ1 ≥ . . .≥ γk:\nIf FK+2 < γ\nγk\n≤ FK+3, then\n▷ the longest code word of a Huffman code will always have length at most K,\n▷ the remaining weights γ1, . . . , γk−1 can be chosen in such way that at least\none code word will have length K.\n▷ As Φk−2 < Fk < Φk−1 for Φ = 1+\n√\n5\n2 we have that in the worst-case the\nlongest code word will have length\n≈ 1 + logΦ\nγ\nγk\n≈ 1 + 1.4404 log2\nγ\nγk\n▷ As we will see, there is always a (prefix) code with code word lengths\n⌈log2\nγ\nγi\n⌉, i.e. a Huffman code will in general not minimize the longest code\nword length (absolute height), but only the weighted code word length\n(weighted height).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "8 Basics of information theory: Shannon entropy, Huffman codes, compression\nPrefix codes and Huffman’s algorithm\nOptimal codes and Shannon’s entropy\nEntropy and information\nArithmetic codes\nData compression\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Outline 217 (595)\n▷ Simpler to use pa = γaP\na γa\nrelative frequencies/probabilities from now on.\n▷ Only re-scales the minimum, so wlog.\n▷ (pa)a∈Σ with pa ∈ [0, 1], P\na pa = 1 is called a distribution.\n▷ Story so far:\nGiven a distribution (pa), Huffman’s algorithm yields a prefix-code\nc: Σ → {0, 1}∗ that minimizes the expected code word length/tree heightP\na pa|c(a)|.\n▷ 1. Question: Optimality of Huffman codes\nIs there some uniquely decodable code that can achieve a better expected\ncode word length?\n▷ 2. Question: Minimal expected code word length\nWhat is the best we can obtain for P\na pa|c(a)|?\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Outline: Optimality of Huffman codes 218 (596)\n▷ 1. Question: Is there some uniquely decodable code that can achieve a better\nexpected code word length?\n1 Kraft’s inequality:\nFor every uniquely decodable code c: Σ → {0, 1}∗ we have P\na 2−|c(a)| ≤ 1.\n2 Given code word lengths la ∈ N 0 with P\na 2−la ≤ 1,\nwe can always construct a prefix code c: Σ → {0, 1}∗ so that |c(a)| = la.\n3 Consequence: For every uniquely decodable code ˆc there is a prefix code c so\nthat |c(a)| = |ˆc(a)|.\nThus, Huffman codes also obtain the minimal expected code word length wrt.\nall uniquely decodable codes.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kraft’s inequality 219 (597)\nL Let c: Σ∗ → {0, 1}∗ be a uniquely decodable code. Then P\na∈Σ 2−|c(a)| ≤ 1.\n▷ Set L := max{|c(a)|: a ∈ Σ}. For m ∈ N consider\n X\na∈Σ\n2−|c(a)|\n!m\n=\nX\nw=a1...am∈Σm\n2−|c(w)| =\nm·LX\nl=1\nnm,l2−l\nwhere nm,l = |{w ∈ c(Σm): |w| = l} is then number of words in Σm that are\nmapped on a code word of length l.\nIf c is uniquely decodable, then nm,l ≤ 2l has to hold, and thus\n X\na∈Σ\n2−|c(a)|\n!m\n≤\nm·LX\nl=1\n2l2−l = m · L i.e.\nX\na∈Σ\n2−|c(a)| ≤ (m · L)1/m\nNote that this has to hold for all m, but the LHS does not depend on m.\nAs (Lm)1/m = e1/m log m+1/m log L m→∞\n− − − − →e0 = 1 the claim follows.\n! This is only a necessary condition for a code to be uniquely decodable.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kraft’s inequality 219 (598)\nL Let c: Σ∗ → {0, 1}∗ be a uniquely decodable code. Then P\na∈Σ 2−|c(a)| ≤ 1.\nL For an alphabet Σ let la ∈ N s.t. P\na∈Σ 2−la ≤ 1.\nThen there also exists a prefix code c with |c(a)| ≤la.\n▷ Set nl = {a ∈ Σ | la = l} and L := maxa la.\nWe define greedily a prefix code by pruning the perfect binary tree of height L\nwhich has initially 2L leaves, namely {0, 1}L:\nFor l from 0 to L, simply turn nl nodes on level l into leaves by pruning the\ncorresponding subtrees.\nThis removes nl2L−l leaves from the perfect tree as a node on level l has\nexactly 2L−l leaves as descendants.\nAs PL\nl=0 nl2L−l = 2L Pl\nl=0 nl2−l ≤ 2L we cannot run out of leaves.\nC Huffman codes are optimal wrt. to all uniquely decodable codes.\n▷ For every uniquely decodable code c′ there exists a prefix code c s.t.\n|c(a)| ≤ |c′(a)| for all a ∈ Σ.\nThus P\na γa|c(a)| ≤P\na γa|c′(a)| for all (γa) ∈ R Σ\n≥0.",
      "As PL\nl=0 nl2L−l = 2L Pl\nl=0 nl2−l ≤ 2L we cannot run out of leaves.\nC Huffman codes are optimal wrt. to all uniquely decodable codes.\n▷ For every uniquely decodable code c′ there exists a prefix code c s.t.\n|c(a)| ≤ |c′(a)| for all a ∈ Σ.\nThus P\na γa|c(a)| ≤P\na γa|c′(a)| for all (γa) ∈ R Σ\n≥0.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Outline: Minimal expected code word length 220 (599)\n▷ 2. Question: What is the best we can obtain for P\na pa|c(a)|?\n1 First observation: for every distribution (pa) we have\n1 =\nX\na\npa =\nX\na\n2log2 pa =\nX\na\n2−log2 p−1\na ≥\nX\na\n2−⌈log2 p−1\na ⌉\ni.e. there is a prefix code c′ with |c′(a)| = ⌈log2 p−1\na ⌉.\nThus for every Huffman code c we have\nX\na\npa|c(a)| ≤\nX\na\npa|c′(a)| =\nX\na\npa⌈log2 p−1\na ⌉ ≤1 +\nX\na\npa log2 p−1\na\nH(p) := P\na pa log2 p−1\na is called the entropy of p.\n2 For every uniquely decodable code c we have\nH(p) ≤\nX\na\npa|c(a)|\nRequires Gibbs’ inequality, Jensen’s inequality and the convexity of x log x.\n3 Consequence: (Shannon’s source coding theorem) for every Huffman code\nH(p) ≤\nX\na\npa|c(a)| ≤1 + H(p)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Convex functions and Jensen’s inequality 221 (600)\nD A function f : [a, b] → R is convex if for all λ ∈ [0, 1] and x1, x2 ∈ [a, b]\nf(λx1 + (1 − λ)x2) ≤ λf(x1) + (1− λ)f(x2)\nIf ≤ can be strengthened to <, then f is strictly convex.\nF If f : [a, b] → R is convex, it is continuous on (a, b) and every local minimum\nis globally minimal; if f is strictly convex, it has a unique global minimum.\nF −log x is strictly convex on [a, 1] for any a ∈ (0, 1]:\nx log x is strictly convex on [0, 1] (with limx→0+ x log x = 0).\nx\ny\nx\ny\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Convex functions and Jensen’s inequality 221 (601)\nD A function f : [a, b] → R is convex if for all λ ∈ [0, 1] and x1, x2 ∈ [a, b]\nf(λx1 + (1 − λ)x2) ≤ λf(x1) + (1− λ)f(x2)\nIf ≤ can be strengthened to <, then f is strictly convex.\nL Jensen’s inequality: If f : [a, b] → R is convex, then also\nf(\nkX\ni=1\nλixi) ≤\nkX\ni=1\nλif(xi)\nfor all xi ∈ [a, b] and λi ∈ [0, 1] with Pk\ni=1 λi = 1\n▷ Induction on k: f(Pk\ni=1 λixi + λk+1xk+1) ≤ f(Pk\ni=1 λixi) + λk+1f(xk+1).\n▷ Convexity generalizes linearity.\n▷ Jensen’s inequality extends to the expected value of random variables over\narbitrary probability spaces.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Convex functions and Jensen’s inequality 221 (602)\nD A function f : [a, b] → R is convex if for all λ ∈ [0, 1] and x1, x2 ∈ [a, b]\nf(λx1 + (1 − λ)x2) ≤ λf(x1) + (1− λ)f(x2)\nIf ≤ can be strengthened to <, then f is strictly convex.\nD For X a finite set D(X) := {(pa)a∈X ∈ [0, 1]X | P\na∈X pa = 1}.\nC Let f : [a, b] → R be convex and x1, . . . , xk ∈ [a, b]. Then for all p ∈ D([k]):\nf(\nkX\ni=1\nxipi) ≤\nkX\ni=1\npif(xi)\nFurther (for k fixed)\nF : D([k]) → R , p7→\nkX\ni=1\nf(pixi)\nis also convex, i.e. for all λ ∈ [0, 1] and p, q∈ D([k])\nF(λp+µq) ≤\nkX\ni=1\nf(λpixi+µqixi) ≤\nkX\ni=1\nλf(pixi)+µf(qixi) = λF(p)+µF(q)\nIf f is strictly convex, so is F.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Gibbs’ inequality and entropy 222 (603)\nL Gibbs’ inequality: For every fixed p ∈ D([k]) the function\nHp : D([k]) → R , q7→ −\nkX\ni=1\npi log qi\nhas a global maximum at p with Hp(p) ≤ log(k).\n▷ wlog q >0 as −log 0 = +∞. Let f(x) := x log(x). Then\n0 = f(1) = f(\nkX\ni=1\npi) = f(\nkX\ni=1\nqi\npi\nqi\n) ≤\nkX\ni=1\nqif(pi\nqi\n) =\nkX\ni=1\npi (log pi − log qi)\ni.e. Hp(p) ≤ Hp(q). In particular thus Hp(p) ≤ Hp(1/k, . . . ,1/k) = log k.\nD Let Σ be a finite alphabet and p ∈ D(Σ). The entropy of p is\nH(p) := Hp(p) = −\nX\na∈Σ\npa log pa ≤ log2 |Σ|\nH(p) = log2 |Σ| iff pa = 1\n|Σ| for all a ∈ Σ. (−H is a strictly convex on D([k]).)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Minimal expected code word length 223 (604)\nT Let c: Σ → {0, 1}∗ be a uniquely decodable code. Then\nX\na\npa|c(a)| ≥H(p)\nhas to hold for every distribution p ∈ D(Σ).\n▷ Set γa = 2−|c(a)| and γ = P\na γa.\nBy Kraft’s inequality we have γ ≤ 1, i.e. log2 γ ≤ 0.\nThen choose qi := γi/γ in Gibbs’ inequality:\nH(p) = Hp(p) ≤ Hp(q) =\nX\na∈Σ\npa log2 q−1\ni =\nX\na∈Σ\npa(|c(a)| + log2 γ)\nHence:\nX\na∈Σ\npa|c(a)| ≥ −log2 γ −\nX\na∈Σ\npa log2 pa ≥ −\nX\na∈Σ\npa log2 pa\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Shannon’s source coding theorem for symbol codes 224 (605)\nT For every finite alphabet Σ and p ∈ D(Σ) there exists a prefix code with\nH(p) ≤\nX\na\npa|c(a)| < 1 + H(p)\nIn particular, this has to hold for any (optimal) Huffman code for p.\n▷ Set la := ⌈log2 p−1\na ⌉ = −⌊log2 pa⌋. Then\nX\na∈Σ\n2−la =\nX\na∈Σ\n2⌊log2 pa⌋ ≤\nX\na∈Σ\npa = 1\nHence, there exists a prefix code with |c(a)| ≤la ≤ 1 + log2 p−1\na and\nX\na∈Σ\n|c(a)|pa ≤\nX\na∈Σ\npa(1 − log2 pa) = 1 + H(p).\n(In fact, P\na pa|c(a)| < 1 + H(p) as la < 1 + log2 p−1\na for at least one a.)\n▷ Recall that in general a Huffman code will node minimize the maximal code\nword length, i.e. |c(a)| > ⌈log2 p−1\na ⌉ is possible for some symbols.\n▷ The uniform distribution maximizes the entropy, i.e. the expected code word\nlength.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Block alphabets for reducing expected code length 225 (606)\n▷ Assuming that a sender randomly emits symbols from Σ with fixed\nprobabilities pa, the average number of bits transmitted by symbol is\nH(p) ≤\nX\na\npa|c(a)| < H(p) + 1\ni.e. on average we might transmit one bit more then required.\n▷ If we instead compute a Huffman code c′ for Σ2 wrt. p′\nab := papb, then\nH(p′) = −\nX\na,b\npapb log2(papb) = 2H(p)\nand thus\n2H(p) = H(p′) ≤\nX\na\np′\nab|c′(ab)| < H(p′) + 1 = 2H(p) + 1\ni.e. now we transmit only 0.5 “unnecessary bits” per original symbol.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Block alphabets for reducing expected code length 225 (607)\n▷ Assuming that a sender randomly emits symbols from Σ with fixed\nprobabilities pa, the average number of bits transmitted by symbol is\nH(p) ≤\nX\na\npa|c(a)| < H(p) + 1\ni.e. on average we might transmit one bit more then required.\n▷ Wrt. Σn (“n-grams”) let p⊗n denote the distribution on Σn with\np⊗n\na1...an :=\nnY\ni=1\npai\nthen\nH(p⊗n) = −\nX\na1,...,an∈Σ\npa1 ··· pan log2(pa1 ··· pan) = nH(p)\nso that for every Huffman code cn for Σn wrt. p⊗n we have\nnH(p) = H(p⊗n) ≤ −\nX\nw∈Σn\np⊗n\nw |cn(w)| < H(p⊗n) + 1 = nH(p) + 1\ni.e. we can reduce the average overhead to 1/n.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Block alphabets for reducing expected code length 225 (608)\n▷ Assuming that a sender randomly emits symbols from Σ with fixed\nprobabilities pa, the average number of bits transmitted by symbol is\nH(p) ≤\nX\na\npa|c(a)| < H(p) + 1\ni.e. on average we might transmit one bit more then required.\n▷ Better models for a sender/source can be obtained by Markov chains:\n▷ Assume L ⊆ Σ∗ is a finite/regular language of possible words to be sent.\n▷ Construct a DFA M for (L{$})∗ and define for every state q of M the\nconditional probability pa|q that a ∈ Σ will be emitted.\n▷ The DFA wrt. these probabilities is called a Markov chain: starting in the initial\nstate q0, the “random surfer” chooses in state qi the next symbol ai ∈ Σ to be\nsent with probability pai|qi and then transitions to qiai − →∗\nM qi+1.\n▷ If there are no transmission errors, we can use for each state q a Huffman code\nwrt. pa|q as the receiver can reconstruct the run of the DFA.",
      "state q0, the “random surfer” chooses in state qi the next symbol ai ∈ Σ to be\nsent with probability pai|qi and then transitions to qiai − →∗\nM qi+1.\n▷ If there are no transmission errors, we can use for each state q a Huffman code\nwrt. pa|q as the receiver can reconstruct the run of the DFA.\nAssuming that the sender generates an infinite stream of words we can at least\nuse the Cesaro limit to obtain better average probabilities for emitting a ∈ Σ.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "8 Basics of information theory: Shannon entropy, Huffman codes, compression\nPrefix codes and Huffman’s algorithm\nOptimal codes and Shannon’s entropy\nEntropy and information\nArithmetic codes\nData compression\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Discrete probability spaces 227 (610)\nD A discrete probability space (Ω, Pr[·]) consists of countable set Ω and a map\nPr[] : 2Ω → [0, 1] with Pr[Ω] = 1 and Pr[A] =\nX\nω∈A\nPr[ω] for all A ⊆ Ω\nω ∈ Ω is called an elementary event and A ⊆ Ω an event.\nSet A = Ω \\ A s.t. Pr[¬A] := Pr\n\u0002\nA\n\u0003\n= 1 − Pr[A].\nGiven two events A, Bset Pr[A, B] := Pr[A ∧ B] := Pr[A ∩ B].\nThe (conditional) probability of A conditioned on B is\nPr[A|B] := Pr[A, B]\nPr[B] with 0\n0 := 1 Pr[ A|B] Pr[B] = Pr[B|A] Pr[A]\n(i.e. prob. that both A, Bhappen relative to prob. that B happens)\n▷ If A ⊆ U\ni Bi, then Pr[A] = P\ni Pr[A|Bi] · Pr[Bi].\nA, Bare (stochastically) independent if Pr[A, B] = Pr[A] Pr[B].\n▷ I.e. if A, Bare independent, then Pr[A|B] = Pr[A].\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Discrete probability spaces 227 (611)\nD A discrete probability space (Ω, Pr[·]) consists of countable set Ω and a map\nPr[] : 2Ω → [0, 1] with Pr[Ω] = 1 and Pr[A] =\nX\nω∈A\nPr[ω] for all A ⊆ Ω\nA random variable is a map X : Ω → EX; its distribution is\nPr[X = x] = Pr[{ω ∈ Ω | X(ω) = x}]\n(event “[X = x]”: experiment X has the outcome x.)\nIf EX ⊆ R , its expected value and variance are\nE [X] :=\nX\nx∈EX\nx ·Pr[X = x] Var[X] := E [(X −E [X])2] = E [X2] −E [X]2\n(E is linear: E [λX + µY ] = λE [X] + µE [Y ] with E [1] = 1.)\nX, Yare independent if\nPr[X = x, Y= y] = Pr[X = x] Pr[Y = y] for all x ∈ EX, y∈ EY\n(Jensen’s inequality: if f : R → R is convex, then f(E [X]) ≤ E [f(X)].)No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Discrete probability spaces 227 (612)\nD A discrete probability space (Ω, Pr[·]) consists of countable set Ω and a map\nPr[] : 2Ω → [0, 1] with Pr[Ω] = 1 and Pr[A] =\nX\nω∈A\nPr[ω] for all A ⊆ Ω\nD Laplacian probability space: Ω a finite set and Pr[A] := |A|\n|Ω| (i.e. Pr[ω] = 1\n|Ω|).\n▷ Example: To model two (independent and fair) dice rolls choose Ω = [6] × [6],\nand let Xi : Ω → [6], (x1, x2) 7→ xi be the (outcome of the) i-th dice roll.\nPr[“Two 6”] = Pr[ X1 = X2 = 6] = |{(6,6)}|\n|[6]2| = 1\n36\nPr[“Doublets”] = Pr[ X1 = X2] = |{(i,i)|i∈[6]}|\n|[6]2| = 6\n36 = 1\n6\nPr[“No 6”] = Pr[ X1 ̸= 6, X2 ̸= 6] = |[5]2|\n|[6]2| = 25\n36\nPr[“At least one 6” ] = 1 − Pr[“No 6”] = 1 − 25\n36 = 11\n36\nThe dice rolls are indeed “fair” (equiprobable, uniformly distributed) :\nPr[X1 = i] = |{i}×[6]|\n|[6]2| = 1\n6 = Pr[X2 = i] E [Xi] = 1\n6\n6X\ni=1\ni = 7\n2\nand also (stochastically) independent:\nPr[X1 = i, X2 = j] = |{(i,j)}|\n|[6]2| = 1\n36 = 1\n6 · 1\n6 = Pr[X1 = i] · Pr[X2 = j]",
      "36 = 11\n36\nThe dice rolls are indeed “fair” (equiprobable, uniformly distributed) :\nPr[X1 = i] = |{i}×[6]|\n|[6]2| = 1\n6 = Pr[X2 = i] E [Xi] = 1\n6\n6X\ni=1\ni = 7\n2\nand also (stochastically) independent:\nPr[X1 = i, X2 = j] = |{(i,j)}|\n|[6]2| = 1\n36 = 1\n6 · 1\n6 = Pr[X1 = i] · Pr[X2 = j]\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Probabilities on words 228 (613)\n▷ Consider the discrete probability space with Pr[w] = 1/4 and\nΩ = {THESIS, THERMODYNAMICS, THEOREM, THEORY}\nLet Xi : Ω → {A, . . . ,Z, ⊥} be the i-th letter, if defined, else ⊥:\n(Visualize Ω as prefix tree, then Xi is the position ofter i steps.)\n▷ Pr[X1 = T] = Pr[X2 = H] = Pr[X3 = E] = 1.\n▷ Pr[X4 = S] = 1\n4 , Pr[X4 = R] = 1\n4 , Pr[X4 = O] = 1\n2 .\n▷ Pr[X5 = R|X4 = O] = 1, Pr[X5 = I|X4 = S] = 1, Pr[X7 = M|X7 ̸= ⊥] = 1\n2\n▷ X1, X2, X3 are trivially independent, but e.g. X4, X5 are dependent.\n▷ Let L: Ω → N 0 be the random variable that returns the length:\n▷ Pr[L = 6] = 1\n2 , Pr[L = 7] = 1\n4 , Pr[L = 14] = 1\n4 ,\nE [L] = 8.25, Var[L] = 11.1875\n▷ Pr[X4 = S|L ≤ 6] = 1\n2 , Pr[X4 = S|L ≤ 7] = 1\n3 , Pr[X4 = S|L ≥ 7] = 0\n! The conditioning on some event narrows down the possible elementary events\n(=situations).No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: Random walk on N 0 229(614)\n0 1 2 3\n. . .\n1 − p 1 − p 1 − p 1 − p\np p p p\n▷ One-way random walk on N 0 (to infinity and beyond!) :\nStart at 0.\nIn every time step with prob. 1 − p “stay” and with prob. p “move”.\n(Ω is the set of all infinite paths.)\n▷ Prob. to move from 0 to 1 after k time steps: (1 − p)k−1p.\n(first k − 1 times “stay”, then “move”; geometrically distributed; waiting time.)\n▷ Prob. of being at n after k time steps:\n\u0000k\nn\n\u0001\npn(1 − p)k−n\n(n out of k times “move”, rest “stay”; binomially distributed.)\n▷ Prob. of reaching n after k time steps for the first time:\n\u0000k−1\nn−1\n\u0001\n(1 − p)k−npn.\n(need to move to n exactly in the k-th step, so choose n − 1 out of k − 1 times to “move”,\nrest “stay”; negative-binomially distributed; sum of geometric distribution.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Probability as measure for belief/plausibility/trust 230 (615)\n▷ The Bayesian interpretation of probabilities is that of a measure of “belief”\nthat some proposition A is true in a given situation (quantitative logics).\n▷ This interpretation is motivated by Cox’s theorem:\nCox’s postulates/axioms (excerpt):\n▷ Plausibility of a proposition A given some proposition (information) I is some\nreal number denoted by p(A|I) with p(A) := p(A|true).\n▷ The plausibility p(¬A|I) of the negation ¬A given I satisfies\np(¬A|I) = f(p(A|I)) for some fixed f : R → R .\n▷ The plausibility p(A, B|I) of the conjunction A ∧ B given I satisfies\np(A, B|I) = g(p(A|I), p(B|A, I)) for some fixed associative g : R 2 → R .\n▷ Both f and g are monotonic.\nAny notion of belief/plausibility that satisfies these axioms can be shown to\ngive rise to some probability space.\nIn particular, conditional probabilities Pr[A|B] are then interpreted as the\nbelief that A is true given/assuming B is true.",
      "▷ Both f and g are monotonic.\nAny notion of belief/plausibility that satisfies these axioms can be shown to\ngive rise to some probability space.\nIn particular, conditional probabilities Pr[A|B] are then interpreted as the\nbelief that A is true given/assuming B is true.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Shannon’s characterization of information 231 (616)\n▷ Similar to Cox’s axioms (cf. also det), Shannon also characterized what a\nmeaningful measurement of “information” should satisfy:\n▷ The information (content) I(A) of an event A ⊆ Ω with prob. p = Pr[A]\nshould be (strictly) mon. decreasing in p, in particular I(A) = 0 if p = 1.\n▷ The information I(A, B) of two stochastically independent events A, B⊆ Ω\nshould be I(A, B) = I(A) + I(B), i.e. the information of two independent\nevents can be measured by observing each event on its own.\nOne obvious choice for I(A) is thus −logb Pr[A] for some basis b >0.\n▷ In fact, one can show that except for the choice of the basis it is also the only\nsolution where b = 2 is natural choice wrt. the codes.\n▷ The entropy can thus be interpreted as the expected information of a\ndistribution; in particular for a (discrete) random variable X : Ω → EX:\nH(X) := −\nX\nx∈EX\nPr[X = x] log2 Pr[X = x]",
      "solution where b = 2 is natural choice wrt. the codes.\n▷ The entropy can thus be interpreted as the expected information of a\ndistribution; in particular for a (discrete) random variable X : Ω → EX:\nH(X) := −\nX\nx∈EX\nPr[X = x] log2 Pr[X = x]\nIf Pr[X = x] = |EX|−1, then H(X) = log2 |EX| (bits required to describe EX).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Discrete probability spaces: Entropy 232 (617)\nD Let (Ω, Pr[]) be some discrete probability space and X : Ω → EX,\nY : Ω → EY two random variables.\nThe entropy of X is the entropy of its distribution:\nH(X) := −\nX\nx∈EX\nPr[X = x] log2 Pr[X = x]\nThe conditional entropy of a X wrt. an event A ⊆ Ω is\nH(X|A) := −\nX\nx∈EX\nPr[X = x|A] log2 Pr[X = x|A]\nThe joint entropy of X, Yis the entropy of their joint distribution:\nH(X, Y) := −\nX\nx∈EX,y∈EY\nPr[X = x, Y= y] log2 Pr[X = x, Y= y]\nThe conditional entropy of X wrt. Y is\nH(X|Y ) :=\nX\ny∈EY\nPr[Y = y] H(X|Y = y) = H(X, Y) − H(Y )\n! If X, Yare independent: H(X, Y) = H(X) + H(Y ) and H(X|Y ) = H(X).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Entropy of random variables: example 233 (618)\n▷ Consider again the discrete probability space with Pr[w] = 1/4 and\nΩ = {THESIS, THERMODYNAMICS, THEOREM, THEORY}\nAs before, let Xi : Ω → {A, . . . ,Z, ⊥} be the i-th letter (if defined, else ⊥) and\nL: Ω → N 0 the length. We then have\n▷ H(X1) = H(X2) = H(X3) = 0\n▷ H(X4) = H(X5) = −1\n4 log2\n1\n4 − 1\n2 log2\n1\n2 − 1\n4 log2\n1\n4 = 3\n2\n▷ H(X6) = −4 · 1\n4 log2\n1\n4 = 2\n▷ H(X6|X5 = I) = H(X6|X5 = M) = 0, H(X6|X5 = R) = −2 · 1\n2 log2 = 1\nIG(X6, X5 = a) := H(X6) − H(X6|X5 = a) is the information gained by\nobserving X5 = a. E.g. IG (X6, X5 = I) = 2 as X5 = I already implies X6 = S.\n▷ H(X6|X5) = 1\n4 · 0 + 1\n4 · 0 + 1\n2 · 1 = 1\n2 . (Remaining uncertainty: RE vs RO.)\n▷ H(X5, X6) = H(X6|X5) + H(X5) = 2 (four possible cases again) .\n! H(X5|X6) = H(X5, X6) −H(X6) = 0. (We can infer X5 from X6 with certainty.)\nD IG(X, A) = H(X) − H(X|A) is the information gained from event A.",
      "▷ H(X6|X5) = 1\n4 · 0 + 1\n4 · 0 + 1\n2 · 1 = 1\n2 . (Remaining uncertainty: RE vs RO.)\n▷ H(X5, X6) = H(X6|X5) + H(X5) = 2 (four possible cases again) .\n! H(X5|X6) = H(X5, X6) −H(X6) = 0. (We can infer X5 from X6 with certainty.)\nD IG(X, A) = H(X) − H(X|A) is the information gained from event A.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Huffman codes and decision trees 234 (619)\n▷ Let X : Ω → [k] be some random number in [k].\n▷ Assume we need to guess X and are allowed to ask question of the form\nX\n?\n∈ S where S ⊆ [k].\n▷ Each question leads to a dichotomy (X ∈ S vs. X ∈ [k] \\ S) of the remaining\npossibilities, i.e. we build implicitly a binary decision tree (=prefix code) whose\nleaves are labeled by the correct answer.\n▷ The expected number of questions we are going to ask is thus the expected\nheight of the decision tree.\n▷ Hence, the expected height of such a decision tree is lower bounded by H(X).\n▷ A Huffman tree will thus minimize the expected height.\n▷ In concrete applications, the kind of questions (=predicates) that we are\nallowed to ask might be restricted:\n▷ E.g. in case of multidimensional data, i.e. X = (X1, . . . , Xn) ∈ R n, we may\nonly be able to ask whether the i-th component of X has a certain value.\nHere, the C4.5 algorithm splits the data by maximizing the average",
      "allowed to ask might be restricted:\n▷ E.g. in case of multidimensional data, i.e. X = (X1, . . . , Xn) ∈ R n, we may\nonly be able to ask whether the i-th component of X has a certain value.\nHere, the C4.5 algorithm splits the data by maximizing the average\ninformation gain H(X) − H(X|Xi) (in general, the tree will not be binary) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: decision trees via minimizing average entropy 235 (620)\n▷ Example: Assume X1, X2, X3 are directly observable (for simplicity binary)\nproperties based on which we want to decide if some dependent property\nY = f(X1, X2, X3) is true.\n▷ One simple heuristic to obtain a decision/classification procedure (to “learn”\nthe function/predicate Y from the data) is to recursively split the data wrt. that\nproperty Xi that minimizes the conditional entropy\nH(Y |Xi) =\nX\na∈EXi\nPr[Xi = a] H(Y |Xi = a)\nwhere relative frequencies are used to estimate the required probabilities.\nThe recursions stops e.g. if the entropy in the remaining data is sufficiently\nclose to 0.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: decision trees via minimizing average entropy 235 (621)\n▷ Example: Assume X1, X2, X3 are directly observable (for simplicity binary)\nproperties based on which we want to decide if some dependent property\nY = f(X1, X2, X3) is true.\n▷ Assume experimentally we have obtained the data:\nx1 x2 x3 y\n0 1 1 0\n1 0 1 1\n1 1 1 1\n0 0 1 1\n1 0 0 1\nWith η(p) = −p log2 p − (1 − p) log2(1 − p) the estimated total entropy of Y\nis η(4/5) = 0.7219 . . .while the estimates for H(Y |Xi) are\ni estimated conditional entropy\n1 3/5 · η(3/3) + 2/5 · η(1/2) = 0.4\n2 2/5 · η(1/2) + 3/5 · η(3/3) = 0.4\n3 4/5 · η(3/4) + 1/5 · η(1/1) = 0.649 . . .\nSo splitting wrt. X1 or X2 reduces the estimated conditional entropy the\nmost.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: decision trees via minimizing average entropy 235 (622)\n▷ Example: Assume X1, X2, X3 are directly observable (for simplicity binary)\nproperties based on which we want to decide if some dependent property\nY = f(X1, X2, X3) is true.\n▷ Assume we split wrt. X2, then the (conditional) data sets are:\nx1 x2 x3 y\n0 1 1 0\n1 1 1 1\nx1 x2 x3 y\n1 0 1 1\n0 0 1 1\n1 0 0 1\nIn this example, the measured data suggests that Y = 1 in the event of\nX2 = 0, as the estimated value of H(Y |X2 = 0) is 0.\nWe thus proceed with H(Y |X2 = 1, Xi):\ni estimated conditional entropy\n1 1/2 · η(1/1) + 1/2 · η(1/1) = 0\n3 2/2 · η(1/2) + 0/2 · η(0/0) = 1\ni.e. splitting further wrt. X1 reduces the entropy to 0 as now y = x1 for the\nremaining data. So the decision tree/procedure becomes:\n“If x2 = 0, assume (predict) y = 1, else assume y = x1.”\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Example: decision trees via minimizing average entropy 235 (623)\n▷ Example: Assume X1, X2, X3 are directly observable (for simplicity binary)\nproperties based on which we want to decide if some dependent property\nY = f(X1, X2, X3) is true.\n! If the experimentally obtained data is biased (does not reflect the real world) , the\nobtained decision procedure will be biased, too, obviously.\n▷ Note that the obtained tree/procedure depends on the order in which we split\nthe data recursively. (For comparison, check what happens if we split wrt. X1 first.)\n▷ Minimizing the average entropy is just one approach to “learn” a\ndecision/classification procedure from a given set of observations.\nE.g. also splitting/clustering based on Bayes’ theorem or (assuming a notion of\ndistance or similarity) principal axis or nearest neighbor is used (or simply DNF) .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Sorting and entropy 236 (624)\n▷ Consider a1 < a2 < . . . < an distinct numbers.\n▷ Assume we are given the n numbers in some arbitrary order, i.e.\naπ(1), . . . , aπ(n)\nwhere π : [n]\nbij\n− →[n] is an a-priori unknown permutation.\n▷ Sorting amounts to determining π resp. π−1.\n▷ Entropy/uncertainty is maximized under the assumption that every\npermutation is equiprobable:\nI.e. the permutation is a random variable X that is uniformly distributed over\nthe set Sn of all permutations of [n].\n▷ A standard sorting algorithm uses binary comparisons of the form ai\n?\n< aj\nand thus generates implicitly a decision tree on Sn.\n▷ The expected number of comparisons is thus lower bounded by\nH(X) = log2 n! ≈ n log2 n\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Remark: Entropy in physics (*) 237 (625)\n▷ Entropy is denoted by the letter H (cf. Heta) in reference to Boltzmann’s H\ntheorem which is considered central for the development of statistical\nmechanics, in particular thermodynamics.\n▷ Entropy is used in physics e.g. to measure the “disorder” resp. “uncertainty”\nregarding the internal/microscopic state of a physical system that can only be\nobserved from a macroscopic point of view (like a gas):\n▷ A “discretized gas” is a discrete probability space (Ω, Pr[]) where an\nelementary event ω ∈ Ω is called a microstate.\n▷ The Gibbs entropy of the gas is S := −kB\nP\nω∈Ω Pr[ω] log Pr[ω] where kB is\nthe Boltzmann constant. (i.e. up to kB\nlog 2 Shannon’s entropy.)\n▷ The second law of thermodynamics (“law”=axiom) postulates that in the\nequilibrium the distribution Pr[] maximizes entropy, i.e. Pr[ω] = 1/|Ω| so that\nS = kB log |Ω| (called Boltzmann’s entropy formula).\n▷ So Gibbs formula relates Boltzmann’s “macroscopic” formula to the",
      "log 2 Shannon’s entropy.)\n▷ The second law of thermodynamics (“law”=axiom) postulates that in the\nequilibrium the distribution Pr[] maximizes entropy, i.e. Pr[ω] = 1/|Ω| so that\nS = kB log |Ω| (called Boltzmann’s entropy formula).\n▷ So Gibbs formula relates Boltzmann’s “macroscopic” formula to the\ndistribution of the microstates so that uncertainty is maximized.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Remark: Entropy in physics (*) 237 (626)\n▷ Entropy is denoted by the letter H (cf. Heta) in reference to Boltzmann’s H\ntheorem which is considered central for the development of statistical\nmechanics, in particular thermodynamics.\n▷ Entropy is used in physics e.g. to measure the “disorder” resp. “uncertainty”\nregarding the internal/microscopic state of a physical system that can only be\nobserved from a macroscopic point of view (like a gas):\n▷ As a consequence of the second law of thermodynamics any irreversible\ncomputation requires (additional) external energy:\nEvery non-injective map f : A → B reduces entropy as it maps at least two\nstates a, a′ onto a single state f(a) = b = f(a′) thereby reducing disorder (it\nprojects A onto A/ ≡f ). In particular logic gates for disjunction and\nconjunction are themselves irreversible.\n▷ It can be shown that every TM can be transformed into a reversible TM (cf.\nreversible computing).",
      "states a, a′ onto a single state f(a) = b = f(a′) thereby reducing disorder (it\nprojects A onto A/ ≡f ). In particular logic gates for disjunction and\nconjunction are themselves irreversible.\n▷ It can be shown that every TM can be transformed into a reversible TM (cf.\nreversible computing).\n▷ In quantum computing, irreversible computation results from measuring\n(=projection) quantum states, whereas quantum logic gates are themselves\nreversible and modeled by unitary operators (cf. probabilistic FAs).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "8 Basics of information theory: Shannon entropy, Huffman codes, compression\nPrefix codes and Huffman’s algorithm\nOptimal codes and Shannon’s entropy\nEntropy and information\nArithmetic codes\nData compression\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 239 (628)\n▷ Reminder: Given a distribution (pa) a Huffman code achieves an expected\ncode word length of\nH(p) ≤\nX\na\npa|c(a)| ≤H(p) + 1\ni.e. we might “waste” one bit on average.\nBuilding a Huffman code for n-grams Σn, i.e. for the n-fold distribution p⊗n\n(i.e. assuming stochastic idependence) reduces the overhead averaged over Σ to\n1/n – but this requires to fix n.\n▷ Codes based on homomorphisms (e.g. block and prefix codes) are sometimes also\ncalled symbol codes (see e.g. MacKay) .\n▷ In contrast to block and prefix codes, an arithmetic code 1 associates with\nan input word w an interval [αw, βw) (with 0 ≤ αw < βw ≤ 1) and 2 then\nmaps w to a shortest binary word u = u1 . . . ul ∈ {0, 1}∗ that represents a\nnumber Pl\ni=1 2−iui ∈ [αw, βw).\nArithmetic codes asymptotically also achieve an expected code word length\nof H(p) but without the need to fix n.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (629)\nε\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\n▷ Assume an ordered alphabet a1, . . . , ak and a distribution p ∈ D([k]).\n(wlog pi > 0 for all i ∈ [k].)\n▷ E.g. A, B, C, D, E and p = (1/8, 3/8, 1/4, 1/8, 1/8)\nLet Pj := Pj\ni=1 pi the accumulated probabilities.\nWe then can identify the i-th symbol ai with the interval [Pi−1, Pi).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (630)\nε\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\n▷ Using the binary representation of [0, 1] every interval [α, β) ⊆ [0, 1]\ncorresponds to a subset of (countably) infinite binary sequences/words:\nx = (xi)i∈N ∈ {0, 1}ω represents the number JxK2 := P∞\ni=1 xi2−i.\n▷ Typically, the set of (countably) infinite words is denoted by Σω and\naω = aaa . . .denotes the (countably) infinite repetition of the symbol a.\n▷ x = (xi) ∈ {0, 1}ω is a path through the infinite binary tree {0, 1}∗.\n▷ The paths u01ω and u10ω meet at infinity at the same real number again.\nE.g. J010ωK2 = 1/4 = J001ωK2.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (631)\nε\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\n▷ Using the binary representation of [0, 1] every interval [α, β) ⊆ [0, 1]\ncorresponds to a subset of (countably) infinite binary sequences/words:\nx = (xi)i∈N ∈ {0, 1}ω represents the number JxK2 := P∞\ni=1 xi2−i.\n▷ A Huffman code c prunes the infinite tree and assigns each symbol a implicitly\nthe interval [Jc(a)0ωK2, Jc(a)1ωK).\n! An arithmetic code instead encodes a word as an infinite path (“bit stream”)\nthrough the tree: it is thus in general not an homomorphism.\n(Sometimes “symbol code” (homomorphism, Huffman) and “stream code”\n(arithmetic code) are used.)No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (632)\nε\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\nD Arithmetic code for an order alphabet a1, . . . , ak and p ∈ D([k]):\n▷ For input w = ai1 . . . ail, initially set α0 := 0 and β0 := 1.\n▷ For m from 0 to l − 1 set\nαm+1 := αm +(βm −αm)·P(im+1 −1) βm+1 := αm +(βm −αm)·P(im+1)\ni.e. rescale [αm, βm) to [0, 1) and descend into the interval assigned to aim+1 .\n(By construction, |βm − αm| = 1 · pi1 ··· pim.)\n▷ Finally output a shortest word u with Ju0ωK2 ∈ [αl, βl)\ne.g.: [0, 1)\nB\n− →[1/8, 1/2)\nA\n− →[5/16, 13/32)\nD\n− →[83/256, 23/64) ∋ J010110ωK2\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (633)\nε\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\n▷ In order to decode the original word from w its binary representation u\nsimplify recursively split [0, 1) wrt. the partition defined by the distribution p\nand descend into the interval that contains Ju0ωK2.\n! The only problem is when to stop: Either introduce a termination symbol $\nand encode w$ or remember |w|.\n▷ If we know u and |w|, we simply stop after |w| recursive calls.\n▷ If we use e.g. $ as “terminator”, we simply stop as soon as we would descend\ninto the interval assigned to $.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (634)\nε\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\n▷ For instance, w = AAA would lead to the interval [0, 1/512) in the example;\nso we could use u = ε as its encoding together with |w| = 3.\n▷ If we use E as “terminator”, we would instead encode wE = AAAE which\nwould result in the interval [7/4096, 1/512) instead with u = 09111.\n! In order to send u = 0913 we actually need to transmit 12 bits except we use\nrun-length encoding (RLE).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (635)\nε\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\n▷ For comparison, a Huffman code in our example would be\nc(B) = 00 c(A) = 010 c(D) = 011 c(C) = 10 c(E) = 11\nso that e.g.\nc(AAA) = 010 010 010 c(AAAE) = 010 010 010 11\n! If we fix the input length n, we can still use an arithmetic code\nα: Σ∗ → {0, 1}∗ as “symbol” code for the n-grams Σn:\nThe resulting code still has to transmit at least nH(p) bits on average as it is\na uniquely decodable code.No commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (636)\nε\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\n▷ Encoding a word w = w1 . . . wn yields an interval [αw, αw + δw) with\nδw := p⊗n\nw = pw1 pw2 ··· pwn\nso that\n−log2 δw = −\nnX\ni=1\nlog2 pwi\nAveraging over all words of length n we obtain as expected value\n−\nX\nw∈Σn\np⊗n\nw log2 δw = nH(p)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (637)\nε\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\n▷ A shortest word u ∈ {0, 1}∗ with Ju0ωK2 ∈ [αw, αw + δw) has\n▷ at least length |u| ≥ −⌈log2 δw⌉ as\nJ0m−110ωK2 = 2−m ≤ δw iff m ≥ log2 δ−1\nw\n▷ at most length |u| ≤ −⌈log2 δw⌉ + 1 as the strings u ∈ {0, 1}−⌈log2 δw⌉+1\npartition [0, 1) into intervals of length\n2⌈log2 δw⌉−1 ≤ 2⌊log2 δw⌋ < δw\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (638)\nε\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\n▷ Hence, if we encode w using a shortest word ca(w) := u, then\n−log2 δw ≤ −⌈log2 δw⌉ ≤ |ca(w)| < −⌈log2 δw⌉ + 1 ≤ −log2 δw + 2\nSo, averaging again over all w ∈ Σn obtain as expected code word length\nnH(p) ≤\nX\nw∈Σn\np⊗n\nw |ca(w)| < nH(p) + 2\ni.e. we have an average overhead of 2/n for fixed input length n.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Arithmetic codes 240 (639)\nε\n0 1\n0 1 0 1\n0 1 0 1 0 1 0 1\n0 1 0 1 0 1 0 1 0 10 1\n0 1\n0 1 0 1\n0 1/8 1/2 3/4 7/8 1A B C D E\nL Every arithmetic code c: Σn → {0, 1}∗ wrt. a given p ∈ D(Σ) and fixed input\nlength n achieves an expected code word length of\nn · H(p) ≤\nX\nw∈Σn\np⊗n\nw |c(w)| < n· H(p) + 2\nand is thus asymptotically optimal wrt. the average number of bits\ntransmitted per original symbol.\n▷ A Huffman code for Σn will reduce the expected overhead to 1/n, but an\narithmetic code does not require to fix n.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "8 Basics of information theory: Shannon entropy, Huffman codes, compression\nPrefix codes and Huffman’s algorithm\nOptimal codes and Shannon’s entropy\nEntropy and information\nArithmetic codes\nData compression\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Data compression 242 (641)\n▷ For an a priori unknown word w ∈ Σ, relative frequencies and a\ncorresponding Huffman or arithmetic code can be re-computed (in principle)\nfrom time to time: as long as transmission errors can be ruled out, the\nreceiver can simply re-compute the frequencies itself.\n▷ In contrast, in case of a fixed input w ∈ Σ∗ (text, image, music) the relative\nfrequencies can be directly computed in order to “compress” the “datum”\n(=the given) w.\nMost compression algorithms combine several different approaches for\nreducing redundancy, e.g.:\n▷ gzip resp. the underlying DEFLATE first apply LZ77 and then apply a\nHuffman code.\n▷ bzip2 uses a sequence of multiple transformations/encodings:\nfirst, a run-length encoding (e.g. aaaaabbbbccc = a5b4c3 is sent as a5b4c3),\nthen Burrows-Wheeler transform, followed by a Move-to-front transform, and\nfinally a Huffman code (and some further transformations, see the references).",
      "▷ bzip2 uses a sequence of multiple transformations/encodings:\nfirst, a run-length encoding (e.g. aaaaabbbbccc = a5b4c3 is sent as a5b4c3),\nthen Burrows-Wheeler transform, followed by a Move-to-front transform, and\nfinally a Huffman code (and some further transformations, see the references).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Subword sharing, SLPs, smallest grammar problem 243 (642)\n▷ As discussed, we either use an arithmetic code or an Huffman code for Σn to\nobtain an asymptotically optimal code (for n → ∞).\n▷ In case of an Huffman code, we need to use a fixed n; another approach is,\nto introduce auxiliary symbols for subwords that occur very frequently, a\nsimple example is\n▷ Run-length encoding (RLE): instead of transmitting a2n\nexplicitly using 2n\nbits, extend the decoder so that it can handle the implicit representation, e.g.\nlet [a, 2n] stand for a2n\n. Note that we only need n = log2 2n bits for\ntransmitting 2n this way.\n▷ RLE can be understood as encoding the word a2n\nusing the regular grammar\nS − →G [a, k], [a, k] − →G [a, k− 1], . . ., [a, 1] − →G a.\n▷ Using a context-free grammar, we can use also repeated squaring S − →G Xk,\nXk − →G Xk−1Xk−1, . . ., X0 − →G a. Now, we only need to send Xn which\nrequires only log2 n = log2 log2 2n many bits.",
      "using the regular grammar\nS − →G [a, k], [a, k] − →G [a, k− 1], . . ., [a, 1] − →G a.\n▷ Using a context-free grammar, we can use also repeated squaring S − →G Xk,\nXk − →G Xk−1Xk−1, . . ., X0 − →G a. Now, we only need to send Xn which\nrequires only log2 n = log2 log2 2n many bits.\n▷ In general, we can ask what is the least/smallest context-free grammar (SLP)\nG with L(G) = {w} (where least is measured wrt. the number non-terminals).\nIt can be shown that this “smallest grammar problem” is NP-complete (cf. eg.\nhere).\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "SLPs for compression 244 (643)\nD For w = w1 . . . wn ∈ Σn and indices 1 ≤ k ≤ l ≤ n set w[k:l] = wk . . . wl.\nD A Cut-SLP (V, Σ, P, Xn) is an SLP where we allow in also rules of the form\nXi − →G Xj[k:l] for i > jwhere Xj[k:l] generates w[k:l] if Xj generates w.\nD The (simple) LZ77 factorization of a word w ∈ Σ∗ is the unique factorization\nw = w(1) . . . w(l) satisfying\n1 w(j) = a ∈ Σ and a does not occur in w(1) . . . w(j−1), or\n2 w(j) = w(1) . . . w(j−1)[k:l] ∈ Σ+ with l − k maximal.\n▷ E.g. for w = a b a aba baaba abwe obtain the grammar\nX1 − →G a X 2 − →G X1b X 3 − →G X2a X 4 − →G X3X3\nX5 − →G X4Y4 Y4 − →G X4[2:6] X6 − →G X5Y5 Y5 − →G X5[1:2]\nThe actual compression results from the encoding of the rules:\nab(1, 1)(1, 3)(2, 5)(1, 2)\nMore efficient variants are used in practice, e.g. we can also allow to repeat\nsubwords that are longer than the so-far reconstructed prefix by means of\nrecursive expansion: ab(1, 3) = aba(2, 2) = abab(3, 1) = ababa",
      "ab(1, 1)(1, 3)(2, 5)(1, 2)\nMore efficient variants are used in practice, e.g. we can also allow to repeat\nsubwords that are longer than the so-far reconstructed prefix by means of\nrecursive expansion: ab(1, 3) = aba(2, 2) = abab(3, 1) = ababa\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "SLPs for compression 244 (644)\nD For w = w1 . . . wn ∈ Σn and indices 1 ≤ k ≤ l ≤ n set w[k:l] = wk . . . wl.\nD A Cut-SLP (V, Σ, P, Xn) is an SLP where we allow in also rules of the form\nXi − →G Xj[k:l] for i > jwhere Xj[k:l] generates w[k:l] if Xj generates w.\nD The LZ78 factorization of a word w ∈ Σ∗ is the unique factorization\nw = w(1) . . . w(l) satisfying (with w(0) := ε):\n1 w(l) = w(i) for some 0 ≤ i < land for all 0 ≤ i < j < l: w(i) ̸= w(j)\n2 for all 1 ≤ i < lthere exists a 0 ≤ j < is.t. w(i) = w(j)a for some a ∈ Σ.\n▷ E.g. for w = a b aa ba baa baabwe obtain the grammar:\nX1 − →G a X 2 − →G b\nX3 − →G X1a X 4 − →G X2a X 5 − →G X4a X 6 − →G X5b\nX9 − →G X1X2X3X4X5X6\nwhich can be encoded e.g. as\n(0, a)(0, b)(1, a)(2, a)(4, a)(5, b)\n▷ If w is generated by a Markov model, LZ78 can be shown to be\nasymptotically optimal wrt. the stationary distribution/Cesaro limit.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "SLPs for compression 244 (645)\nD For w = w1 . . . wn ∈ Σn and indices 1 ≤ k ≤ l ≤ n set w[k:l] = wk . . . wl.\nD A Cut-SLP (V, Σ, P, Xn) is an SLP where we allow in also rules of the form\nXi − →G Xj[k:l] for i > jwhere Xj[k:l] generates w[k:l] if Xj generates w.\n▷ For a comparision, consider w = a26−1.\nLZ77 factorization:\na a a2 a4 a8 a16 a31 = a(1, 1)(1, 2)(1, 4)(1, 8)(1, 16)(1, 31) = a(1, 63)\nLZ78 factorization:\na a2 a3 a4 a5 a6 a7 a8 a9 a10 a8 = (0, a)(1, a)(2, a)(3, a) . . .(9, a)(7, a)\n▷ “LZ” stands for Abraham Lempel and Jacob Ziv who proposed both\nalgorithms in 1977 and 1978, respectively.\n▷ For more examples, see e.g. the articles by Charikar et al. and Lohrey et al.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Remark: Kolmogorov complexity (*) 245 (646)\nD Fix any Turing complete programming language L.\n▷ E.g. the set of all D1TMs with Γ = {0, 1, □} and Σ = {0, 1}.\nGiven a ( wlog binary) string w ∈ {0, 1}∗ its Kolmogorov complexity KL(w)\nis the length of a shortest program wrt. L that terminates and outputs w.\n▷ Some results:\n▷ KL is uncomputable, KL(w) = n is undecidable (see busy beavers).\n▷ Any Turing complete language can expand LZ77 and LZ78 compressed strings\nor decode a Huffman code yielding another upper bound for KL(w).\nSo KL(w) is upper bounded by the length of a compression algorithm and the\ncompressed representation of w.\n▷ For every L there exists a constant cL s.t. KL(w) ≤ |w| + cL. (“return w”)\n▷ For every n ∈ N there exists a string wn s.t. KL(wn) ≥ n.\n(There are only finitely many programs of length n.)\n▷ For every two L, L′ there exists a constant s.t. KL(w) ≤ KL′(w) + cL,L′:\n(Simply include a “compiler/interpreter”.)",
      "▷ For every n ∈ N there exists a string wn s.t. KL(wn) ≥ n.\n(There are only finitely many programs of length n.)\n▷ For every two L, L′ there exists a constant s.t. KL(w) ≤ KL′(w) + cL,L′:\n(Simply include a “compiler/interpreter”.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1 Church-Turing conjecture and models of computation\n2 Once upon a time: tales from the first and second semester\n3 Turing machines, computable functions, semidecidable languages\n4 String rewriting, grammars, Chomsky hierarchy\n5 Regular languages\n6 Context-free languages\n7 Complexity theory: deterministic vs. nondeterministic polynomial time\n8 Basics of information theory: Shannon entropy, Huffman codes, compression\n9 Appendix: (under construction)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "9 Appendix: (under construction)\nTuring Machines in Python\nHalting probem in pseudo-python\nString rewriting systems and formal grammars in Python\nFrom 1TM to grammar and PCP: example\nFinite Automata in Python\nPushdown Automata in Python (accept on final state)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM in Python 248 (649)\nimport networkx as nx\nfrom c o l l e c t i o n s import deque # Python ’ s double = ended queue\nfrom t y p i n g import *\nS t a t e = Any # Q\nSymbol = Any # e l e m e n t s o fΣ and Γ\new = ( ) # ε = ()\nWord = Tuple [ Symbol ] # Σ∗ , Γ∗ ; r e c a l l words a r e a c t u a l l y t u p l e s\nRule = Tuple [ Tuple [ State , Symbol ] , Tuple [ State , Symbol , i n t] ] # (Q × Γ) × (Q × Γ × Z )\nC o n f i g = Tuple [ Word , State , Word ] # Γ∗ × Q × Γ∗\nc f g 2 s t r = lambda c : ’ ’ . j o i n ( s t r( x ) f o rx i n c [ 0 ] + ( c [ 1 ] , ) + c [ 2 ] )",
      "# Draws the ’ ’ c a l l graph ’ ’ o f the t r a n s i t i o n r u l e s o f a 1TM\ndef callgraphTM ( r u l e s : Set [ Rule ] ) :\nG = nx . DiGraph ( )\ne d g e l a b e l s = d i c t( )\nf o rt i n r u l e s :\nqa , rbd = t\nq , a = qa\nr , b , d = rbd\nG . add edge ( s t r( q ) , s t r( r ) )\ne d g e l a b e l s [ (s t r( q ) , s t r( r ) ) ] = f ” {a}:{b}/{d}”\npos = nx . s p r i n g l a y o u t (G)\nnx . draw (G, pos=pos , w i t h l a b e l s=True )\nnx . d r a w n e t w o r k xe d g e l a b e l s (G, pos=pos , e d g e l a b e l s=e d g e l a b e l s )\nr e t u r n\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM in Python: single run 249 (650)\ndef applyTMrule ( a l p h a : Word , beta : Word , r : State , b : Symbol , d : i n t, b la n k : Symbol ) = > C o n f i g :\na s s e r t = 1 <= d <= 1\ni f d == 0 :\nr e t u r n( alpha , r , ( b , ) + beta [ 1 : ] ) # αqaβ′ − →M αrbβ′\ni f d == 1 :\nr e t u r n( a l p h a + ( b , ) , r , beta [ 1 : ] ) # αqaβ′ − →M αbrβ′\nz = a l p h a [ = 1] i f l e n( a l p h a ) e l s eb la nk\nr e t u r n( a l p h a [:= 1] , r , ( z , b ) + beta [ 1 : ] ) # α′zqaβ′ − →M α′rzbβ′",
      "# I f the t r a n s i t i o n r u l e s a r e n o n d e t e r m i n i s t i c , ask the u s e r which r u l e to a p p l y\ndef runTM(w: Word , r u l e s : Set [ Rule ] , q0 : State , f i n a l : Set [ S t a t e ] , b la nk : s t r, max steps : i n t=10)= >bool :\nc u r c o n f i g = (ew , q0 , w)\nw h i l emax steps :\np r i n t( c f g 2 s t r ( c u r c o n f i g ) , c u r c o n f i g )\nmax steps = = 1\nalpha , q , beta = c u r c o n f i g\na = beta [ 0 ] i f l e n( beta ) e l s ebl a nk\na p p l i c a b l e r u l e s = [ t [ 1 ] f o rt i n r u l e s i f t [ 0 ] == ( q , a ) ] # {(r, b, d) | ((q, a), (r, b, d)) ∈ δ}\ni f l e n( a p p l i c a b l e r u l e s ) == 0 :\nr e t u r nq i n f i n a l\ni d x = 0 i f l e n( a p p l i c a b l e r u l e s ) == 1 e l s e i n t( i n p u t( ’ choose one : ’ + r e p r( l i s t( enumerate (\na p p l i c a b l e r u l e s ) ) ) ) )\nr , b , d = a p p l i c a b l e r u l e s [ i d x ]\nc u r c o n f i g = applyTMrule ( alpha , beta , r , b , d , b l an k )",
      "i d x = 0 i f l e n( a p p l i c a b l e r u l e s ) == 1 e l s e i n t( i n p u t( ’ choose one : ’ + r e p r( l i s t( enumerate (\na p p l i c a b l e r u l e s ) ) ) ) )\nr , b , d = a p p l i c a b l e r u l e s [ i d x ]\nc u r c o n f i g = applyTMrule ( alpha , beta , r , b , d , b l an k )\nr e t u r nF a l s e\nr u l e s = {\n( ( ’A ’ , ’ □’ ) , ( ’A ’ , ’ □’ , 0 ) ) , ( ( ’A ’ , ’ a ’ ) , ( ’A ’ , ’ a ’ ,1 ) ) , ( ( ’A ’ , ’ a ’ ) , ( ’B ’ , ’ a ’ ,1 ) ) ,\n( ( ’B ’ , ’ □) , ( ’B ’ , ’ □’ , 0) ) , ( ( ’B ’ , ’ a ’ ) , ( ’B ’ , ’ a ’ , 0 ) ) , ( ( ’B ’ , ’ b ’ ) , ( ’H ’ , ’ b ’ ,0 ) ) ,\n}\nrunTM( t u p l e ( ’ aab ’ ) , r u l e s , ’A ’ , {’H ’ }, ’ □’ )\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "1TM in Python: reachable configurations (BFS) 250 (651)",
      "# DFS can get trapped in a nonterminating run even if there is an accepting run; BFS avoids this\ndef bfsTM (w: Word , r u l e s : Set [ Rule ] , q0 : State , f i n a l : Set [ S t a t e ] , bl an k : Symbol , m a x s i z e : i n t=20)= >bool :\nc u r c o n f i g = (ew , q0 , w)\ntodo = deque ( [ c u r c o n f i g ] ) #\na c c e p t = F a l s e\nG = nx . DiGraph ( )\nG . add node ( c f g 2 s t r ( c u r c o n f i g ) )\nw h i l eG . number of nodes ( ) < m a x s i z e and l e n( todo ) :\nc u r c o n f i g = todo . p o p l e f t ( )\nalpha , q , beta = c u r c o n f i g\na = beta [ 0 ] i f l e n( beta ) e l s ebl a nk\na p p l i c a b l e r u l e s = [ t [ 1 ] f o rt i n r u l e s i f t [ 0 ] == ( q , a ) ]\na c c e p t = a c c e p t or ( l e n( a p p l i c a b l e r u l e s ) == 0 and q i n f i n a l )\nf o rr , b , d i n a p p l i c a b l e r u l e s :\nn x t c o n f i g = applyTMrule ( alpha , beta , r , b , d , bl an k )\ni f c f g 2 s t r ( n x t c o n f i g ) not i n G :",
      "a c c e p t = a c c e p t or ( l e n( a p p l i c a b l e r u l e s ) == 0 and q i n f i n a l )\nf o rr , b , d i n a p p l i c a b l e r u l e s :\nn x t c o n f i g = applyTMrule ( alpha , beta , r , b , d , bl an k )\ni f c f g 2 s t r ( n x t c o n f i g ) not i n G :\ntodo . append ( n x t c o n f i g )\nG . add edge ( c f g 2 s t r ( c u r c o n f i g ) , c f g 2 s t r ( n x t c o n f i g ) ) # TODO: s t r i p l e a d i n g& t r a i l i n g b l a n k s\nnx . draw (G, w i t h l a b e l s=True )\nr e t u r na c c e p t\nr u l e s = {\n( ( ’A ’ , ’ □’ ) , ( ’A ’ , ’ □’ , 0 ) ) , ( ( ’A ’ , ’ a ’ ) , ( ’A ’ , ’ a ’ ,1 ) ) , ( ( ’A ’ , ’ a ’ ) , ( ’B ’ , ’ a ’ ,1 ) ) ,\n( ( ’B ’ , ’ □’ ) , ( ’B ’ , ’ □’ , 0 ) ) , ( ( ’B ’ , ’ a ’ ) , ( ’B ’ , ’ a ’ ,0 ) ) , ( ( ’B ’ , ’ b ’ ) , ( ’H ’ , ’ b ’ ,0 ) ) ,\n}\nbfsTM ( t u p l e( ’ aab ’ ) , r u l e s , ’A ’ , {’H ’ }, ’ □’ )\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "9 Appendix: (under construction)\nTuring Machines in Python\nHalting probem in pseudo-python\nString rewriting systems and formal grammars in Python\nFrom 1TM to grammar and PCP: example\nFinite Automata in Python\nPushdown Automata in Python (accept on final state)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Special halting problem in pseudo-python 252 (653)\n# g mod . py :\ndef h ( p y s r c : b y t e s ) = > bool :\n# unknown i m p l e m e n t a t i o n o fh: Σ∗ → {0, 1} with ” h(w) = 1 IF Mw h a l t s onw ELSE h(w) = 0”\n# E s s e n t i a l l y : ”w” i s the s o u r c e code as a ( b i n a r y ) s t r i n g , w h i l e ”Mw” i s the e x e c u t a b l e\n# F o r m a l l yw has to d e s c r i b e a d e t e r m i n i s t i c one= tape TM Mw\n# Here , we assume w to be the s o u r c e code o f a python module\ndef g ( p y s r c : b y t e s ) :\ni f h ( p y s r c ) i s True :\nw h i l eTrue :\npass\nr e t u r n",
      "# r u s s e l l sp a r a d o x . py :\nfrom g mod import h , g\ndef c o n t r a d i c t i o n ( ) :\ng s r c = b u f f e r f i l e ( ”g mod . py” )\nr e t u r nh ( g s r c ) # h ( gs r c ) r e t u r n s True IFF g ( gs r c ) r e t u r n s IFF h ( gs r c ) r e t u r n s F a l s e\n! In general, for a specific predicate (e.g. h: “program halts on a specific input”) there is\nno algorithm (source code, D1TM) that takes the source code w and then decides\nwhether the implemented function Mw satifies the predicate or not.\n(I.e. computing a single bit of information on the implemented function can be impossible.)\n▷ But we can still “semidecide” whether Mw halts on w:\nsimply run Mw on w and return True (only) after Mw has halted.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Recursively enumerable from semidecidable/recognaziable 253 (654)\nfrom t y p i n g import *\nimport m u l t i p r o c e s s i n g\nfrom m u l t i p r o c e s s i n g . s h a r e d c t y p e simport Value\nimport i t e r t o o l s\na l p h a b e t = [ 0 , 1 ] # Σ\ndef f ( x : bytes , r e t : Value ) :\n. . . # a c t u a l i m p l e m e n t a t i o n o ff : Σ∗ ,→ {0, 1}\nr e t . v a l u e = i n t( a c c e p t ) # need to use a s h a r e d v a r i a b l e i n Python\nr e t u r n\ndef r u n f o n x f o r n s e c o n d s ( f : C a l l a b l e [ [ bytes , Value ] , None ] , x : bytes , n : i n t)= >bool :\nr e t = Value ( ’ i ’ , 0) # s h a r e d ‘ i n t ‘ v a r i a b l e f o r r e t u r n v a l u e o f f\nfoo = m u l t i p r o c e s s i n g . P r o c e s s ( t a r g e t=f , name=” foo ” , a r g s =(x , r e t ) )\nfoo . s t a r t ( )\nfoo . j o i n ( n ) # run ‘ f ( x ) ‘ f o r ‘ n ‘ s e c o n d s\ni f foo . i s a l i v e ( ) : # s i m u l a t i o n has not t e r m i n a t e d\nfoo . t e r m i n a t e ( )",
      "foo . s t a r t ( )\nfoo . j o i n ( n ) # run ‘ f ( x ) ‘ f o r ‘ n ‘ s e c o n d s\ni f foo . i s a l i v e ( ) : # s i m u l a t i o n has not t e r m i n a t e d\nfoo . t e r m i n a t e ( )\nfoo . j o i n ( )\nr e t u r nF a l s e\nr e t u r nr e t . v a l u e == 1",
      "# dove = t a i l i n g : i t e r a t e d BFS on a l l i n p u t s f o r i n c r e a s i n g depth / time= bound\ndef c r e a t e f e n u m e r a t i o n ( f : C a l l a b l e [ [ bytes , Value ] , None ] ) :\nn = 0 # time bound\nw h i l eTrue :\nf o rL i n range ( n ) :\nf o rx i n i t e r t o o l s . product ( alphabet , r e p e a t=L ) :\nx = b y t e s ( ’ ’ . j o i n ( x ) , ’ u t f 8 ’ ) # L= t u p l e to word to u t f 8 encoded b i n a r y word\ni f r u n f o n x f o r n s e c o n d s ( f , x , n ) i s True :\ny i e l d x # same x can be output m u l t i p l e times , i n f a c t i n f i n i t e l y o f t e n\nn += 1\n! {w ∈ Σ∗ | L(Mw) ̸= ∅} is semidecidable: enumerate L(Mw).\n(But L(Mw) = ∅ is not semidecidable.)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem and friends 254 (655)\ndef f ( x : b y t e s ) :\n. . . # i n l i n e s o u r c e code o f a g i v e n f u n c t i o nf : Σ∗ ,→ Σ∗ h e r e\n# s o u r c e code now as s t r i n g , t a k e s the p l a c e o fw i n the p r o o f s\ns r c f = rb ””” d e f f ( x : s t r ) :\n. . . # i n l i n e s o u r c e code a g a i n\n”””\ndef r f ( x : b y t e s ) = > bool :\nf ( s r c f ) # output does not matter ( f o r m a l l y : run Mw on w)\nr e t u r nTrue # a c c e p t i n p u tx IFF f h a l t s on i t s own s o u r c e code\n# r f ( x ) does NOT h a l t / r e t u r n / a c c e p t x ( i n d e p e n d e n t o f i n p u t x )\n# IFF f ( s r c f ) does NOT h a l t / r e t u r n",
      "# r f ( x ) does NOT h a l t / r e t u r n / a c c e p t x ( i n d e p e n d e n t o f i n p u t x )\n# IFF f ( s r c f ) does NOT h a l t / r e t u r n\n# IFF h ( f s r c ) r e t u r n s F a l s e /0\n! Given w (and w′), it is not semidecidable, if\n▷ Mw does not halt on ε (resp. some given input x) (i.e. ε ̸∈ L(Mw)?)\n▷ there is exists some x ∈ Σ∗ on which Mw does not halt (i.e. L(Mw) ̸= Σ∗?)\n▷ for all x ∈ Σ∗ we have that Mw does not halt. (i.e. L(Mw) = ∅?)\nNote:\n▷ LH,ε = {w ∈ Σ∗ | ε ∈ L(Mw)} is semidecidable: run Mw on ε.\n▷ LH,? = {w ∈ Σ∗ | L(Mw) ̸= ∅} is semidecidable: enumerate L(Mw).\n▷ LH,∗ = {w ∈ Σ∗ | L(Mw) = Σ∗} is also not semidecidable: next slide.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Halting problem and friends (2) 255 (656)\nimport m u l t i p r o c e s s i n g\nimport time\ndef f ( x : b y t e s ) :\n. . . # i n l i n e s o u r c e code o f a g i v e n f u n c t i o nf : Σ∗ ,→ Σ∗ h e r e\n# s o u r c e code now as s t r i n g , t a k e s the p l a c e o fw i n the p r o o f s\ns r c f = rb ””” d e f f ( x : b y t e s ) :\n. . . # i n l i n e s o u r c e code a g a i n\n”””\ndef r 2 f ( x : b y t e s ) = > bool :\nfoo = m u l t i p r o c e s s i n g . P r o c e s s ( t a r g e t=f , name=” foo ” , a r g s =( s r c f , ) )\nfoo . s t a r t ( )\nfoo . j o i n ( l e n( x ) ) # run ‘ f ( s r ff ) ‘ f o r|x| s e c o n d s\ni f foo . i s a l i v e ( ) : # run has not t e r m i n a t e d\nfoo . t e r m i n a t e ( )\nfoo . j o i n ( )\nr e t u r nTrue\nw h i l eTrue :\npass\n# r 2 f ( x ) does h a l t / r e t u r n / a c c e p t x ( i n d e p e n d e n t o f i n p u t x )\n# IFF f ( s r c f ) does NOT h a l t / r e t u r n",
      "# r 2 f ( x ) does h a l t / r e t u r n / a c c e p t x ( i n d e p e n d e n t o f i n p u t x )\n# IFF f ( s r c f ) does NOT h a l t / r e t u r n\n# IFF h ( f s r c ) r e t u r n s F a l s e /0\n! Given w, it is not semidedicable, if Mw does halt for all x ∈ Σ∗ (i.e. L(Mw) = Σ∗?)\n▷ Both LH,∗ = {w ∈ Σ∗ | L(Mw) = Σ∗} and LH,∗ are undecidable\n▷ Both L= = {(w, w′) ∈ Σ∗ × Σ∗ | L(Mw) = L(Mw′)} and L= are undecidable.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "9 Appendix: (under construction)\nTuring Machines in Python\nHalting probem in pseudo-python\nString rewriting systems and formal grammars in Python\nFrom 1TM to grammar and PCP: example\nFinite Automata in Python\nPushdown Automata in Python (accept on final state)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "SRS and grammars in Python 257 (658)\nimport networkx as nx\nfrom c o l l e c t i o n s import deque # Python ’ s double = ended queue\nfrom t y p i n g import *\nSymbol = Any # s t r i n g s w i l l be used as symbols , a l l o w s to use ‘ [ x ] ‘ as symbol\new = ( ) # ε = ()\nWord = Tuple [ Symbol ] # words a r e t u p l e s o v e r s t r i n g s\nRule = Tuple [ Word , Word ]\nw o r d 2 s t r = lambda c : ’ ’ . j o i n ( s t r( x ) f o rx i n c ) # f l a t t e n t u p l e to s t r i n g\ndef p r e t t y p r i n t g r a m m a r r u l e s ( r u l e s : s e t[ Rule ] ) :\np r i n t( ’G : ’ )\nf o rl h s , r h s i n s o r t e d( r u l e s ) :\np r i n t( ’ \\t %s = > %s ’ % ( w o r d 2 s t r ( l h s ) , w o r d 2 s t r ( r h s ) ) )\nr e t u r n\ndef bfsSRS (w: Word , r u l e s : s e t[ Rule ] , m a x s i z e : i n t=10) :\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\nc u r c o n f i g = w\ntodo = deque ( [ c u r c o n f i g ] )\nG = nx . DiGraph ( )",
      "r e t u r n\ndef bfsSRS (w: Word , r u l e s : s e t[ Rule ] , m a x s i z e : i n t=10) :\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\nc u r c o n f i g = w\ntodo = deque ( [ c u r c o n f i g ] )\nG = nx . DiGraph ( )\nG . add node ( w o r d 2 s t r ( c u rc o n f i g ) )\nw h i l eG . number of nodes ( ) < m a x s i z e and l e n( todo ) :\nc u r c o n f i g = todo . p o p l e f t ( )\ni f l e n( c u r c o n f i g ) == 0 :\ncontinue\nf o rl h s , r h s i n r u l e s :\nf o r i i n range ( l e n( c u r c o n f i g )= l e n( l h s ) +1) :\ni f c u r c o n f i g [ i : i+ l e n( l h s ) ] == l h s :\nn x t c o n f i g = c u r c o n f i g [ : i ] + r h s + c u r c o n f i g [ i+l e n( l h s ) : ]\ni f w o r d 2 s t r ( n x tc o n f i g ) not i n G :\ntodo . append ( n x t c o n f i g )\nG . add edge ( w o r d 2 s t r ( c u r c o n f i g ) , w o r d 2 s t r ( n x t c o n f i g ) )\npass\nnx . draw (G, w i t h l a b e l s=True )\nr e t u r n",
      "i f w o r d 2 s t r ( n x tc o n f i g ) not i n G :\ntodo . append ( n x t c o n f i g )\nG . add edge ( w o r d 2 s t r ( c u r c o n f i g ) , w o r d 2 s t r ( n x t c o n f i g ) )\npass\nnx . draw (G, w i t h l a b e l s=True )\nr e t u r n\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kuroda NF in Python (1/3) 258 (659)\ndef kurodaNF ( r u l e s : s e t[ Rule ] ) :\nv a r i a b l e s = s e t( )\nt e r m i n a l s = s e t( )\nf o rl h s , r h s i n r u l e s :\nv a r i a b l e s |= s e t( l h s )\nt e r m i n a l s|= s e t( r h s )\nt e r m i n a l s= = v a r i a b l e s # Assumption : a symbol i s n o n t e r m i n a l i f i t o c c u r s on the LHS\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\n# A u x i l i a r y v a r i a b l e s f o r t e r m i n a l s :[a] − →G a , [ε]− > ε\n# Might break , i f e . g . [ a ] i s a l r e a d y used a v a r i a b l e or [ i s used a t e r m i n a l\n# A l t e r n a t i v e : use n e s t e d words / terms\nn e w r u l e s = {(( ’ [% s ] ’ % x , ) , ( x , ) ) f o rx i n t e r m i n a l s}\ni f any ( l e n( l h s )>l e n( r h s ) f o rl h s , r h s i n r u l e s ) :\nn e w r u l e s . add ( ( ( ’ [ ew ] ’ , ) , ew ) )\nf o rl h s , r h s i n r u l e s :",
      "# Replace ABC − →G aB by ABC − →G [a]B[ε] , [a] − →G a , [ε] − →G ε\nr h s = l i s t( r h s )\nf o r i i n range ( l e n( r h s ) ) :\ni f r h s [ i ] i n t e r m i n a l s :\nr h s [ i ] = ’ [% s ] ’ % r h s [ i ]\nr h s += [ ’ [ ew ] ’ f o r i n range ( l e n( l h s )= l e n( r h s ) ) ]\nn e w r u l e s . add ( ( l h s ,t u p l e( r h s ) ) )\n. . .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kuroda NF in Python (2/3) 259 (660)\ndef kurodaNF ( r u l e s : s e t[ Rule ] ) :\n. . .\nr u l e s = n e w r u l e s\nn e w r u l e s = s e t( )\nf o rl h s , r h s i n r u l e s :\ni f l e n( r h s ) <= 2 or l e n( l h s ) == l e n( r h s ) :\nn e w r u l e s . add ( ( l h s , r h s ) )\ne l s e:\n# Replace A − →G CDE by AB − →G C[DE] , [DE] − →G DE\n# Replace AB − →G CDE by AB − →G C[DE] , [DE] − →G DE",
      "# Replace A − →G CDE by AB − →G C[DE] , [DE] − →G DE\n# Replace AB − →G CDE by AB − →G C[DE] , [DE] − →G DE\n# ( This s k i p s the c h a i n r u l e used i n the s l i d e s )\na s s e r t l e n( r h s ) > l e n( l h s )\ns t a r t s u f f i x = 1 i f l e n( l h s ) == 1 e l s e l e n( l h s ) = 1\nn e w r u l e s . add ( (\nl h s ,\nr h s [ : s t a r t s u f f i x ] + ( ’ [% s ] ’ % w o r d 2 s t r ( r h s [ s t a r t s u f f i x : ] ) , )\n) )\nf o r i i n range ( s t a r t s u f f i x , l e n( r h s )= 2) :\nn e w r u l e s . add ( (\n( ’ [% s ] ’ % w o r d 2 s t r ( r h s [ i : ] ) , ) ,\n( r h s [ i ] , ’ [% s ] ’ % w o r d 2 s t r ( r h s [ i + 1 : ] ) )\n) )\nn e w r u l e s . add ( (\n( ’ [% s ] ’ % w o r d 2 s t r ( r h s [= 2:]) , ) ,\nr h s [= 2] , r h s [ = 1])\n) )\n. . .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Kuroda NF in Python (3/3) 260 (661)\ndef kurodaNF ( r u l e s : s e t[ Rule ] ) :\n. . .\nr u l e s = n e w r u l e s\nn e w r u l e s = s e t( )\nf o rl h s , r h s i n r u l e s :\ni f l e n( l h s ) <= 2 :\nn e w r u l e s . add ( ( l h s , r h s ) )\ne l s e:",
      "# Replace ABC − →G DEF by AB − →G D[C; EF ] , [C; EF ]C − →G EF\na s s e r t l e n( l h s ) == l e n( r h s )\nn e w r u l e s . add ( (\n( l h s [ 0 ] , l h s [ 1 ] ) ,\n( r h s [ 0 ] , ’ [% s ;% s ] ’ % ( w o r d 2 s t r ( l h s [ 2 : ] ) , w o r d 2 s t r ( r h s [ 1 : ] ) ) , )\n) )\nf o r i i n range ( 1 , l e n( l h s )= 2) :\nn e w r u l e s . add ( (\n( ’ [% s ;% s ] ’ % ( w o r d 2 s t r ( l h s [ i + 1 : ] ) , w o r d 2 s t r ( r h s [ i : ] ) ) , l h s [ i +1]) ,\n( r h s [ i ] , ’ [% s ;% s ] ’ % ( w o r d 2 s t r ( l h s [ i + 2 : ] ) , w o r d 2 s t r ( r h s [ i + 1 : ] ) ) ) ,\n) )\nn e w r u l e s . add ( (\n( ’ [% s ;% s ] ’ % ( w o r d 2 s t r ( l h s [= 1:]) , w o r d 2 s t r ( r h s [ = 2:]) ) , l h s [ = 1]) ,\n( r h s [= 2] , r h s [ = 1]) ,\n) )\np r e t t y p r i n t g r a m m a r r u l e s ( n e w r u l e s )\nr e t u r nn e w r u l e s\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Simplifying context-free grammars in Python (1/2) 261 (662)\nfrom i t e r t o o l s import product , c o m b i n a t i o n s\ndef s i m p l i f yc o n t e x t f r e e g r a m m a r ( r u l e s : s e t[ Rule ] , axiom : Symbol , c o n t r a c t c h a i n s : bool =F a l s e ) :\na s s e r t a l l( l e n( l h s ) == 1 f o rl h s , r h s i n r u l e s )\nv a r i a b l e s = s e t( )\nt e r m i n a l s = s e t( )\nf o rl h s , r h s i n r u l e s :\nv a r i a b l e s |= s e t( l h s )\nt e r m i n a l s|= s e t( r h s )\nt e r m i n a l s= = v a r i a b l e s\np r i n t( ’ T e r m i n a l s : ’ , t e r m i n a l s )\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\ncan produce empty word = {l h s [ 0 ] f o rl h s , r h s i n r u l e s i f r h s == ew }\nw h i l eTrue :\nc u r l e n = l e n( can produce empty word )\ncan produce empty word |= {l h s [ 0 ] f o rl h s , r h s i n r u l e s i f a l l( x i n can produce empty word f o rx i n r h s )\n}",
      "can produce empty word = {l h s [ 0 ] f o rl h s , r h s i n r u l e s i f r h s == ew }\nw h i l eTrue :\nc u r l e n = l e n( can produce empty word )\ncan produce empty word |= {l h s [ 0 ] f o rl h s , r h s i n r u l e s i f a l l( x i n can produce empty word f o rx i n r h s )\n}\ni f c u r l e n == l e n( can produce empty word ) :\nbreak\np r i n t( ’ Can produce empty word : ’ , can produce empty word )\nn e w r u l e s = s e t( )\nf o rl h s , r h s i n r u l e s :\nf o rk i n range ( 1 , l e n( r h s ) ) : # s e l e c t at l e a s t one , but not a l l ; i f i n Kuroda NF : k = 1\nf o r s e l e c t i o n i n c o m b i n a t i o n s (range ( l e n( r h s ) ) , r=k ) :\ni f a l l( r h s [ i ] i n can produce empty word f o r i i n s e l e c t i o n ) :\nn e w r u l e s . add ( ( l h s ,t u p l e( r h s [ i ] f o r i i n range ( l e n( r h s ) ) i f i not i n s e l e c t i o n ) ) )\nr u l e s = = {r f o rr i n r u l e s i f r [ 1 ] == ew }\nr u l e s |= n e w r u l e s",
      "n e w r u l e s . add ( ( l h s ,t u p l e( r h s [ i ] f o r i i n range ( l e n( r h s ) ) i f i not i n s e l e c t i o n ) ) )\nr u l e s = = {r f o rr i n r u l e s i f r [ 1 ] == ew }\nr u l e s |= n e w r u l e s\ni f axiom i n can produce empty word :\nnew axiom = ’ [% s ] ’ % axiom\nr u l e s |= {(( new axiom , ) , ( axiom , ) ) , ( ( new axiom , ) , ew ) }\naxiom = new axiom\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\n. . .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "Simplifying context-free grammars in Python (2/2) 262 (663)\ndef s i m p l i f yc o n t e x t f r e e g r a m m a r ( r u l e s : s e t[ Rule ] , axiom : Symbol , c o n t r a c t c h a i n s : bool =F a l s e ) :\n. . .\ni f c o n t r a c t c h a i n s :\nc h a i n r u l e s = {( l h s , r h s ) f o rl h s , r h s i n r u l e s i f l e n( r h s ) == 1 and r h s [ 0 ] not i n t e r m i n a l s}\nw h i l eTrue :\nc u r l e n = l e n( c h a i n r u l e s )\nc h a i n r u l e s |= {(e1 [ 0 ] , e2 [ 1 ] ) f o re1 , e2 i n product ( c h a i n r u l e s , r e p e a t =2) i f e1 [ 1 ] == e2 [0] }\ni f c u r l e n == l e n( c h a i n r u l e s ) :\nbreak\nr u l e s = = c h a i n r u l e s\nf o rr i n c h a i n r u l e s :\nr u l e s |= {( r [ 0 ] , r h s ) f o rl h s , r h s i n r u l e s i f l h s == r [1] }\np r i n t( ’ Chains : ’ , c h a i n r u l e s )\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\ni s p r o d u c t i v e = {x f o rx i n t e r m i n a l s}\nw h i l eTrue :",
      "r u l e s |= {( r [ 0 ] , r h s ) f o rl h s , r h s i n r u l e s i f l h s == r [1] }\np r i n t( ’ Chains : ’ , c h a i n r u l e s )\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\ni s p r o d u c t i v e = {x f o rx i n t e r m i n a l s}\nw h i l eTrue :\nc u r l e n = l e n( i s p r o d u c t i v e )\ni s p r o d u c t i v e |= {l h s [ 0 ] f o rl h s , r h s i n r u l e s i f a l l( x i n i s p r o d u c t i v e f o rx i n r h s )}\ni f c u r l e n == l e n( i s p r o d u c t i v e ) :\nbreak\np r i n t( ’ I s p r o d u c t i v e : ’ , i s p r o d u c t i v e )\nr u l e s = {( l h s , r h s ) f o rl h s , r h s i n r u l e s i f l h s [ 0 ] i n i s p r o d u c t i v e and a l l( x i n i s p r o d u c t i v e f o rx i n\nr h s )}\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\ni s r e a c h a b l e = {axiom }\nw h i l eTrue :\nc u r l e n = l e n( i s r e a c h a b l e )\nf o rl h s , r h s i n r u l e s :\ni f l h s [ 0 ] i n i s r e a c h a b l e :",
      "r h s )}\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\ni s r e a c h a b l e = {axiom }\nw h i l eTrue :\nc u r l e n = l e n( i s r e a c h a b l e )\nf o rl h s , r h s i n r u l e s :\ni f l h s [ 0 ] i n i s r e a c h a b l e :\ni s r e a c h a b l e |= {x f o rx i n r h s}\ni f c u r l e n == l e n( i s r e a c h a b l e ) :\nbreak\np r i n t( ’ I s r e a c h a b l e : ’ , i s r e a c h a b l e )\nr u l e s = {( l h s , r h s ) f o rl h s , r h s i n r u l e s i f l h s [ 0 ] i n i s r e a c h a b l e}\np r e t t y p r i n t g r a m m a r r u l e s ( r u l e s )\nr e t u r nr u l e s\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "SRS and Grammars in Python: examples 263 (664)\nr u l e s = {\n( t u p l e( ’ S ’ ) , t u p l e( ’ASBC ’ ) ) , # ‘ t u p l e ( ’ASBC ’ ) ‘ becomes ‘ ( ’A ’ , ’ S ’ , ’B ’ , ’C ’ ) ‘ i . e . Python f a c t o r s the\ns t r i n g i n t o i t s symbols\n( t u p l e( ’ S ’ ) , t u p l e( ’R ’ ) ) ,\n( t u p l e( ’RB ’ ) , t u p l e( ’BR ’ ) ) ,\n( t u p l e( ’RCB ’ ) , t u p l e( ’BCR ’ ) ) ,\n( t u p l e( ’RC ’ ) , t u p l e( ’CR ’ ) ) ,\n( t u p l e( ’RC ’ ) , t u p l e( ’ Lc ’ ) ) ,\n( t u p l e( ’CL ’ ) , t u p l e( ’ Lc ’ ) ) ,\n( t u p l e( ’BL ’ ) , t u p l e( ’ Lb ’ ) ) ,\n( t u p l e( ’AL ’ ) , t u p l e( ’ La ’ ) ) ,\n( t u p l e( ’AL ’ ) , t u p l e( ’ a ’ ) )\n}\nbfsSRS ( t u p l e( ’ S ’ ) , r u l e s , m a x s i z e =20)\nkurodaNF ( r u l e s )\nr u l e s = {\n( t u p l e( ’ S ’ ) , t u p l e( ’TT ’ ) ) ,\n( t u p l e( ’ S ’ ) , t u p l e( ’U ’ ) ) ,\n( t u p l e( ’T ’ ) , t u p l e( ’ aSb ’ ) ) ,\n( t u p l e( ’T ’ ) , ew ) ,\n( t u p l e( ’U ’ ) , t u p l e( ’ c ’ ) ) ,\n}",
      "kurodaNF ( r u l e s )\nr u l e s = {\n( t u p l e( ’ S ’ ) , t u p l e( ’TT ’ ) ) ,\n( t u p l e( ’ S ’ ) , t u p l e( ’U ’ ) ) ,\n( t u p l e( ’T ’ ) , t u p l e( ’ aSb ’ ) ) ,\n( t u p l e( ’T ’ ) , ew ) ,\n( t u p l e( ’U ’ ) , t u p l e( ’ c ’ ) ) ,\n}\ns i m p l i f yc o n t e x t f r e e g r a m m a r ( r u l e s , axiom=’ S ’ , c o n t r a c t c h a i n s=True )\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "9 Appendix: (under construction)\nTuring Machines in Python\nHalting probem in pseudo-python\nString rewriting systems and formal grammars in Python\nFrom 1TM to grammar and PCP: example\nFinite Automata in Python\nPushdown Automata in Python (accept on final state)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From 1TM to grammar 265 (666)\nTuring machine M: Σ = {a, b}, Γ = {a, b,□}, L(M) = {ε, au| u ∈ Σ∗}\nq0start qf\n□: a/0\na: □/ + 1\nGrammar rules to guess word in L(M):\nS − →G BIB | B\n\u0014q0□\n□\n\u0015\nB B − →G\n\u0014□\n□\n\u0015\nB |\n\u0014□\n□\n\u0015\nI − →G I\n\u0014x\nx\n\u0015\n|\n\u0014q0x\nx\n\u0015\n(x ∈ Σ)\nGrammar rules to simulate run in the upper part of the nonterminals:\n\u0014q0□\nx\n\u0015\n− →G\n\u0014q0a\nx\n\u0015 \u0014 q0a\nx\n\u0015\u0014z\ny\n\u0015\n− →G\n\u0014□\nx\n\u0015\u0014qf z\ny\n\u0015\n(x, y, z∈ {a, b,□})\nGrammar rules to forget upper part (if the final state has been reached) :\n\u0014z\nx\n\u0015\n− →G x\n\u0014qf z\nx\n\u0015\n− →G x\n\u0014z\n□\n\u0015\n− →G ε\n\u0014qf z\n□\n\u0015\n− →G ε (z ∈ Γ, x∈ Σ)\nSimulation of a run on ε:\nS − →G B\n\u0014q0□\n□\n\u0015\nB − →2\nG\n\u0014□\n□\n\u0015\u0014q0□\n□\n\u0015\u0014□\n□\n\u0015\n− →G\n\u0014□\n□\n\u0015\u0014q0a\n□\n\u0015\u0014□\n□\n\u0015\n− →G\n\u0014□\n□\n\u0015\u0014□\n□\n\u0015\u0014qf □\n□\n\u0015\n− →∗\nG ε ∈ Σ∗\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From 1TM to grammar 265 (667)\nTuring machine M: Σ = {a, b}, Γ = {a, b,□}, L(M) = {ε, au| u ∈ Σ∗}\nq0start qf\n□: a/0\na: □/ + 1\nGrammar rules to guess word in L(M):\nS − →G BIB | B\n\u0014q0□\n□\n\u0015\nB B − →G\n\u0014□\n□\n\u0015\nB |\n\u0014□\n□\n\u0015\nI − →G I\n\u0014x\nx\n\u0015\n|\n\u0014q0x\nx\n\u0015\n(x ∈ Σ)\nGrammar rules to simulate run in the upper part of the nonterminals:\n\u0014q0□\nx\n\u0015\n− →G\n\u0014q0a\nx\n\u0015 \u0014 q0a\nx\n\u0015\u0014z\ny\n\u0015\n− →G\n\u0014□\nx\n\u0015\u0014qf z\ny\n\u0015\n(x, y, z∈ {a, b,□})\nGrammar rules to forget upper part (if the final state has been reached) :\n\u0014z\nx\n\u0015\n− →G x\n\u0014qf z\nx\n\u0015\n− →G x\n\u0014z\n□\n\u0015\n− →G ε\n\u0014qf z\n□\n\u0015\n− →G ε (z ∈ Γ, x∈ Σ)\nSimulation of a run on a:\nS − →G BIB − →3\nG\n\u0014□\n□\n\u0015\u0014q0a\na\n\u0015\u0014□\n□\n\u0015\n− →G\n\u0014□\n□\n\u0015\u0014□\na\n\u0015\u0014qf □\n□\n\u0015\n− →∗\nG a ∈ Σ∗\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From 1TM to grammar 265 (668)\nTuring machine M: Σ = {a, b}, Γ = {a, b,□}, L(M) = {ε, au| u ∈ Σ∗}\nq0start qf\n□: a/0\na: □/ + 1\nGrammar rules to guess word in L(M):\nS − →G BIB | B\n\u0014q0□\n□\n\u0015\nB B − →G\n\u0014□\n□\n\u0015\nB |\n\u0014□\n□\n\u0015\nI − →G I\n\u0014x\nx\n\u0015\n|\n\u0014q0x\nx\n\u0015\n(x ∈ Σ)\nGrammar rules to simulate run in the upper part of the nonterminals:\n\u0014q0□\nx\n\u0015\n− →G\n\u0014q0a\nx\n\u0015 \u0014 q0a\nx\n\u0015\u0014z\ny\n\u0015\n− →G\n\u0014□\nx\n\u0015\u0014qf z\ny\n\u0015\n(x, y, z∈ {a, b,□})\nGrammar rules to forget upper part (if the final state has been reached) :\n\u0014z\nx\n\u0015\n− →G x\n\u0014qf z\nx\n\u0015\n− →G x\n\u0014z\n□\n\u0015\n− →G ε\n\u0014qf z\n□\n\u0015\n− →G ε (z ∈ Γ, x∈ Σ)\nSimulation of a run on b (nonterminal containing state deadlocks) :\nS − →G BIB − →3\nG\n\u0014□\n□\n\u0015\u0014q0b\nb\n\u0015\u0014□\n□\n\u0015\n− →∗\nG\n\u0014q0b\nb\n\u0015\n̸∈ Σ∗\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From 1TM to PCP 266 (669)\nTuring machine M: Σ = {a, b}, Γ = {a, b,□}, L(M) = {ε, au| u ∈ Σ∗}\nq0start qf\n□: a/0\na: □/ + 1\nStart, end, clean-up (left/right), copy, rules (top = pre/bot = post; treat # as □):\n\u0014 #\n#q0#\n\u0015\n,\n\u0014qf ##\n#\n\u0015\n,\n\"\nxqf\nqf\n#\n,\n\"\nqf x\nqf\n#\n,\n\u0014x\nx\n\u0015\n,\n\u0014q0□\nq0a\n\u0015\n,\n\u0014 q0#\nq0a#\n\u0015\n,\n\u0014q0a\n□qf\n\u0015\nAccepting run on ε:\nq0□ − →M q0a − →M □qf\ntranslates into the solution:\n\u0014 #\n#q0#\n\u0015\u0014 q0#\nq0a#\n\u0015\u0014q0a\n□qf\n\u0015\u0014#\n#\n\u0015\"\n□qf\nqf\n#\u0014#\n#\n\u0015\u0014qf ##\n#\n\u0015\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From 1TM to PCP 266 (670)\nTuring machine M: Σ = {a, b}, Γ = {a, b,□}, L(M) = {ε, au| u ∈ Σ∗}\nq0start qf\n□: a/0\na: □/ + 1\nStart, end, clean-up (left/right), copy, rules (top = pre/bot = post; treat # as □):\n\u0014 #\n#q0#\n\u0015\n,\n\u0014qf ##\n#\n\u0015\n,\n\"\nxqf\nqf\n#\n,\n\"\nqf x\nqf\n#\n,\n\u0014x\nx\n\u0015\n,\n\u0014q0□\nq0a\n\u0015\n,\n\u0014 q0#\nq0a#\n\u0015\n,\n\u0014q0a\n□qf\n\u0015\nIn general, for a given input w change the start to\n\u0014 #\n#q0w#\n\u0015\n, e.g. for w = a:\nq0a□ − →M □qf\ntranslates into the solution:\n\u0014 #\n#q0a#\n\u0015\u0014q0a\n□qf\n\u0015\u0014#\n#\n\u0015\"\n□qf\nqf\n#\u0014#\n#\n\u0015\u0014qf ##\n#\n\u0015\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From 1TM to PCP 266 (671)\nTuring machine M: Σ = {a, b}, Γ = {a, b,□}, L(M) = {ε, au| u ∈ Σ∗}\nq0start qf\n□: a/0\na: □/ + 1\nStart, end, clean-up (left/right), copy, rules (top = pre/bot = post; treat # as □):\n\u0014 #\n#q0#\n\u0015\n,\n\u0014qf ##\n#\n\u0015\n,\n\"\nxqf\nqf\n#\n,\n\"\nqf x\nqf\n#\n,\n\u0014x\nx\n\u0015\n,\n\u0014q0□\nq0a\n\u0015\n,\n\u0014 q0#\nq0a#\n\u0015\n,\n\u0014q0a\n□qf\n\u0015\nIf there is no accepting run, we do not reach qf and thus the upper part will trail\nbehind forever:\n\u0014 #\n#q0b#\n\u0015\u0014q0\nq0\n\u0015\u0014b\nb\n\u0015\u0014#\n#\n\u0015\u0014q0\nq0\n\u0015\u0014b\nb\n\u0015\u0014#\n#\n\u0015\n. . .\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "From 1TM to PCP 266 (672)\nTuring machine M: Σ = {a, b}, Γ = {a, b,□}, L(M) = {ε, au| u ∈ Σ∗}\nq0start qf\n□: a/0\na: □/ + 1\nStart, end, clean-up (left/right), copy, rules (top = pre/bot = post; treat # as □):\n\u0014 #\n#q0#\n\u0015\n,\n\u0014qf ##\n#\n\u0015\n,\n\"\nxqf\nqf\n#\n,\n\"\nqf x\nqf\n#\n,\n\u0014x\nx\n\u0015\n,\n\u0014q0□\nq0a\n\u0015\n,\n\u0014 q0#\nq0a#\n\u0015\n,\n\u0014q0a\n□qf\n\u0015\nNote that in a general PCP, we may start with any pair.\nThis can be enforced as follows:\n\u0014 |#|\n|#|q0|#\n\u0015\n,\n\u0014qf |#|#|\n|#\n\u0015\n,\n\"\nx|qf |\n|qf\n#\n,\n\"\nqf |x|\n|qf\n#\n,\n\u0014x|\n|x\n\u0015\n,\n\u0014$\n|$\n\u0015\n,\n\u0014q0|□|\n|q0|a\n\u0015\n,\n\u0014 q0|#|\n|q0|a|#\n\u0015\n,\n\u0014q0|a|\n|□|qf\n\u0015\nwith |, $ so-far unsused symbols that act a separator resp. terminator.\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "9 Appendix: (under construction)\nTuring Machines in Python\nHalting probem in pseudo-python\nString rewriting systems and formal grammars in Python\nFrom 1TM to grammar and PCP: example\nFinite Automata in Python\nPushdown Automata in Python (accept on final state)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "FA in Python 268 (674)\nimport networkx as nx\nfrom c o l l e c t i o n s import deque # Python ’ s double = ended queue\nfrom t y p i n g import *\nS t a t e = Any # Q\nSymbol = Any # e l e m e n t s o fΣ\new = ( ) # ε = ()\nWord = Tuple [ Symbol ] # Σ∗ r e c a l l words a r e a c t u a l l y t u p l e s\nRule = Tuple [ Tuple [ State , Symbol ] , S t a t e ] # (Q × (Σ ∪ {ew})) × Q\nc f g 2 s t r = lambda c : ’ ’ . j o i n ( s t r( x ) f o rx i n ( c [ 0 ] , ) + c [ 1 ] )\n# The s t a n d a r d g r a p h i c a l r e p r e s e n t a t i o n o f an FA",
      "# The s t a n d a r d g r a p h i c a l r e p r e s e n t a t i o n o f an FA\n# TODO: h i g h l i g h t f i n a l s t a t e s .\ndef c a l l g r a p h F A ( r u l e s : Set [ Rule ] ) :\nG = nx . DiGraph ( )\ne d g e l a b e l s = d i c t( )\nf o rt i n r u l e s :\nqa , r = t\nq , a = qa\nG . add edge ( s t r( q ) , s t r( r ) )\ne d g e l a b e l s [ (s t r( q ) , s t r( r ) ) ] = f ” {a}”\npos = nx . s p r i n g l a y o u t (G)\nnx . draw (G, pos=pos , w i t h l a b e l s=True )\nnx . d r a w n e t w o r k xe d g e l a b e l s (G, pos=pos , e d g e l a b e l s=e d g e l a b e l s )\nr e t u r n\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "FA in Python: reachable configurations (BFS) 269 (675)",
      "# I n c a s e o f an FA , t h e r e a r e o n l y f i n i t e l y many c o n f i g u r a t i o n s f o r a g i v e n i n p u t\ndef bfsFA (w: Word , r u l e s : Set [ Rule ] , q0 : State , f i n a l : Set [ S t a t e ] ) = >bool :\nc u r c o n f i g = ( q0 ,w)\ntodo = deque ( [ c u r c o n f i g ] )\na c c e p t = F a l s e\nG = nx . DiGraph ( )\nG . add node ( c f g 2 s t r ( c u r c o n f i g ) )\nw h i l e l e n( todo ) :\nc u r c o n f i g = todo . p o p l e f t ( )\nq , beta = c u r c o n f i g\na c c e p t = a c c e p t or ( q i n f i n a l and l e n( beta ) == 0) # a FA has to c o m p l e t e l y read i t s i n p u t\na = beta [ 0 ] i f l e n( beta ) e l s eew\na p p l i c a b l e r u l e s = [ ( t [ 0 ] [ 1 ] , t [ 1 ] ) f o rt i n r u l e s i f t [ 0 ] == ( q , a ) or t [ 0 ] == ( q , ew ) ]\nf o ra , r i n a p p l i c a b l e r u l e s :\nn x t c o n f i g = ( r , beta [ 1 : ] ) i f a != ew e l s e( r , beta )\ni f c f g 2 s t r ( n x t c o n f i g ) not i n G :",
      "f o ra , r i n a p p l i c a b l e r u l e s :\nn x t c o n f i g = ( r , beta [ 1 : ] ) i f a != ew e l s e( r , beta )\ni f c f g 2 s t r ( n x t c o n f i g ) not i n G :\ntodo . append ( n x t c o n f i g )\nG . add edge ( c f g 2 s t r ( c u r c o n f i g ) , c f g 2 s t r ( n x t c o n f i g ) )\npass\nnx . draw (G, w i t h l a b e l s=True )\nr e t u r na c c e p t\nr u l e s = {\n( ( ’A ’ , ew ) , ’A ’ ) , ( ( ’A ’ , ’ a ’ ) , ’A ’ ) , ( ( ’A ’ , ’ a ’ ) , ’B ’ ) ,\n( ( ’B ’ , ew ) , ’B ’ ) , ( ( ’B ’ , ’ a ’ ) , ’B ’ ) , ( ( ’B ’ , ’ b ’ ) , ’H ’ ) ,\n}\nbfsFA ( t u p l e( ’ aab ’ ) , r u l e s , ’A ’ , {’H ’})\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "9 Appendix: (under construction)\nTuring Machines in Python\nHalting probem in pseudo-python\nString rewriting systems and formal grammars in Python\nFrom 1TM to grammar and PCP: example\nFinite Automata in Python\nPushdown Automata in Python (accept on final state)\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "PDA in Python (accept on final state) 271 (677)\nimport networkx as nx\nfrom c o l l e c t i o n s import deque # Python ’ s double = ended queue\nfrom t y p i n g import *\nS t a t e = Any # Q\nSymbol = Any # e l e m e n t s o fΣ , Γ\new = ( ) # ε = ()\nWord = Tuple [ Symbol ] # Σ∗ , Γ∗ r e c a l l words a r e a c t u a l l y t u p l e s\nRule = Tuple [ Tuple [ State , Symbol , Symbol ] , Tuple [ State , Word ] ] # (Q × (Σ ∪ {ew}) × Γ) × (Q × Γ∗)\nc f g 2 s t r = lambda c : ’ ’ . j o i n ( s t r( x ) f o rx i n c [ 0 ] + ( c [ 1 ] , ) + c [ 2 ] )\n# C a l l g r a p h o f the PDA",
      "# C a l l g r a p h o f the PDA\n# TODO: h i g h l i g h t f i n a l s t a t e s .\ndef callgraphPDA ( r u l e s : Set [ Rule ] ) :\nG = nx . DiGraph ( )\ne d g e l a b e l s = d i c t( )\nf o rt i n r u l e s :\nqaX , rYZ = t\nq , a , X = qaX\nr , YZ = rYZ\nG . add edge ( s t r( q ) , s t r( r ) )\ne d g e l a b e l s [ (s t r( q ) , s t r( r ) ) ] = f ” {a},{X}:{ ’ ’. j o i n ( s t r ( x ) f o r x i n YZ) }”\npos = nx . s p r i n g l a y o u t (G)\nnx . draw (G, pos=pos , w i t h l a b e l s=True )\nnx . d r a w n e t w o r k xe d g e l a b e l s (G, pos=pos , e d g e l a b e l s=e d g e l a b e l s )\nr e t u r n\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS",
      "PDA in Python: reachable configurations (BFS) 272 (678)\n# J u s t as normal r e c u r s i v e programs i n g e n e r a l ,\n# PDAs can r e c u r s e f o r e v e r , i . e . the ( c a l l ) s t a c k can grow unboundedly .",
      "# TODO: I n c o n t r a s t to TMs, we can a c t u a l d e c i d e whether a run i s n o n t e r m i n a t i n g .\ndef bfsPDA (w: Word , r u l e s : Set [ Rule ] , q0 : State , f i n a l : Set [ S t a t e ] , bot : Symbol , m a x s i z e : i n t=20)= >bool :\nc u r c o n f i g = ( t u p l e( [ bot ] ) , q0 ,w)\ntodo = deque ( [ c u r c o n f i g ] )\na c c e p t = F a l s e\nG = nx . DiGraph ( )\nG . add node ( c f g 2 s t r ( c u r c o n f i g ) )\nw h i l eG . number of nodes ( ) < m a x s i z e and l e n( todo ) :\nc u r c o n f i g = todo . p o p l e f t ( )\nalpha , q , beta = c u r c o n f i g\na c c e p t = a c c e p t or ( q i n f i n a l and l e n( beta ) == 0) # a c c e p t on f i n a l s t a t e\ni f l e n( a l p h a ) == 0 :\ncontinue\na = beta [ 0 ] i f l e n( beta ) e l s eew # l e f t= most l e t t e r o f i n p u t\nX = a l p h a [ = 1] # top = most s t a c k symbol",
      "a c c e p t = a c c e p t or ( q i n f i n a l and l e n( beta ) == 0) # a c c e p t on f i n a l s t a t e\ni f l e n( a l p h a ) == 0 :\ncontinue\na = beta [ 0 ] i f l e n( beta ) e l s eew # l e f t= most l e t t e r o f i n p u t\nX = a l p h a [ = 1] # top = most s t a c k symbol\na p p l i c a b l e r u l e s = [ ( t [ 0 ] [ 1 ] , * t [ 1 ] ) f o rt i n r u l e s i f t [ 0 ] == ( q , a , X) or t [ 0 ] == ( q , ew , X) ]\nf o ra , r , gamma i n a p p l i c a b l e r u l e s :\ngamma = t u p l e( r e v e r s e d(gamma) )\nn x t c o n f i g = ( a l p h a [: = 1] + gamma , r , beta [ 1 : ] ) i f a != ew e l s e( a l p h a [:= 1]+gamma , r , beta )\ni f c f g 2 s t r ( n x t c o n f i g ) not i n G :\ntodo . append ( n x t c o n f i g )\nG . add edge ( c f g 2 s t r ( c u r c o n f i g ) , c f g 2 s t r ( n x t c o n f i g ) )\npass\nnx . draw (G, w i t h l a b e l s=True )\nr e t u r na c c e p t\nr u l e s = {",
      "i f c f g 2 s t r ( n x t c o n f i g ) not i n G :\ntodo . append ( n x t c o n f i g )\nG . add edge ( c f g 2 s t r ( c u r c o n f i g ) , c f g 2 s t r ( n x t c o n f i g ) )\npass\nnx . draw (G, w i t h l a b e l s=True )\nr e t u r na c c e p t\nr u l e s = {\n( ( ’ q ’ , ’ c ’ , ’ ⊥’ ) , ( ’ f ’ , t u p l e( ’ ’ ) ) ) , ( ( ’ q ’ , ’ c ’ , ’ c ’ ) , ( ’ q ’ , t u p l e( ’ ’ ) ) ) ,\n( ( ’ q ’ , ’ a ’ , ’ ⊥’ ) , ( ’ q ’ , t u p l e( ’ c⊥’ ) ) ) , ( ( ’ q ’ , ’ a ’ , ’ c ’ ) , ( ’ q ’ , t u p l e( ’ cc ’ ) ) ) ,\n( ( ’ q ’ , ’ b ’ , ’ ⊥’ ) , ( ’ q ’ , t u p l e( ’ ⊥’ ) ) ) ,\n}\nbfsPDA ( t u p l e( ’ acacbc ’ ) , r u l e s , ’ q ’ , {’ f ’ }, ’ ⊥’ )\nNo commercial use, only for the use in Theory of Computation and Information Theory (INHN0013), 23w, TUM CIT CS"
    ],
    "tags": [],
    "vector_store_path": "data/vector_stores/Info_-58193115880710748"
  },
  {
    "id": "45a04cb1-479b-4742-90d3-4b7bca31a54c",
    "title": "Bio",
    "upload_date": "2025-02-03",
    "file_name": "Biology 101.pdf",
    "chunks": [
      "Biology 101: Cellular Respiration \nKey Concepts: Mitochondria, ATP , Glycolysis, Krebs Cycle, Electron Transport Chain \n \n1. Overview \nCellular respiration is the process by which cells convert glucose and oxygen into ATP \n(adenosine triphosphate), the cell’s energy currency. \nEquation: \nC6H12O6+6O2→6CO2+6H2O+ATPC6H12O6+6O2→6CO2+6H2O+ATP \n \n2. Stages of Cellular Respiration \na. Glycolysis \n• Occurs in the cytoplasm. \n• Breaks 1 glucose molecule into 2 pyruvate molecules. \n• Produces 2 ATP and 2 NADH. \nb. Krebs Cycle (Citric Acid Cycle) \n• Takes place in the mitochondrial matrix. \n• Generates 2 ATP, 6 NADH, and 2 FADH₂ per glucose. \nc. Electron Transport Chain (ETC) \n• Located in the inner mitochondrial membrane. \n• Uses NADH/FADH₂ to produce ~34 ATP via oxidative phosphorylation. \n \n3. Mitochondria Structure \nLabeled diagram (imagine an image here): \n1. Outer membrane \n2. Inner membrane (cristae) \n3. Matrix \nFunction: The \"powerhouse of the cell\" where the Krebs Cycle and ETC occur.",
      "• Uses NADH/FADH₂ to produce ~34 ATP via oxidative phosphorylation. \n \n3. Mitochondria Structure \nLabeled diagram (imagine an image here): \n1. Outer membrane \n2. Inner membrane (cristae) \n3. Matrix \nFunction: The \"powerhouse of the cell\" where the Krebs Cycle and ETC occur. \n \n4. Key Terms",
      "• ATP Synthase: Enzyme that produces ATP in the ETC. \n• Anaerobic Respiration: Occurs without oxygen (e.g., fermentation). \n• Chemiosmosis: Movement of protons to drive ATP synthesis"
    ],
    "tags": [],
    "vector_store_path": "data/vector_stores/Bio_5295260443130547705"
  }
]